{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/raytaylorism/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/weixin_favicon.png","path":"weixin_favicon.png","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/jquery.min.js","path":"js/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/materialize.min.js","path":"js/materialize.min.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/prettify.js","path":"js/prettify.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","path":"css/images/side-user-cover.jpg","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","path":"css/lib/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","path":"css/lib/materialize.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","path":"css/lib/prettify-tomorrow-night-eighties.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","path":"css/font/font-awesome/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","path":"css/font/font-awesome/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","path":"css/font/font-awesome/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","path":"css/font/font-awesome/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","path":"css/font/font-awesome/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","path":"css/font/font-awesome/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","path":"css/font/roboto/Roboto-Bold.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","path":"css/font/roboto/Roboto-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","path":"css/font/roboto/Roboto-Bold.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","path":"css/font/roboto/Roboto-Bold.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","path":"css/font/roboto/Roboto-Light.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","path":"css/font/roboto/Roboto-Light.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","path":"css/font/roboto/Roboto-Light.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","path":"css/font/roboto/Roboto-Light.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","path":"css/font/roboto/Roboto-Medium.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","path":"css/font/roboto/Roboto-Medium.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","path":"css/font/roboto/Roboto-Medium.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","path":"css/font/roboto/Roboto-Medium.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","path":"css/font/roboto/Roboto-Regular.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","path":"css/font/roboto/Roboto-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","path":"css/font/roboto/Roboto-Regular.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","path":"css/font/roboto/Roboto-Regular.woff2","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"b6a134c56a9ae69dc06ef4b8902090fc27a3684b","modified":1619329314460},{"_id":"source/CNAME","hash":"5dff077b04ab5f4ebffd9b9d2b1d0811c1a796a8","modified":1609744231340},{"_id":"source/_data/about.json","hash":"216087d175b2c94e1b909ff38d99c89bc443ea27","modified":1619319111065},{"_id":"source/_data/hint.json","hash":"de3a18d3d62276ffa8ad7ddda6ddd1d964425ff7","modified":1609743005904},{"_id":"source/_data/link.json","hash":"624a0f86ef9057f9f767fda6bd936c02c735d3fd","modified":1609981748554},{"_id":"source/_data/reading.json","hash":"e9622b6ddaa22a86dd5f8ebd614d156a28e1f7ad","modified":1617083565991},{"_id":"source/_data/slider.json","hash":"98fe742cf6b11d824e5a0d82dd6d542a6eaa20c3","modified":1609746178917},{"_id":"source/_posts/.DS_Store","hash":"0d7a694619263d9bd002dccf8b7da99c3578cd48","modified":1619329323004},{"_id":"source/_posts/CS61B-Week2.md","hash":"8dbe04a856c5e6e7d2da787505a38f64549a4ddd","modified":1619324708496},{"_id":"source/_posts/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers.md","hash":"194c243efcd051a9aef782bbe8c4a2e90ce52b19","modified":1613567620346},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function.md","hash":"50218e321af539651a821d8f22b3a31692ab9113","modified":1609993016724},{"_id":"source/_posts/Survey-vision-transformer.md","hash":"8fee6c7eb85bc02113aaab707b976c16b0f0cf7a","modified":1619329373111},{"_id":"source/_posts/notes-ML-FM.md","hash":"7d00d0721b820ea3e6078e86d55e94e8df60fc46","modified":1613567606433},{"_id":"source/about/index.md","hash":"3a4933ee66a724bdc9c6779e244a085e4004bdd4","modified":1619319312153},{"_id":"source/categories/index.md","hash":"21c01aad3dc05ec6bad883f2fbf4e0d654c26465","modified":1609993317207},{"_id":"source/reading/index.md","hash":"a866087ffe1d857e9b2a2c30b2c1c13b1dc4f0f3","modified":1609846070007},{"_id":"source/tags/index.md","hash":"d2445b6751df9d39bd57d243229878830bc1e5f6","modified":1609992490112},{"_id":"source/_posts/Survey-vision-transformer/clip_image001.png","hash":"0e13cb32e27691f2b64c82e5a01b6a566d555ee7","modified":1619329308787},{"_id":"source/_posts/Survey-vision-transformer/clip_image002.png","hash":"9abb1a82fb63b3da66b36125c91af33c941bcb63","modified":1619329308787},{"_id":"source/_posts/Survey-vision-transformer/clip_image003.png","hash":"64250098098634ca8b4876edb7d8d94e72f3c07d","modified":1619329308788},{"_id":"source/_posts/Survey-vision-transformer/clip_image004.png","hash":"a3f243df8db455c891a5434c1d3e0fe3acd5e9bd","modified":1619329308789},{"_id":"source/_posts/Survey-vision-transformer/clip_image005.png","hash":"dea126817372ad308a68e054dda964ee1f08572d","modified":1619329308791},{"_id":"source/_posts/Survey-vision-transformer/clip_image007.png","hash":"5175998d2d23dc50914ae7360cc30e0854408566","modified":1619329308792},{"_id":"source/_posts/Survey-vision-transformer/clip_image009.png","hash":"71961dc1647451676df0184ac001693b112b1e26","modified":1619329308793},{"_id":"source/_posts/Survey-vision-transformer/clip_image011.png","hash":"dc456312ae88d30a55d75d1d1a273ce0aea7c038","modified":1619329308794},{"_id":"source/_posts/Survey-vision-transformer/clip_image013.png","hash":"ba95703381ae371f34732381eb418a2088d65620","modified":1619329308795},{"_id":"source/_posts/Survey-vision-transformer/clip_image016.png","hash":"d8324302e6072cb5ebf229474d0e7d24e52ee76b","modified":1619329308796},{"_id":"source/_posts/Survey-vision-transformer/clip_image019.png","hash":"bf8d4b484a63e658343cac998e0095cecbac262c","modified":1619329308797},{"_id":"source/_posts/Survey-vision-transformer/clip_image023.png","hash":"28bc904909e227f40c1e3d1113f8dc8efbad3d18","modified":1619329308798},{"_id":"source/_posts/Survey-vision-transformer/clip_image027.png","hash":"ecec8a2ca638cc1becd1f4d5aba156e7c74ed27c","modified":1619329308799},{"_id":"source/_posts/Survey-vision-transformer/clip_image029.png","hash":"690e6b8f9f178f51a31bdf8dcf33bda8ffd46b16","modified":1619329308800},{"_id":"source/_posts/Survey-vision-transformer/clip_image025.png","hash":"7e72a1207eeffc4d9d39781f486fba701b1a8bc2","modified":1619329308798},{"_id":"source/_posts/Survey-vision-transformer/clip_image031.png","hash":"5f0dafa83089b1976b07733b2fd8c092d834608e","modified":1619329308801},{"_id":"source/_posts/Survey-vision-transformer/clip_image034.png","hash":"6960d9d25a36b3e2c19ad6bf2cc1fcbe89ebdd47","modified":1619329308802},{"_id":"source/_posts/Survey-vision-transformer/clip_image033.png","hash":"e27674f98bad707a921905b503c9fbf6185a4c48","modified":1619329308801},{"_id":"source/_posts/Survey-vision-transformer/clip_image035.png","hash":"eeaad5325e0b2a5b9137e2d726224beb7045cf8d","modified":1619329308803},{"_id":"source/_posts/Survey-vision-transformer/clip_image038.png","hash":"1bef009b19dcf6cd30c48d6a7e20ce7c0eeb09eb","modified":1619329308805},{"_id":"source/_posts/Survey-vision-transformer/clip_image040.png","hash":"cd478ee47a55a10429c82a682691ad01f93fbbd9","modified":1619329308806},{"_id":"source/_posts/Survey-vision-transformer/clip_image044.png","hash":"a3c299e01669750baef8171cf7c9fb50bf960ffc","modified":1619329308807},{"_id":"source/_posts/Survey-vision-transformer/clip_image042.png","hash":"ebbbf27f816951c5b9337d76f02c6a07f0951bc6","modified":1619329308807},{"_id":"source/_posts/Survey-vision-transformer/clip_image046.png","hash":"78b01fea1ab08599d564f6a849fa390417c68f12","modified":1619329308808},{"_id":"source/_posts/Survey-vision-transformer/clip_image048.png","hash":"38166a000ee6a5794cec4b73281682d511623b9e","modified":1619329308809},{"_id":"source/_posts/Survey-vision-transformer/clip_image051.png","hash":"0fd96824cf4901e9a1f7e48148a2d50f77d26097","modified":1619329308810},{"_id":"source/_posts/Survey-vision-transformer/clip_image055.png","hash":"2b44069f3eea7fbe6d3ca7da6e3769e97dac29f4","modified":1619329308813},{"_id":"source/_posts/CS61B-Week2/image-20210425113258793.png","hash":"6e04996ee4c28c81087ae0e8cd03091497d5245d","modified":1619324646031},{"_id":"source/_posts/Survey-vision-transformer/clip_image036.png","hash":"8e4fda135bebedfe89d7e345ba2199b3b7d5a65f","modified":1619329308804},{"_id":"source/_posts/Survey-vision-transformer/clip_image050.png","hash":"791f67513d849895843434534bdeefc382df896a","modified":1619329308810},{"_id":"source/_posts/Survey-vision-transformer/clip_image053.png","hash":"4160ce8f9d2eabf070cb2473fa277deda06126dd","modified":1619329308811},{"_id":"source/_posts/notes-ML-FM/image-20210214174913278.png","hash":"12553f8cbd22b15068b4cf0bc2173c1f83c16c93","modified":1613296153278},{"_id":"source/_posts/Survey-vision-transformer/clip_image056.png","hash":"a78943be90d65cbfb63565aa43f1a10cb363353c","modified":1619329308814},{"_id":"source/_posts/Survey-vision-transformer/clip_image057.png","hash":"a10f2c36b0e93e183047abcc1dce5d72222f6d79","modified":1619329308815},{"_id":"source/_posts/Survey-vision-transformer/clip_image054.png","hash":"d618ab0543d394ff056d59436aa16b0f7893baf8","modified":1619329308812},{"_id":"source/_posts/notes-ML-FM/image-20210214174809740.png","hash":"55f75f26acb1919ea76e2a618dd71d6f0d2804fc","modified":1613296089740},{"_id":"source/_posts/notes-ML-FM/image-20210214182520461.png","hash":"6fa531b36a88eef90f1aae1f264009a4f022ac8c","modified":1613298320461},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/1.png","hash":"2db678db0d744b6abf0148fc4f7690bcd61635c3","modified":1609844564413},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/2.png","hash":"e27010e5c846311585587427d4dc5716696c0d57","modified":1609844597221},{"_id":"source/_posts/notes-ML-FM/image-20210214173356481.png","hash":"24868d2884527b8b00b7288fe2475d8f9a39bca2","modified":1613295236482},{"_id":"source/_posts/notes-ML-FM/image-20210214174422276.png","hash":"588366cb2e4d3ae87401f5972cccd294b4a55bb2","modified":1613295862277},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/5.png","hash":"918add3b718d520c1cc6b3c340dfaf58625e44b7","modified":1609844687196},{"_id":"themes/raytaylorism/.gitignore","hash":"3ba1b7b4baceca12575cc0f212e13a504af994a7","modified":1609740676881},{"_id":"themes/raytaylorism/Gruntfile.js","hash":"f0c204fc7b3343417cc1a0fdca12ab089394b654","modified":1609740676882},{"_id":"themes/raytaylorism/LICENSE","hash":"391dd84c2091363936603d63cc71ba1628a13c86","modified":1609740676882},{"_id":"themes/raytaylorism/README.md","hash":"23151e838b2f0c029c5cc928dad86105b8c06ba4","modified":1609740676882},{"_id":"themes/raytaylorism/_config.yml","hash":"812c4e7ae28acf6ee0fd14601234d1b108502d54","modified":1619328745534},{"_id":"themes/raytaylorism/log.md","hash":"e6b93891f941c723524b143b69b0d303064db54b","modified":1609740676889},{"_id":"themes/raytaylorism/_data/about.json","hash":"068d6827b50f1c0ae7bb7bb1a9032508143a5bc2","modified":1609740676882},{"_id":"themes/raytaylorism/_data/hint.json","hash":"178fe345ea95511364ed70da86186cb834947993","modified":1609740676882},{"_id":"themes/raytaylorism/_data/link.json","hash":"b865125d0440e6717ee3d88d1b518a4ebdb32d61","modified":1609740676882},{"_id":"themes/raytaylorism/_data/reading.json","hash":"e8045e22b2d499a5d813320a8c2b1bccdbedd46d","modified":1609740676883},{"_id":"themes/raytaylorism/_data/slider.json","hash":"361373b57cfb5371027af42000bbaec4e03333a3","modified":1609740676883},{"_id":"themes/raytaylorism/languages/default.yml","hash":"936a72db42d299cd5e912198b9ace4fd64c476fd","modified":1609847014718},{"_id":"themes/raytaylorism/languages/zh-CN.yml","hash":"c5fcb3d2b353d9747238ca78106953301f9a2018","modified":1609740676884},{"_id":"themes/raytaylorism/languages/zh-TW.yml","hash":"ae281c898cea81f4c897c0a69c45e2ce6a4314a6","modified":1609740676884},{"_id":"themes/raytaylorism/layout/about.ejs","hash":"54e74d61dba41f173f111e32deeb58447260f0e3","modified":1609740676889},{"_id":"themes/raytaylorism/layout/archive.ejs","hash":"0a21af8903e95c6d8bb7554b089ac219e8708ad7","modified":1609740676889},{"_id":"themes/raytaylorism/layout/index.ejs","hash":"34cbcb6c75e2eef622fea6fecebfe15fb7522a95","modified":1609740676889},{"_id":"themes/raytaylorism/layout/layout.ejs","hash":"0fbced6bf0129f550ad66d57735d269b70728b49","modified":1609740676889},{"_id":"themes/raytaylorism/layout/page.ejs","hash":"90441f114859ce63ef7c7d93d668dbe5939995c2","modified":1609740676889},{"_id":"themes/raytaylorism/layout/post.ejs","hash":"8e550fd95ef761909294ed3a4aa428ff0509fbf0","modified":1609740676889},{"_id":"themes/raytaylorism/layout/reading.ejs","hash":"52906ee0e7e679d417d5bc385e054e16e9ff0256","modified":1609740676889},{"_id":"themes/raytaylorism/layout/tag.ejs","hash":"42ecab14917abd40c0a38e6ab629f089352a24b1","modified":1609740676889},{"_id":"themes/raytaylorism/source/favicon.png","hash":"d44008b0d6298287cdcfe744a2c8c562569f67ff","modified":1609740676906},{"_id":"themes/raytaylorism/_md/about/index.md","hash":"ee081f0766b1bbdd72b9a254a934033878dd1571","modified":1609740676883},{"_id":"themes/raytaylorism/source/weixin_favicon.png","hash":"4a8466bd7d8cf4753cab8fb68647b40b91a246ad","modified":1609740676909},{"_id":"themes/raytaylorism/_md/reading/index.md","hash":"ffe64363f79a74ca022f15447a03a96808c64794","modified":1609740676883},{"_id":"themes/raytaylorism/layout/_partial/after_footer.ejs","hash":"77476565bc85987d7656751cbc27b473223b0186","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/archive.ejs","hash":"6fc4dc05d153dbf1dd955df4ff19c380692f87e9","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/archive_title.ejs","hash":"37c38ef6972ddd92668ea08983f4b34230b39d52","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/article.ejs","hash":"068cd4f944f8f0810d06bc79d11042da406c7067","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/construction.ejs","hash":"21190b5a0d567ed4ea5d5289459690b72c1452f0","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/feature_guide.ejs","hash":"752d5c0a4a6f2f2228ae99bb6bede195080a15d8","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/float.ejs","hash":"42ad838e39c007226eb4151292a459173e30d8ea","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/footer.ejs","hash":"6634e01d620e2f341c5e3dcda180caf83f042252","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/head.ejs","hash":"406c0bdb3ef224bb5ec375123426a0babae2724a","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/header.ejs","hash":"426eeb374b409a6ab6eb8e21a7213b6a6147d6f9","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/menu_drawer.ejs","hash":"28a46dd851b971216c788ace1ca5609d961c2446","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/pagenav.ejs","hash":"e7ada8faaee878ea4dde267d1b420bb45421670d","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/pagination.ejs","hash":"247c1507971b3e41ed539dd7f8d15af8e21c0d58","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/search.ejs","hash":"1285a8ecb670f6460b31c0fbca9af13b202f5838","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_partial/side_nav.ejs","hash":"b12e72453fb981924d17fa48904af6951f07450f","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_partial/simple_article.ejs","hash":"fdcbb516a3745d0a70c94e565d53510d9f47693c","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_partial/slider.ejs","hash":"bb7b53f6ca9c852808d955fb074f88112e51ea59","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/blogroll.ejs","hash":"cf42209342e51e1729dcc9b01b1e5497f152844f","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/category.ejs","hash":"fb2d42083c97dfaba2717fb2e63e744259ec4530","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/recent_posts.ejs","hash":"0025878eb4cbf17ddc909f82497e9c73e4091c20","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/tag.ejs","hash":"31b93c078a03af98de504eeb0153f9c0dbc74ed9","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/tagcloud.ejs","hash":"1da338304f94f042606b73e235e9c725628c34ad","modified":1609740676888},{"_id":"themes/raytaylorism/source/css/style.styl","hash":"2c7ef7179e29084efe77c653d537b56889734a22","modified":1609740676906},{"_id":"themes/raytaylorism/source/js/prettify.js","hash":"d24b1da342b5c2d0582f0922118aaf0b2a6840d5","modified":1609740676908},{"_id":"themes/raytaylorism/layout/_partial/plugin/analytics.ejs","hash":"b88303620558f833c6d7505af762d12e21f90f90","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/comment.ejs","hash":"7bdcfd6b3a5b7dee57e9b96ca90a127b7562fc3f","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/google_code_prettify.ejs","hash":"3aecf1e3e706417131918e3f489557e2d5f438af","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/main_javascript.ejs","hash":"cc246117386c6cbde13e3b4316ba5e85af659df6","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/mathjax.ejs","hash":"7d8c369b14e75d2f120c033d319a4eb749392f38","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/noscript.ejs","hash":"182650c8be93b093997ac4d5fe14af2f835b98d9","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/page_stat.ejs","hash":"0dcd6b1185fa311cd5172b25067436f14e6d7429","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/reward.ejs","hash":"fde8d42347f72f3b3594c36b1f3c94c6d90a31b6","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/category.ejs","hash":"f48f487dc952f5703d9bc7737fc6eb2e6b439608","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/gallery.ejs","hash":"0adb673358b42b9ac52b2c1f72b92368cdcf5f2e","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/prevnext.ejs","hash":"dbb790cf454fda546c82a411a3b50ebb0129a1e8","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/readtimes.ejs","hash":"c829d0598f9906f663a8ace1c86f2aa6024d642c","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/tablecontents.ejs","hash":"a851061909d4e27321d1792a262f55385529fb2d","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/tag.ejs","hash":"36cbf8f85f93be441d47d1a4a0584afd85480d4f","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/time.ejs","hash":"e11498240ece670129a2fbfb2fed16ff315344d4","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/title.ejs","hash":"c6c44ea53bbfd3838c7bf7cc236c6db1a4b9460e","modified":1609740676887},{"_id":"themes/raytaylorism/source/css/_base/icons.css","hash":"ab167f1694ffe10c3c51d18a633efd41be121555","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_base/layout.styl","hash":"14a747f9fce53f586d11ed67a597a8e71a802d17","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_base/lib_customize.styl","hash":"5f25b295a3ad99991952f864573c0f1ccc6a1591","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_base/variable.styl","hash":"6812c6940c7c59b9fab5b41e6b832e89416d11c5","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_partial/about.styl","hash":"b0f80e89e7518d49190b79985c10c8a7b24bfa19","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_partial/archive.styl","hash":"d912cf297c10e78bd90f3210d596ec87a16f74ad","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/article.styl","hash":"94bdf4d6da4ec154124ac66008c8dff66882c7e4","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/comment.styl","hash":"bfa4d7caedffffab18b29f70db9cbf2a15a5f24b","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/footer.styl","hash":"92e1c226202782e5d429fbe72b98ae4e07fc0233","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/header.styl","hash":"534686e8e9de54e8dd99eb1b064f5ad3a0199a4e","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/index.styl","hash":"27918d50e5a69899f184280d9e0048576ac3c85d","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/link_context.styl","hash":"cd403f732319354781c552d26d37bad7c4450ad5","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/other.styl","hash":"710eea97e5c98a1426d1a3c0fc8f648279c7a82d","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/reading.styl","hash":"7abecdfc7fd21f7d11f1568d430418296b34945a","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/search.styl","hash":"caef055a95598415656c417e662264397363704b","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/side_nav.styl","hash":"b239b6b55e87e86d038d6aa821beeb66a9cbaf39","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/slider.styl","hash":"8933a67e92680dbdfd6af7c2ecaa8d86346df907","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/syntax.styl","hash":"20df0f8a54729980094514fc726b51591ada1ad7","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/tablecontents.styl","hash":"e04fa0e7664065077750a7223ae3390cc84a4c56","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1609740676905},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","hash":"683d12731b7429d32ec7de00a6706602e403013f","modified":1609740676905},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","hash":"35e07bd7a4585363060edd558a0e9939e7e68323","modified":1609740676906},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1609740676898},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1609740676899},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1609740676900},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1609740676900},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1609740676903},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1609740676903},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1609740676903},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1609740676904},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1609740676905},{"_id":"source/_posts/notes-ML-FM/image-20210214182334199.png","hash":"774c69ec91d50792fcf91cf68b5b80414fa6d5fc","modified":1613298214200},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/4.png","hash":"be1010578082af53f248773a883ac875af904676","modified":1609844682653},{"_id":"themes/raytaylorism/source/js/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1609740676907},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","hash":"41d1676b73eec020eaeb9b507dcbcd30069ae8cb","modified":1609740676906},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1609740676893},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1609740676894},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1609740676898},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1609740676898},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1609740676899},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1609740676902},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1609740676904},{"_id":"themes/raytaylorism/source/js/materialize.min.js","hash":"c9308fbe808a149aa11061af40a4be5f391cccee","modified":1609740676908},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1609740676897},{"_id":"source/_posts/notes-ML-FM/image-20210214175022879.png","hash":"08cf0616efc761a32e5e38f9c01856eb1e14c714","modified":1613296222879},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/3.png","hash":"9a52e35f1b8c80691a5b0be4b1ab6a4f2d56a7f6","modified":1609844605115},{"_id":"source/_posts/notes-ML-FM/image-20210214175304898.png","hash":"54b725874e2e3cdb8a0f0819902048c955b05083","modified":1613296384898},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","hash":"b06b5c8f67fd632cdc62a33b62ae4f74194131b3","modified":1609740676896},{"_id":"source/_posts/notes-ML-FM/image-20210214173840928.png","hash":"53edcce7af1fbaf7d59acf2062316d81e63f723e","modified":1613295520928},{"_id":"source/_posts/notes-ML-FM/image-20210214174108194.png","hash":"cd7d300b322101079d7f26615f1ca358931e9098","modified":1613295668194},{"_id":"source/_posts/notes-ML-FM/image-20210214174003005.png","hash":"c8c21d653e549d5835ad6ee49237cb6a7b0f60d4","modified":1613295603006},{"_id":"source/_posts/notes-ML-FM/image-20210214174957751.png","hash":"f07858cb57086c6becfbf436e569f90302fbb847","modified":1613296197752},{"_id":"source/_posts/notes-ML-FM/image-20210214174310749.png","hash":"040595ce2e24cd22fab14a223baae6d4a217107f","modified":1613295790750},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/7.png","hash":"cc84be16b5350a69e15af7aeb998acfd9c1fcd5d","modified":1609844697567},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/6.png","hash":"1e4661db20aebb2c3bc315f658beb41e9fe62893","modified":1609844692677},{"_id":"public/content.json","hash":"bcaba7104261532d3bb7c31f06cad7f6de60eea4","modified":1619329411260},{"_id":"public/about/index.html","hash":"3fa7d63d59fdc868cc8a4c1a717b1c2124bba038","modified":1619329411260},{"_id":"public/categories/index.html","hash":"5fb6a377198c5bae8ce0b059c0d84275e1c50f2e","modified":1619329411260},{"_id":"public/reading/index.html","hash":"e25936f57eee13ff382f601fdc3949c054b37122","modified":1619329411260},{"_id":"public/tags/index.html","hash":"744214ccb134ea115f6fefbb36efc2a9c60a6001","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/index.html","hash":"fb7b7cd232f331e22336d4b0a1f5382038f0f0e0","modified":1619329411260},{"_id":"public/2021/04/24/CS61B-Week2/index.html","hash":"d75a13ac7d88dea3572a3c4b3a280027e58f8724","modified":1619329411260},{"_id":"public/2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/index.html","hash":"e368c391630e28c6118ff24643df74aa12501447","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/index.html","hash":"299d526b84b3da38a45c11b48a84cbc41dd59334","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/index.html","hash":"aacaefec814d4b186b824bbf738eb9b6a2632c98","modified":1619329411260},{"_id":"public/archives/index.html","hash":"3a819ccc1db27ddd2fd03b0fdf05a69fac931283","modified":1619329411260},{"_id":"public/archives/2021/index.html","hash":"51473b4176bf785b60084f2a2c1d64ddf9ece202","modified":1619329411260},{"_id":"public/archives/2021/01/index.html","hash":"c555482a5216bfc93cae302125d92fa1278930a9","modified":1619329411260},{"_id":"public/archives/2021/02/index.html","hash":"da0b046964be0295def4065572099a99ec4565ab","modified":1619329411260},{"_id":"public/archives/2021/04/index.html","hash":"5a8538e7325144be15e7d5d42f20c6bf09a7ce86","modified":1619329411260},{"_id":"public/categories/Notes/index.html","hash":"6bdc56b7980ec0a56ac4151c6d0c4c8fcb330a83","modified":1619329411260},{"_id":"public/categories/Research/index.html","hash":"80d92f2328e7f9e5f4fdc0d8c5a279515fd8ff95","modified":1619329411260},{"_id":"public/index.html","hash":"f18a0c9e42107ea1210d4704bc51a8a3667dce25","modified":1619329411260},{"_id":"public/tags/Online-Course/index.html","hash":"aec6d578bcd179876a79db1584486109bf19e13c","modified":1619329411260},{"_id":"public/tags/Algorithm/index.html","hash":"34ea0d541582c2a6ace8c6c32445f8fca1db896a","modified":1619329411260},{"_id":"public/tags/Data-structure/index.html","hash":"4775dfef3dc5539a8894209aa9945392c115d3f8","modified":1619329411260},{"_id":"public/tags/Notes/index.html","hash":"21cce82f3b6ad88d158ddcbf11d0139a4f3f998c","modified":1619329411260},{"_id":"public/tags/Machine-Learning/index.html","hash":"30aae9c7550e99ab6cdc212c635c3c6cc6c348f7","modified":1619329411260},{"_id":"public/tags/Deep-Learning/index.html","hash":"bb2f6f842afefb4f50cd59d3be90202d08deb12c","modified":1619329411260},{"_id":"public/tags/Fluid-Mechanics/index.html","hash":"aa1445fd5b8a756963c983fd01e47a144bdeac3c","modified":1619329411260},{"_id":"public/tags/Partial-Differential-Equations/index.html","hash":"b9bd9509a13c50d0ecab2a1c99e086b16dbed098","modified":1619329411260},{"_id":"public/tags/Class-Project/index.html","hash":"1dcfac0de56cda7d3470c03e27d3818b2fe51728","modified":1619329411260},{"_id":"public/tags/Computer-Vision/index.html","hash":"05212754a71f60a8c995f3bebf5ab563720206a1","modified":1619329411260},{"_id":"public/tags/Survey/index.html","hash":"bfd9d2d7f20d578e80b444064cd7febdc18aa44d","modified":1619329411260},{"_id":"public/tags/Vision-transformer/index.html","hash":"5f2f0426e47f98397f5f8ac73ccbc60dd04c5936","modified":1619329411260},{"_id":"public/tags/Computer-vision/index.html","hash":"3611263ef682238badf3f722b83dbf3b0bc6db15","modified":1619329411260},{"_id":"public/tags/Course-project/index.html","hash":"779edcac00296ae0fbe8b024ae74f6d50bec6b2e","modified":1619329411260},{"_id":"public/favicon.png","hash":"d44008b0d6298287cdcfe744a2c8c562569f67ff","modified":1619329411260},{"_id":"public/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1619329411260},{"_id":"public/CNAME","hash":"5dff077b04ab5f4ebffd9b9d2b1d0811c1a796a8","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image001.png","hash":"0e13cb32e27691f2b64c82e5a01b6a566d555ee7","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image002.png","hash":"9abb1a82fb63b3da66b36125c91af33c941bcb63","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image007.png","hash":"5175998d2d23dc50914ae7360cc30e0854408566","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image005.png","hash":"dea126817372ad308a68e054dda964ee1f08572d","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image009.png","hash":"71961dc1647451676df0184ac001693b112b1e26","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image003.png","hash":"64250098098634ca8b4876edb7d8d94e72f3c07d","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image004.png","hash":"a3f243df8db455c891a5434c1d3e0fe3acd5e9bd","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image011.png","hash":"dc456312ae88d30a55d75d1d1a273ce0aea7c038","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image013.png","hash":"ba95703381ae371f34732381eb418a2088d65620","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image016.png","hash":"d8324302e6072cb5ebf229474d0e7d24e52ee76b","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image019.png","hash":"bf8d4b484a63e658343cac998e0095cecbac262c","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image023.png","hash":"28bc904909e227f40c1e3d1113f8dc8efbad3d18","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image025.png","hash":"7e72a1207eeffc4d9d39781f486fba701b1a8bc2","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image027.png","hash":"ecec8a2ca638cc1becd1f4d5aba156e7c74ed27c","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image029.png","hash":"690e6b8f9f178f51a31bdf8dcf33bda8ffd46b16","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image031.png","hash":"5f0dafa83089b1976b07733b2fd8c092d834608e","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image033.png","hash":"e27674f98bad707a921905b503c9fbf6185a4c48","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image034.png","hash":"6960d9d25a36b3e2c19ad6bf2cc1fcbe89ebdd47","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image038.png","hash":"1bef009b19dcf6cd30c48d6a7e20ce7c0eeb09eb","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image035.png","hash":"eeaad5325e0b2a5b9137e2d726224beb7045cf8d","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image040.png","hash":"cd478ee47a55a10429c82a682691ad01f93fbbd9","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image042.png","hash":"ebbbf27f816951c5b9337d76f02c6a07f0951bc6","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image044.png","hash":"a3c299e01669750baef8171cf7c9fb50bf960ffc","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image046.png","hash":"78b01fea1ab08599d564f6a849fa390417c68f12","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image048.png","hash":"38166a000ee6a5794cec4b73281682d511623b9e","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image051.png","hash":"0fd96824cf4901e9a1f7e48148a2d50f77d26097","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image055.png","hash":"2b44069f3eea7fbe6d3ca7da6e3769e97dac29f4","modified":1619329411260},{"_id":"public/weixin_favicon.png","hash":"4a8466bd7d8cf4753cab8fb68647b40b91a246ad","modified":1619329411260},{"_id":"public/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1619329411260},{"_id":"public/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1619329411260},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1619329411260},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1619329411260},{"_id":"public/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1619329411260},{"_id":"public/2021/04/24/CS61B-Week2/image-20210425113258793.png","hash":"6e04996ee4c28c81087ae0e8cd03091497d5245d","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174913278.png","hash":"12553f8cbd22b15068b4cf0bc2173c1f83c16c93","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image036.png","hash":"8e4fda135bebedfe89d7e345ba2199b3b7d5a65f","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image050.png","hash":"791f67513d849895843434534bdeefc382df896a","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image053.png","hash":"4160ce8f9d2eabf070cb2473fa277deda06126dd","modified":1619329411260},{"_id":"public/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image056.png","hash":"a78943be90d65cbfb63565aa43f1a10cb363353c","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image057.png","hash":"a10f2c36b0e93e183047abcc1dce5d72222f6d79","modified":1619329411260},{"_id":"public/css/style.css","hash":"55a917def994d7b8bd35785f6b3d147435bc0b88","modified":1619329411260},{"_id":"public/js/prettify.js","hash":"d24b1da342b5c2d0582f0922118aaf0b2a6840d5","modified":1619329411260},{"_id":"public/css/lib/prettify-tomorrow-night-eighties.css","hash":"35e07bd7a4585363060edd558a0e9939e7e68323","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214182520461.png","hash":"6fa531b36a88eef90f1aae1f264009a4f022ac8c","modified":1619329411260},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image054.png","hash":"d618ab0543d394ff056d59436aa16b0f7893baf8","modified":1619329411260},{"_id":"public/css/lib/font-awesome.min.css","hash":"683d12731b7429d32ec7de00a6706602e403013f","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174809740.png","hash":"55f75f26acb1919ea76e2a618dd71d6f0d2804fc","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214173356481.png","hash":"24868d2884527b8b00b7288fe2475d8f9a39bca2","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174422276.png","hash":"588366cb2e4d3ae87401f5972cccd294b4a55bb2","modified":1619329411260},{"_id":"public/css/font/font-awesome/fontawesome-webfont.svg","hash":"b06b5c8f67fd632cdc62a33b62ae4f74194131b3","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/1.png","hash":"2db678db0d744b6abf0148fc4f7690bcd61635c3","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/2.png","hash":"e27010e5c846311585587427d4dc5716696c0d57","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/4.png","hash":"be1010578082af53f248773a883ac875af904676","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214182334199.png","hash":"774c69ec91d50792fcf91cf68b5b80414fa6d5fc","modified":1619329411260},{"_id":"public/js/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214175022879.png","hash":"08cf0616efc761a32e5e38f9c01856eb1e14c714","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214175304898.png","hash":"54b725874e2e3cdb8a0f0819902048c955b05083","modified":1619329411260},{"_id":"public/css/lib/materialize.min.css","hash":"41d1676b73eec020eaeb9b507dcbcd30069ae8cb","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/5.png","hash":"918add3b718d520c1cc6b3c340dfaf58625e44b7","modified":1619329411260},{"_id":"public/js/materialize.min.js","hash":"c9308fbe808a149aa11061af40a4be5f391cccee","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214173840928.png","hash":"53edcce7af1fbaf7d59acf2062316d81e63f723e","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174108194.png","hash":"cd7d300b322101079d7f26615f1ca358931e9098","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/3.png","hash":"9a52e35f1b8c80691a5b0be4b1ab6a4f2d56a7f6","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174003005.png","hash":"c8c21d653e549d5835ad6ee49237cb6a7b0f60d4","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174957751.png","hash":"f07858cb57086c6becfbf436e569f90302fbb847","modified":1619329411260},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174310749.png","hash":"040595ce2e24cd22fab14a223baae6d4a217107f","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/7.png","hash":"cc84be16b5350a69e15af7aeb998acfd9c1fcd5d","modified":1619329411260},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/6.png","hash":"1e4661db20aebb2c3bc315f658beb41e9fe62893","modified":1619329411260}],"Category":[{"name":"Notes","_id":"cknwqx6rq0004bczw6onuhp7h"},{"name":"Research","_id":"cknwqx6s2000sbczwht4h1uys"}],"Data":[{"_id":"about","data":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]}},{"_id":"hint","data":{"new":{"selector":[".menu-about"]}}},{"_id":"link","data":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}}},{"_id":"reading","data":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}}},{"_id":"slider","data":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}],"Page":[{"title":"Xiaoyu Xie","layout":"about","_content":"\n<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n# Background\n\nI am Xiaoyu Xie, a first-year PhD student at Northwestern University. \n\n# Research Interests:\n\n- Computer Vision;\n- Deep Learning in Manufacturing, Mechanics, and Finace;\n- Bayesian Deep Learning;\n","source":"about/index.md","raw":"title: Xiaoyu Xie\nlayout: about\n---\n\n<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n# Background\n\nI am Xiaoyu Xie, a first-year PhD student at Northwestern University. \n\n# Research Interests:\n\n- Computer Vision;\n- Deep Learning in Manufacturing, Mechanics, and Finace;\n- Bayesian Deep Learning;\n","date":"2021-04-25T02:55:12.179Z","updated":"2021-04-25T02:55:12.153Z","path":"about/index.html","comments":1,"_id":"cknwqx6rh0000bczwgar9ba9s","content":"<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>I am Xiaoyu Xie, a first-year PhD student at Northwestern University. </p>\n<h1 id=\"Research-Interests\"><a href=\"#Research-Interests\" class=\"headerlink\" title=\"Research Interests:\"></a>Research Interests:</h1><ul>\n<li>Computer Vision;</li>\n<li>Deep Learning in Manufacturing, Mechanics, and Finace;</li>\n<li>Bayesian Deep Learning;</li>\n</ul>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":"<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>I am Xiaoyu Xie, a first-year PhD student at Northwestern University. </p>\n<h1 id=\"Research-Interests\"><a href=\"#Research-Interests\" class=\"headerlink\" title=\"Research Interests:\"></a>Research Interests:</h1><ul>\n<li>Computer Vision;</li>\n<li>Deep Learning in Manufacturing, Mechanics, and Finace;</li>\n<li>Bayesian Deep Learning;</li>\n</ul>\n"},{"title":"categories","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: \"categories\"\ntype: categories \nlayout: \"categories\"\n---\n","date":"2021-01-07T04:21:57.215Z","updated":"2021-01-07T04:21:57.207Z","path":"categories/index.html","comments":1,"_id":"cknwqx6ro0002bczw9khycwcy","content":"","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":""},{"title":"Reading","layout":"reading","_content":"\n","source":"reading/index.md","raw":"title: Reading\nlayout: reading\n---\n\n","date":"2021-01-05T11:27:50.020Z","updated":"2021-01-05T11:27:50.007Z","path":"reading/index.html","comments":1,"_id":"cknwqx6rs0006bczwgi96atka","content":"","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":""},{"title":"tags","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: \"tags\"\ntype: tags\nlayout: \"tags\"\n---","date":"2021-01-07T04:08:10.118Z","updated":"2021-01-07T04:08:10.112Z","path":"tags/index.html","comments":1,"_id":"cknwqx6rt0007bczwggov40wk","content":"","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":""}],"Post":[{"title":"CS61B-Week2-Notes","date":"2021-04-24T14:26:44.000Z","_content":"\n# Week 2 - Notes\n\n## Exercise B Level\n\n1. Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method `deleteFirst`, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.\n2. Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.\n\n<!-- more -->\n\n```java\n// Code for the question 1 and 2\n\nimport edu.princeton.cs.algs4.In;\n\n//import java.util.Arrays;\n\n// Using sentinel to replace the first\n\npublic class SLList2 {\n    private static class IntNode {\n        public int item;\n        public IntNode next;\n\n        public IntNode(int i, IntNode n) {\n            item = i;\n            next = n;\n        }\n    }\n\n    private IntNode sentinel;\n    private int size;\n\n    public SLList2() {\n        // initialize with no inputs\n        sentinel = new IntNode(63, null);\n        size = 1;\n    }\n\n    public SLList2(int x) {\n        // initialize with a integer\n        sentinel = new IntNode(63, null);\n        sentinel.next = new IntNode(x, null);\n        size = 1;\n    }\n\n    public SLList2(int[] x) {\n        // initialize with an array\n        size = 0;\n        sentinel = new IntNode(63, null);\n        for (int i = 0; i < x.length; i++) {\n            // get the item from array inversely\n            sentinel.next = new IntNode(x[x.length-i-1], null);\n            size += 1;\n        }\n    }\n\n    /** Add the first item in the list */\n    public void addFirst(int x) {\n        sentinel.next = new IntNode(x, sentinel.next);\n        size += 1;\n    }\n\n    /** Returns the first item in the list */\n    public int getFirst() {\n        return sentinel.next.item;\n    }\n\n    /**\n     * Returns the last item in the list\n     * @return the last item\n     */\n    public int getLast() {\n        if (sentinel.next == null) {\n            return sentinel.item;\n        }\n        sentinel = sentinel.next;\n        return getLast();\n    }\n\n    /**\n     * Add an item to a list\n     * @param args int x\n     */\n    public void addLast(int x) {\n        size += 1;\n        IntNode p = sentinel;\n\n        /* Advance p to the end of the list. */\n        while (p.next != null) {\n            p = p.next;\n        }\n        p.next = new IntNode(x, null);\n    }\n\n    public int size() {\n        return size;\n    }\n\n    public int deleteFirst() {\n        /* sentinel.next or sentinel.next.next\n        could be null when size == 0 */\n        if (sentinel.next == null) {\n            return -1;\n        }\n\n        IntNode deleteNode = sentinel.next;\n\n        if (sentinel.next.next == null) {\n            sentinel.next = new IntNode(-1, null);\n            return deleteNode.item;\n        }\n        sentinel.next = sentinel.next.next;\n        return deleteNode.item;\n    }\n\n    public static void main(String[] args) {\n        /** Test the constructor that takes in an array of integers*/\n        int[] arr = new int[]{1,2,3};\n        SLList2 L = new SLList2(arr);\n        System.out.println(L.getFirst());\n      \n        SLList2 L = new SLList2(1);\n        L.addFirst(2);\n        System.out.println(L.getFirst());\n        L.addFirst(3);\n        L.deleteFirst();\n        System.out.print(\"Final: \");\n        System.out.println(L.getFirst());\n//        L.addLast(100);\n//        System.out.println(L.getLast());\n//        System.out.println(L.size());\n    }\n}\n```\n\n# Exercise A Level\n\n![image-20210425113258793](CS61B-Week2/image-20210425113258793.png)\n\n[Problem Link: Osmosis](https://www.kartikkapur.com/documents/mt1.pdf#page=7)\n\n```java\npublic class IntList2 {\n    public int first;\n    public IntList2 rest;\n\n    public IntList2(int f, IntList2 r){\n        first = f;\n        rest = r;\n    }\n\n    // Iterative\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentIter(IntList2 p){\n        /* if p == null, p.rest will no longer execute */\n        if (p.rest == null) {\n            /* size <= 1 */\n            return;\n        }\n\n        /**\n         * p.rest != null\n         * p ends at the last node finally\n         * loop through 1st ~ last 2nd node\n         */\n        while (p.rest != null) { /* p ends at the last node */\n            if (p.first == p.rest.first) {\n                /* merge */\n                p.first *= 2;\n                p.rest = p.rest.rest; /* it's okay if it is null */\n            } else {\n                p = p.rest;\n            }\n        }\n    }\n\n    // recursion\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentRec(IntList2 p) {\n        if (p == null) return;\n        adj(p, p.rest);\n    }\n\n    // helper function - pass previous node recursively\n    private void adj(IntList2 prev, IntList2 current) {\n        if (current == null) return;\n        if (prev.first == current.first) {\n            prev.first *= 2;\n            prev.rest = current.rest; // maybe null\n            adj(prev, prev.rest); // I fixed this part that is wrong in the reference link.\n        } else {\n            adj(current, current.rest);\n        }\n    }\n\n    // Display an IntList\n    public void display(IntList2 L) {\n        while (L.rest != null) {\n            System.out.print(L.first);\n            System.out.print(\", \");\n            L.first = L.rest.first;\n            L.rest = L.rest.rest;\n        }\n        System.out.println(L.first);\n    }\n\n    public static void main(String[] args) {\n        IntList2 L = new IntList2(3, null);\n        L =new IntList2(2, L);\n        L =new IntList2(1, L);\n        L =new IntList2(1, L);\n\n      \t// There are two methods.\n        // Method 1: Recursive\n        L.addAdjacentRec(L);\n\n        // Method 2: Iterative\n//        L.addAdjacentIter(L);\n        System.out.print(\"Final: \");\n        L.display(L);\n    }\n}\n```\n\n# Reference:\n\n- [CS 61B | Part 1 | List (Linked List & Array List)](https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html)","source":"_posts/CS61B-Week2.md","raw":"---\ntitle: CS61B-Week2-Notes\ndate: 2021-04-24 22:26:44\ntags: \n- Online Course\n- Algorithm\n- Data structure\n- Notes\ncategories: Notes\n---\n\n# Week 2 - Notes\n\n## Exercise B Level\n\n1. Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method `deleteFirst`, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.\n2. Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.\n\n<!-- more -->\n\n```java\n// Code for the question 1 and 2\n\nimport edu.princeton.cs.algs4.In;\n\n//import java.util.Arrays;\n\n// Using sentinel to replace the first\n\npublic class SLList2 {\n    private static class IntNode {\n        public int item;\n        public IntNode next;\n\n        public IntNode(int i, IntNode n) {\n            item = i;\n            next = n;\n        }\n    }\n\n    private IntNode sentinel;\n    private int size;\n\n    public SLList2() {\n        // initialize with no inputs\n        sentinel = new IntNode(63, null);\n        size = 1;\n    }\n\n    public SLList2(int x) {\n        // initialize with a integer\n        sentinel = new IntNode(63, null);\n        sentinel.next = new IntNode(x, null);\n        size = 1;\n    }\n\n    public SLList2(int[] x) {\n        // initialize with an array\n        size = 0;\n        sentinel = new IntNode(63, null);\n        for (int i = 0; i < x.length; i++) {\n            // get the item from array inversely\n            sentinel.next = new IntNode(x[x.length-i-1], null);\n            size += 1;\n        }\n    }\n\n    /** Add the first item in the list */\n    public void addFirst(int x) {\n        sentinel.next = new IntNode(x, sentinel.next);\n        size += 1;\n    }\n\n    /** Returns the first item in the list */\n    public int getFirst() {\n        return sentinel.next.item;\n    }\n\n    /**\n     * Returns the last item in the list\n     * @return the last item\n     */\n    public int getLast() {\n        if (sentinel.next == null) {\n            return sentinel.item;\n        }\n        sentinel = sentinel.next;\n        return getLast();\n    }\n\n    /**\n     * Add an item to a list\n     * @param args int x\n     */\n    public void addLast(int x) {\n        size += 1;\n        IntNode p = sentinel;\n\n        /* Advance p to the end of the list. */\n        while (p.next != null) {\n            p = p.next;\n        }\n        p.next = new IntNode(x, null);\n    }\n\n    public int size() {\n        return size;\n    }\n\n    public int deleteFirst() {\n        /* sentinel.next or sentinel.next.next\n        could be null when size == 0 */\n        if (sentinel.next == null) {\n            return -1;\n        }\n\n        IntNode deleteNode = sentinel.next;\n\n        if (sentinel.next.next == null) {\n            sentinel.next = new IntNode(-1, null);\n            return deleteNode.item;\n        }\n        sentinel.next = sentinel.next.next;\n        return deleteNode.item;\n    }\n\n    public static void main(String[] args) {\n        /** Test the constructor that takes in an array of integers*/\n        int[] arr = new int[]{1,2,3};\n        SLList2 L = new SLList2(arr);\n        System.out.println(L.getFirst());\n      \n        SLList2 L = new SLList2(1);\n        L.addFirst(2);\n        System.out.println(L.getFirst());\n        L.addFirst(3);\n        L.deleteFirst();\n        System.out.print(\"Final: \");\n        System.out.println(L.getFirst());\n//        L.addLast(100);\n//        System.out.println(L.getLast());\n//        System.out.println(L.size());\n    }\n}\n```\n\n# Exercise A Level\n\n![image-20210425113258793](CS61B-Week2/image-20210425113258793.png)\n\n[Problem Link: Osmosis](https://www.kartikkapur.com/documents/mt1.pdf#page=7)\n\n```java\npublic class IntList2 {\n    public int first;\n    public IntList2 rest;\n\n    public IntList2(int f, IntList2 r){\n        first = f;\n        rest = r;\n    }\n\n    // Iterative\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentIter(IntList2 p){\n        /* if p == null, p.rest will no longer execute */\n        if (p.rest == null) {\n            /* size <= 1 */\n            return;\n        }\n\n        /**\n         * p.rest != null\n         * p ends at the last node finally\n         * loop through 1st ~ last 2nd node\n         */\n        while (p.rest != null) { /* p ends at the last node */\n            if (p.first == p.rest.first) {\n                /* merge */\n                p.first *= 2;\n                p.rest = p.rest.rest; /* it's okay if it is null */\n            } else {\n                p = p.rest;\n            }\n        }\n    }\n\n    // recursion\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentRec(IntList2 p) {\n        if (p == null) return;\n        adj(p, p.rest);\n    }\n\n    // helper function - pass previous node recursively\n    private void adj(IntList2 prev, IntList2 current) {\n        if (current == null) return;\n        if (prev.first == current.first) {\n            prev.first *= 2;\n            prev.rest = current.rest; // maybe null\n            adj(prev, prev.rest); // I fixed this part that is wrong in the reference link.\n        } else {\n            adj(current, current.rest);\n        }\n    }\n\n    // Display an IntList\n    public void display(IntList2 L) {\n        while (L.rest != null) {\n            System.out.print(L.first);\n            System.out.print(\", \");\n            L.first = L.rest.first;\n            L.rest = L.rest.rest;\n        }\n        System.out.println(L.first);\n    }\n\n    public static void main(String[] args) {\n        IntList2 L = new IntList2(3, null);\n        L =new IntList2(2, L);\n        L =new IntList2(1, L);\n        L =new IntList2(1, L);\n\n      \t// There are two methods.\n        // Method 1: Recursive\n        L.addAdjacentRec(L);\n\n        // Method 2: Iterative\n//        L.addAdjacentIter(L);\n        System.out.print(\"Final: \");\n        L.display(L);\n    }\n}\n```\n\n# Reference:\n\n- [CS 61B | Part 1 | List (Linked List & Array List)](https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html)","slug":"CS61B-Week2","published":1,"updated":"2021-04-25T04:25:08.496Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknwqx6rl0001bczw2h2d2tks","content":"<h1 id=\"Week-2-Notes\"><a href=\"#Week-2-Notes\" class=\"headerlink\" title=\"Week 2 - Notes\"></a>Week 2 - Notes</h1><h2 id=\"Exercise-B-Level\"><a href=\"#Exercise-B-Level\" class=\"headerlink\" title=\"Exercise B Level\"></a>Exercise B Level</h2><ol>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method <code>deleteFirst</code>, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.</li>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.</li>\n</ol>\n<a id=\"more\"></a>\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Code for the question 1 and 2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> edu.princeton.cs.algs4.In;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//import java.util.Arrays;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Using sentinel to replace the first</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SLList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntNode</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> item;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> IntNode next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntNode</span><span class=\"params\">(<span class=\"keyword\">int</span> i, IntNode n)</span> </span>&#123;</span><br><span class=\"line\">            item = i;</span><br><span class=\"line\">            next = n;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode sentinel;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> size;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with no inputs</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with a integer</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span>[] x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with an array</span></span><br><span class=\"line\">        size = <span class=\"number\">0</span>;</span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; x.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// get the item from array inversely</span></span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(x[x.length-i-<span class=\"number\">1</span>], <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Add the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addFirst</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, sentinel.next);</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Returns the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentinel.next.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns the last item in the list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> the last item</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getLast</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> sentinel.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel = sentinel.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> getLast();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Add an item to a list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> args int x</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addLast</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        IntNode p = sentinel;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Advance p to the end of the list. */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            p = p.next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        p.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">size</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> size;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">deleteFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* sentinel.next or sentinel.next.next</span></span><br><span class=\"line\"><span class=\"comment\">        could be null when size == 0 */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        IntNode deleteNode = sentinel.next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(-<span class=\"number\">1</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel.next = sentinel.next.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/** Test the constructor that takes in an array of integers*/</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span>[] arr = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[]&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>&#125;;</span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(arr);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">      </span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(<span class=\"number\">1</span>);</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">2</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">3</span>);</span><br><span class=\"line\">        L.deleteFirst();</span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\"><span class=\"comment\">//        L.addLast(100);</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.getLast());</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.size());</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Exercise-A-Level\"><a href=\"#Exercise-A-Level\" class=\"headerlink\" title=\"Exercise A Level\"></a>Exercise A Level</h1><p><img src=\"CS61B-Week2/image-20210425113258793.png\" alt=\"image-20210425113258793\"></p>\n<p><a href=\"https://www.kartikkapur.com/documents/mt1.pdf#page=7\">Problem Link: Osmosis</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> first;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> IntList2 rest;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntList2</span><span class=\"params\">(<span class=\"keyword\">int</span> f, IntList2 r)</span></span>&#123;</span><br><span class=\"line\">        first = f;</span><br><span class=\"line\">        rest = r;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Iterative</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentIter</span><span class=\"params\">(IntList2 p)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* if p == null, p.rest will no longer execute */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p.rest == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* size &lt;= 1 */</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * p.rest != null</span></span><br><span class=\"line\"><span class=\"comment\">         * p ends at the last node finally</span></span><br><span class=\"line\"><span class=\"comment\">         * loop through 1st ~ last 2nd node</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.rest != <span class=\"keyword\">null</span>) &#123; <span class=\"comment\">/* p ends at the last node */</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (p.first == p.rest.first) &#123;</span><br><span class=\"line\">                <span class=\"comment\">/* merge */</span></span><br><span class=\"line\">                p.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">                p.rest = p.rest.rest; <span class=\"comment\">/* it&#x27;s okay if it is null */</span></span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                p = p.rest;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// recursion</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentRec</span><span class=\"params\">(IntList2 p)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        adj(p, p.rest);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// helper function - pass previous node recursively</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">adj</span><span class=\"params\">(IntList2 prev, IntList2 current)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (current == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prev.first == current.first) &#123;</span><br><span class=\"line\">            prev.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">            prev.rest = current.rest; <span class=\"comment\">// maybe null</span></span><br><span class=\"line\">            adj(prev, prev.rest); <span class=\"comment\">// I fixed this part that is wrong in the reference link.</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            adj(current, current.rest);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Display an IntList</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">display</span><span class=\"params\">(IntList2 L)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (L.rest != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            System.out.print(L.first);</span><br><span class=\"line\">            System.out.print(<span class=\"string\">&quot;, &quot;</span>);</span><br><span class=\"line\">            L.first = L.rest.first;</span><br><span class=\"line\">            L.rest = L.rest.rest;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        System.out.println(L.first);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        IntList2 L = <span class=\"keyword\">new</span> IntList2(<span class=\"number\">3</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">2</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\"></span><br><span class=\"line\">      \t<span class=\"comment\">// There are two methods.</span></span><br><span class=\"line\">        <span class=\"comment\">// Method 1: Recursive</span></span><br><span class=\"line\">        L.addAdjacentRec(L);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Method 2: Iterative</span></span><br><span class=\"line\"><span class=\"comment\">//        L.addAdjacentIter(L);</span></span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        L.display(L);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h1><ul>\n<li><a href=\"https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\">CS 61B | Part 1 | List (Linked List &amp; Array List)</a></li>\n</ul>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<h1 id=\"Week-2-Notes\"><a href=\"#Week-2-Notes\" class=\"headerlink\" title=\"Week 2 - Notes\"></a>Week 2 - Notes</h1><h2 id=\"Exercise-B-Level\"><a href=\"#Exercise-B-Level\" class=\"headerlink\" title=\"Exercise B Level\"></a>Exercise B Level</h2><ol>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method <code>deleteFirst</code>, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.</li>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.</li>\n</ol>","more":"<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Code for the question 1 and 2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> edu.princeton.cs.algs4.In;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//import java.util.Arrays;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Using sentinel to replace the first</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SLList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntNode</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> item;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> IntNode next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntNode</span><span class=\"params\">(<span class=\"keyword\">int</span> i, IntNode n)</span> </span>&#123;</span><br><span class=\"line\">            item = i;</span><br><span class=\"line\">            next = n;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode sentinel;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> size;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with no inputs</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with a integer</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span>[] x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with an array</span></span><br><span class=\"line\">        size = <span class=\"number\">0</span>;</span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; x.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// get the item from array inversely</span></span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(x[x.length-i-<span class=\"number\">1</span>], <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Add the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addFirst</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, sentinel.next);</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Returns the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentinel.next.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns the last item in the list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> the last item</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getLast</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> sentinel.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel = sentinel.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> getLast();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Add an item to a list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> args int x</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addLast</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        IntNode p = sentinel;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Advance p to the end of the list. */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            p = p.next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        p.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">size</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> size;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">deleteFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* sentinel.next or sentinel.next.next</span></span><br><span class=\"line\"><span class=\"comment\">        could be null when size == 0 */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        IntNode deleteNode = sentinel.next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(-<span class=\"number\">1</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel.next = sentinel.next.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/** Test the constructor that takes in an array of integers*/</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span>[] arr = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[]&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>&#125;;</span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(arr);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">      </span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(<span class=\"number\">1</span>);</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">2</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">3</span>);</span><br><span class=\"line\">        L.deleteFirst();</span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\"><span class=\"comment\">//        L.addLast(100);</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.getLast());</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.size());</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Exercise-A-Level\"><a href=\"#Exercise-A-Level\" class=\"headerlink\" title=\"Exercise A Level\"></a>Exercise A Level</h1><p><img src=\"CS61B-Week2/image-20210425113258793.png\" alt=\"image-20210425113258793\"></p>\n<p><a href=\"https://www.kartikkapur.com/documents/mt1.pdf#page=7\">Problem Link: Osmosis</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> first;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> IntList2 rest;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntList2</span><span class=\"params\">(<span class=\"keyword\">int</span> f, IntList2 r)</span></span>&#123;</span><br><span class=\"line\">        first = f;</span><br><span class=\"line\">        rest = r;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Iterative</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentIter</span><span class=\"params\">(IntList2 p)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* if p == null, p.rest will no longer execute */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p.rest == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* size &lt;= 1 */</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * p.rest != null</span></span><br><span class=\"line\"><span class=\"comment\">         * p ends at the last node finally</span></span><br><span class=\"line\"><span class=\"comment\">         * loop through 1st ~ last 2nd node</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.rest != <span class=\"keyword\">null</span>) &#123; <span class=\"comment\">/* p ends at the last node */</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (p.first == p.rest.first) &#123;</span><br><span class=\"line\">                <span class=\"comment\">/* merge */</span></span><br><span class=\"line\">                p.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">                p.rest = p.rest.rest; <span class=\"comment\">/* it&#x27;s okay if it is null */</span></span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                p = p.rest;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// recursion</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentRec</span><span class=\"params\">(IntList2 p)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        adj(p, p.rest);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// helper function - pass previous node recursively</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">adj</span><span class=\"params\">(IntList2 prev, IntList2 current)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (current == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prev.first == current.first) &#123;</span><br><span class=\"line\">            prev.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">            prev.rest = current.rest; <span class=\"comment\">// maybe null</span></span><br><span class=\"line\">            adj(prev, prev.rest); <span class=\"comment\">// I fixed this part that is wrong in the reference link.</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            adj(current, current.rest);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Display an IntList</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">display</span><span class=\"params\">(IntList2 L)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (L.rest != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            System.out.print(L.first);</span><br><span class=\"line\">            System.out.print(<span class=\"string\">&quot;, &quot;</span>);</span><br><span class=\"line\">            L.first = L.rest.first;</span><br><span class=\"line\">            L.rest = L.rest.rest;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        System.out.println(L.first);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        IntList2 L = <span class=\"keyword\">new</span> IntList2(<span class=\"number\">3</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">2</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\"></span><br><span class=\"line\">      \t<span class=\"comment\">// There are two methods.</span></span><br><span class=\"line\">        <span class=\"comment\">// Method 1: Recursive</span></span><br><span class=\"line\">        L.addAdjacentRec(L);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Method 2: Iterative</span></span><br><span class=\"line\"><span class=\"comment\">//        L.addAdjacentIter(L);</span></span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        L.display(L);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h1><ul>\n<li><a href=\"https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\">CS 61B | Part 1 | List (Linked List &amp; Array List)</a></li>\n</ul>"},{"title":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers","date":"2021-02-17T03:38:03.000Z","_content":"\n# Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\n\nRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.\n\nNote that please check the lastest verison in [github](https://github.com/xiaoyuxie-vico/ML_PDE_Resources).\n\n<!-- more -->\n\n# Model Zoo\n\n| Model                   | Relevant Papers                                              | Link                                                         | Notes                 |\n| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |\n| HiDeNN                  | Saha, Sourav, et al. \"**Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.**\" Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452. | [Paper](https://www.sciencedirect.com/science/article/pii/S004578252030637X) |                       |\n| HiTSs                   | Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. \"**Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.**\" arXiv preprint arXiv:2008.09768 (2020). | [Paper](http://arxiv.org/abs/2102.01010), [Code](https://github.com/luckystarufo/multiscale_HiTS), [Video](https://www.youtube.com/watch?v=Jfl3dIlSTrU) |                       |\n|                         | Kochkov, Dmitrii, et al. \"**Machine learning accelerated computational fluid dynamics.**\" arXiv preprint arXiv:2102.01010 (2021). | [Paper](http://arxiv.org/abs/2102.01010)                     | Google                |\n| Fourier Neural Operator | Li, Zongyi, et al. \"**Fourier neural operator for parametric partial differential equations.**\" arXiv preprint arXiv:2010.08895 (2020). | [Paper](https://arxiv.org/abs/2010.08895), [Code](https://github.com/zongyi-li/fourier_neural_operator), [Video](https://www.youtube.com/watch?v=IaS72aHrJKE) |                       |\n| PINNs                   | Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"**Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.**\" Journal of Computational Physics 378 (2019): 686-707; | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ), [Code](https://github.com/maziarraissi/PINNs), [Video](https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA) |                       |\n|                         | Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"**Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.**\" *Journal of Fluid Mechanics* 807 (2016): 155-166. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB), [Code](https://github.com/tbnn/tbnn) | Pure data             |\n|                         | K. Duraisamy, G. Iaccarino, and H. Xiao, **Turbulence modeling in the age of data**, Annual Review of Fluid Mechanics 51, 357 (2019). | [Paper](https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs) | Pure data, Review     |\n|                         | Maulik, Romit, et al. \"**Subgrid modelling for two-dimensional turbulence using neural networks.**\" *Journal of Fluid Mechanics* 858 (2019): 122-144. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1) |                       |\n|                         | Beck, Andrea, David Flad, and Claus-Dieter Munz. \"**Deep neural networks for data-driven LES closure models.**\" *Journal of Computational Physics* 398 (2019): 108910. | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ) |                       |\n|                         | Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. \"**Deep learning for universal linear embeddings of nonlinear dynamics.**\" *Nature communications* 9.1 (2018): 1-10. | [Paper](https://www.nature.com/articles/s41467-018-07210-0)  | Nature communications |\n|                         | Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. \"**DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).**\" *arXiv preprint arXiv:1911.09145* (2019). | [Paper](https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145) |                       |\n|                         | Um, Kiwon, et al. \"**Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.**\" *arXiv preprint arXiv:2007.00016* (2020). | [Paper](https://arxiv.org/abs/2007.00016)                    |                       |\n\n# Videos\n- [2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations](https://www.youtube.com/watch?v=2Ab-8xTI89c)\n- [Steve Brunton: Machine Learning for Fluid Dynamics](https://www.youtube.com/watch?v=20vB4MzAbCwv)\n- [Petros Koumoutsakos: \"Machine Learning for Fluid Mechanics\"](https://www.youtube.com/watch?v=gv20VsKqgpc)\n\n# Blogs\n- [Fourier Neural Operator](https://zongyi-li.github.io/blog/2020/fourier-pde/)\n\n# Research Groups\n- [Brunton Lab: Data-driven dynamics and control](https://www.eigensteve.com/)\n- [Animashree Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/group.html)\n- [Wing Kam Liu Group](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html)\n\n# Contact\nIf you like, please star or fork.\n\nWelcome any comments or feedbacks!\n\nEmail: xiaoyuxie2020@u.northwestern.edu\n\n\n\n","source":"_posts/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers.md","raw":"---\ntitle: >-\n  Machine Learning / Deep Learning for Partial Differential Equations (PDEs)\n  Solvers\ndate: 2021-02-17 11:38:03\ncategories: Notes\ntags: \n\t- Machine Learning\n\t- Deep Learning\n\t- Fluid Mechanics\n\t- Partial Differential Equations\n---\n\n# Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\n\nRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.\n\nNote that please check the lastest verison in [github](https://github.com/xiaoyuxie-vico/ML_PDE_Resources).\n\n<!-- more -->\n\n# Model Zoo\n\n| Model                   | Relevant Papers                                              | Link                                                         | Notes                 |\n| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |\n| HiDeNN                  | Saha, Sourav, et al. \"**Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.**\" Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452. | [Paper](https://www.sciencedirect.com/science/article/pii/S004578252030637X) |                       |\n| HiTSs                   | Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. \"**Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.**\" arXiv preprint arXiv:2008.09768 (2020). | [Paper](http://arxiv.org/abs/2102.01010), [Code](https://github.com/luckystarufo/multiscale_HiTS), [Video](https://www.youtube.com/watch?v=Jfl3dIlSTrU) |                       |\n|                         | Kochkov, Dmitrii, et al. \"**Machine learning accelerated computational fluid dynamics.**\" arXiv preprint arXiv:2102.01010 (2021). | [Paper](http://arxiv.org/abs/2102.01010)                     | Google                |\n| Fourier Neural Operator | Li, Zongyi, et al. \"**Fourier neural operator for parametric partial differential equations.**\" arXiv preprint arXiv:2010.08895 (2020). | [Paper](https://arxiv.org/abs/2010.08895), [Code](https://github.com/zongyi-li/fourier_neural_operator), [Video](https://www.youtube.com/watch?v=IaS72aHrJKE) |                       |\n| PINNs                   | Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"**Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.**\" Journal of Computational Physics 378 (2019): 686-707; | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ), [Code](https://github.com/maziarraissi/PINNs), [Video](https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA) |                       |\n|                         | Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"**Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.**\" *Journal of Fluid Mechanics* 807 (2016): 155-166. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB), [Code](https://github.com/tbnn/tbnn) | Pure data             |\n|                         | K. Duraisamy, G. Iaccarino, and H. Xiao, **Turbulence modeling in the age of data**, Annual Review of Fluid Mechanics 51, 357 (2019). | [Paper](https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs) | Pure data, Review     |\n|                         | Maulik, Romit, et al. \"**Subgrid modelling for two-dimensional turbulence using neural networks.**\" *Journal of Fluid Mechanics* 858 (2019): 122-144. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1) |                       |\n|                         | Beck, Andrea, David Flad, and Claus-Dieter Munz. \"**Deep neural networks for data-driven LES closure models.**\" *Journal of Computational Physics* 398 (2019): 108910. | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ) |                       |\n|                         | Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. \"**Deep learning for universal linear embeddings of nonlinear dynamics.**\" *Nature communications* 9.1 (2018): 1-10. | [Paper](https://www.nature.com/articles/s41467-018-07210-0)  | Nature communications |\n|                         | Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. \"**DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).**\" *arXiv preprint arXiv:1911.09145* (2019). | [Paper](https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145) |                       |\n|                         | Um, Kiwon, et al. \"**Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.**\" *arXiv preprint arXiv:2007.00016* (2020). | [Paper](https://arxiv.org/abs/2007.00016)                    |                       |\n\n# Videos\n- [2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations](https://www.youtube.com/watch?v=2Ab-8xTI89c)\n- [Steve Brunton: Machine Learning for Fluid Dynamics](https://www.youtube.com/watch?v=20vB4MzAbCwv)\n- [Petros Koumoutsakos: \"Machine Learning for Fluid Mechanics\"](https://www.youtube.com/watch?v=gv20VsKqgpc)\n\n# Blogs\n- [Fourier Neural Operator](https://zongyi-li.github.io/blog/2020/fourier-pde/)\n\n# Research Groups\n- [Brunton Lab: Data-driven dynamics and control](https://www.eigensteve.com/)\n- [Animashree Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/group.html)\n- [Wing Kam Liu Group](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html)\n\n# Contact\nIf you like, please star or fork.\n\nWelcome any comments or feedbacks!\n\nEmail: xiaoyuxie2020@u.northwestern.edu\n\n\n\n","slug":"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers","published":1,"updated":"2021-02-17T13:13:40.346Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknwqx6ro0003bczw232x0w2f","content":"<h1 id=\"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\"><a href=\"#Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\" class=\"headerlink\" title=\"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\"></a>Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers</h1><p>Recently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.</p>\n<p>Note that please check the lastest verison in <a href=\"https://github.com/xiaoyuxie-vico/ML_PDE_Resources\">github</a>.</p>\n<a id=\"more\"></a>\n\n<h1 id=\"Model-Zoo\"><a href=\"#Model-Zoo\" class=\"headerlink\" title=\"Model Zoo\"></a>Model Zoo</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Relevant Papers</th>\n<th>Link</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HiDeNN</td>\n<td>Saha, Sourav, et al. “<strong>Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.</strong>“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S004578252030637X\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td>HiTSs</td>\n<td>Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “<strong>Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.</strong>“ arXiv preprint arXiv:2008.09768 (2020).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a>, <a href=\"https://github.com/luckystarufo/multiscale_HiTS\">Code</a>, <a href=\"https://www.youtube.com/watch?v=Jfl3dIlSTrU\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Kochkov, Dmitrii, et al. “<strong>Machine learning accelerated computational fluid dynamics.</strong>“ arXiv preprint arXiv:2102.01010 (2021).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a></td>\n<td>Google</td>\n</tr>\n<tr>\n<td>Fourier Neural Operator</td>\n<td>Li, Zongyi, et al. “<strong>Fourier neural operator for parametric partial differential equations.</strong>“ arXiv preprint arXiv:2010.08895 (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2010.08895\">Paper</a>, <a href=\"https://github.com/zongyi-li/fourier_neural_operator\">Code</a>, <a href=\"https://www.youtube.com/watch?v=IaS72aHrJKE\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td>PINNs</td>\n<td>Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “<strong>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</strong>“ Journal of Computational Physics 378 (2019): 686-707;</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ\">Paper</a>, <a href=\"https://github.com/maziarraissi/PINNs\">Code</a>, <a href=\"https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “<strong>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.</strong>“ <em>Journal of Fluid Mechanics</em> 807 (2016): 155-166.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB\">Paper</a>, <a href=\"https://github.com/tbnn/tbnn\">Code</a></td>\n<td>Pure data</td>\n</tr>\n<tr>\n<td></td>\n<td>K. Duraisamy, G. Iaccarino, and H. Xiao, <strong>Turbulence modeling in the age of data</strong>, Annual Review of Fluid Mechanics 51, 357 (2019).</td>\n<td><a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs\">Paper</a></td>\n<td>Pure data, Review</td>\n</tr>\n<tr>\n<td></td>\n<td>Maulik, Romit, et al. “<strong>Subgrid modelling for two-dimensional turbulence using neural networks.</strong>“ <em>Journal of Fluid Mechanics</em> 858 (2019): 122-144.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Beck, Andrea, David Flad, and Claus-Dieter Munz. “<strong>Deep neural networks for data-driven LES closure models.</strong>“ <em>Journal of Computational Physics</em> 398 (2019): 108910.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “<strong>Deep learning for universal linear embeddings of nonlinear dynamics.</strong>“ <em>Nature communications</em> 9.1 (2018): 1-10.</td>\n<td><a href=\"https://www.nature.com/articles/s41467-018-07210-0\">Paper</a></td>\n<td>Nature communications</td>\n</tr>\n<tr>\n<td></td>\n<td>Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “<strong>DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).</strong>“ <em>arXiv preprint arXiv:1911.09145</em> (2019).</td>\n<td><a href=\"https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Um, Kiwon, et al. “<strong>Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.</strong>“ <em>arXiv preprint arXiv:2007.00016</em> (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2007.00016\">Paper</a></td>\n<td></td>\n</tr>\n</tbody></table>\n<h1 id=\"Videos\"><a href=\"#Videos\" class=\"headerlink\" title=\"Videos\"></a>Videos</h1><ul>\n<li><a href=\"https://www.youtube.com/watch?v=2Ab-8xTI89c\">2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=20vB4MzAbCwv\">Steve Brunton: Machine Learning for Fluid Dynamics</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=gv20VsKqgpc\">Petros Koumoutsakos: “Machine Learning for Fluid Mechanics”</a></li>\n</ul>\n<h1 id=\"Blogs\"><a href=\"#Blogs\" class=\"headerlink\" title=\"Blogs\"></a>Blogs</h1><ul>\n<li><a href=\"https://zongyi-li.github.io/blog/2020/fourier-pde/\">Fourier Neural Operator</a></li>\n</ul>\n<h1 id=\"Research-Groups\"><a href=\"#Research-Groups\" class=\"headerlink\" title=\"Research Groups\"></a>Research Groups</h1><ul>\n<li><a href=\"https://www.eigensteve.com/\">Brunton Lab: Data-driven dynamics and control</a></li>\n<li><a href=\"http://tensorlab.cms.caltech.edu/users/anima/group.html\">Animashree Anandkumar</a></li>\n<li><a href=\"https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html\">Wing Kam Liu Group</a></li>\n</ul>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>If you like, please star or fork.</p>\n<p>Welcome any comments or feedbacks!</p>\n<p>Email: <a href=\"mailto:&#120;&#105;&#x61;&#111;&#121;&#x75;&#x78;&#x69;&#x65;&#50;&#48;&#x32;&#x30;&#x40;&#117;&#x2e;&#x6e;&#111;&#114;&#116;&#104;&#119;&#101;&#x73;&#x74;&#101;&#x72;&#x6e;&#46;&#101;&#100;&#x75;\">&#120;&#105;&#x61;&#111;&#121;&#x75;&#x78;&#x69;&#x65;&#50;&#48;&#x32;&#x30;&#x40;&#117;&#x2e;&#x6e;&#111;&#114;&#116;&#104;&#119;&#101;&#x73;&#x74;&#101;&#x72;&#x6e;&#46;&#101;&#100;&#x75;</a></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<h1 id=\"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\"><a href=\"#Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\" class=\"headerlink\" title=\"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\"></a>Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers</h1><p>Recently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.</p>\n<p>Note that please check the lastest verison in <a href=\"https://github.com/xiaoyuxie-vico/ML_PDE_Resources\">github</a>.</p>","more":"<h1 id=\"Model-Zoo\"><a href=\"#Model-Zoo\" class=\"headerlink\" title=\"Model Zoo\"></a>Model Zoo</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Relevant Papers</th>\n<th>Link</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HiDeNN</td>\n<td>Saha, Sourav, et al. “<strong>Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.</strong>“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S004578252030637X\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td>HiTSs</td>\n<td>Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “<strong>Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.</strong>“ arXiv preprint arXiv:2008.09768 (2020).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a>, <a href=\"https://github.com/luckystarufo/multiscale_HiTS\">Code</a>, <a href=\"https://www.youtube.com/watch?v=Jfl3dIlSTrU\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Kochkov, Dmitrii, et al. “<strong>Machine learning accelerated computational fluid dynamics.</strong>“ arXiv preprint arXiv:2102.01010 (2021).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a></td>\n<td>Google</td>\n</tr>\n<tr>\n<td>Fourier Neural Operator</td>\n<td>Li, Zongyi, et al. “<strong>Fourier neural operator for parametric partial differential equations.</strong>“ arXiv preprint arXiv:2010.08895 (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2010.08895\">Paper</a>, <a href=\"https://github.com/zongyi-li/fourier_neural_operator\">Code</a>, <a href=\"https://www.youtube.com/watch?v=IaS72aHrJKE\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td>PINNs</td>\n<td>Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “<strong>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</strong>“ Journal of Computational Physics 378 (2019): 686-707;</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ\">Paper</a>, <a href=\"https://github.com/maziarraissi/PINNs\">Code</a>, <a href=\"https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “<strong>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.</strong>“ <em>Journal of Fluid Mechanics</em> 807 (2016): 155-166.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB\">Paper</a>, <a href=\"https://github.com/tbnn/tbnn\">Code</a></td>\n<td>Pure data</td>\n</tr>\n<tr>\n<td></td>\n<td>K. Duraisamy, G. Iaccarino, and H. Xiao, <strong>Turbulence modeling in the age of data</strong>, Annual Review of Fluid Mechanics 51, 357 (2019).</td>\n<td><a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs\">Paper</a></td>\n<td>Pure data, Review</td>\n</tr>\n<tr>\n<td></td>\n<td>Maulik, Romit, et al. “<strong>Subgrid modelling for two-dimensional turbulence using neural networks.</strong>“ <em>Journal of Fluid Mechanics</em> 858 (2019): 122-144.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Beck, Andrea, David Flad, and Claus-Dieter Munz. “<strong>Deep neural networks for data-driven LES closure models.</strong>“ <em>Journal of Computational Physics</em> 398 (2019): 108910.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “<strong>Deep learning for universal linear embeddings of nonlinear dynamics.</strong>“ <em>Nature communications</em> 9.1 (2018): 1-10.</td>\n<td><a href=\"https://www.nature.com/articles/s41467-018-07210-0\">Paper</a></td>\n<td>Nature communications</td>\n</tr>\n<tr>\n<td></td>\n<td>Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “<strong>DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).</strong>“ <em>arXiv preprint arXiv:1911.09145</em> (2019).</td>\n<td><a href=\"https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Um, Kiwon, et al. “<strong>Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.</strong>“ <em>arXiv preprint arXiv:2007.00016</em> (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2007.00016\">Paper</a></td>\n<td></td>\n</tr>\n</tbody></table>\n<h1 id=\"Videos\"><a href=\"#Videos\" class=\"headerlink\" title=\"Videos\"></a>Videos</h1><ul>\n<li><a href=\"https://www.youtube.com/watch?v=2Ab-8xTI89c\">2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=20vB4MzAbCwv\">Steve Brunton: Machine Learning for Fluid Dynamics</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=gv20VsKqgpc\">Petros Koumoutsakos: “Machine Learning for Fluid Mechanics”</a></li>\n</ul>\n<h1 id=\"Blogs\"><a href=\"#Blogs\" class=\"headerlink\" title=\"Blogs\"></a>Blogs</h1><ul>\n<li><a href=\"https://zongyi-li.github.io/blog/2020/fourier-pde/\">Fourier Neural Operator</a></li>\n</ul>\n<h1 id=\"Research-Groups\"><a href=\"#Research-Groups\" class=\"headerlink\" title=\"Research Groups\"></a>Research Groups</h1><ul>\n<li><a href=\"https://www.eigensteve.com/\">Brunton Lab: Data-driven dynamics and control</a></li>\n<li><a href=\"http://tensorlab.cms.caltech.edu/users/anima/group.html\">Animashree Anandkumar</a></li>\n<li><a href=\"https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html\">Wing Kam Liu Group</a></li>\n</ul>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>If you like, please star or fork.</p>\n<p>Welcome any comments or feedbacks!</p>\n<p>Email: <a href=\"mailto:&#120;&#105;&#x61;&#111;&#121;&#x75;&#x78;&#x69;&#x65;&#50;&#48;&#x32;&#x30;&#x40;&#117;&#x2e;&#x6e;&#111;&#114;&#116;&#104;&#119;&#101;&#x73;&#x74;&#101;&#x72;&#x6e;&#46;&#101;&#100;&#x75;\">&#120;&#105;&#x61;&#111;&#121;&#x75;&#x78;&#x69;&#x65;&#50;&#48;&#x32;&#x30;&#x40;&#117;&#x2e;&#x6e;&#111;&#114;&#116;&#104;&#119;&#101;&#x73;&#x74;&#101;&#x72;&#x6e;&#46;&#101;&#100;&#x75;</a></p>"},{"title":"Robust and Explainable Image Classification Based on Logits Kernel Density Estimation","date":"2021-01-05T10:51:00.000Z","_content":"\n# Project Description\n\nFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.\n\n<!-- more -->\n\nThere are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.\n\n# Dataset\n\nThe cat and dog dataset used in this project is downloaded from [Kaggel](https://www.kaggle.com/tongpython/cat-and-dog). Several images are shown in the below:\n\nThe data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.\n\nThe batch size is 32 and the dataset will be shuﬄed.\n\n# Model Training\n\nDetailed information about model training can be found in [trainer_cat_dog.ipynb](https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb).\n\nThe accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.\n\n# Error analysis (test set)\n\nAlthought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.\n\n![](1.png)\n\nTrue label is dog, but predicted label is cat and the score is high.\n\n![](2.png)\n\nThe Grad-CAM results for these wrong predicted images are:\n\n![](3.png)\n\nWe can ﬁnd that: \n\n1.  Apparent misclassiﬁcations tend to have a larger attention area; \n2. Understandable misclassiﬁcations tend to have a smaller attention area;\n\n# Distribution analysis\n\nTo solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.\n\nBelow, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.\n\n![](4.png)\n\nKernel density estimation for logits is:\n\n![](5.png)\n\n# Analyze the robustness of classiﬁcation using logits kernel density estimation\n\nIt is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.\n\nFor the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.\n\nIf we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not \"seen\" these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.\n\n![](6.png)\n\nIn the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.\n\n![](7.png)\n\n# Conclusion\n- The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; \n- The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; \n- The proposed approach is explainable and can be understand visually;\n\n# More resources\n- [Github](https://github.com/xiaoyuxie-vico/ExplainableAI)\n- [Youtube Explanation](https://youtu.be/cIIOdQHTQu4)\n\nNote that this project is my final project for EE475 at Northwestern University in 2020 Fall.\n","source":"_posts/Robust-Image-Classification-with-Kernel-Density-Function.md","raw":"---\ntitle: Robust and Explainable Image Classification Based on Logits Kernel Density Estimation\ndate: 2021-01-05 18:51:00\ncategories: Research\ntags: \n\t- Class Project\n\t- Deep Learning\n\t- Computer Vision\n---\n\n# Project Description\n\nFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.\n\n<!-- more -->\n\nThere are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.\n\n# Dataset\n\nThe cat and dog dataset used in this project is downloaded from [Kaggel](https://www.kaggle.com/tongpython/cat-and-dog). Several images are shown in the below:\n\nThe data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.\n\nThe batch size is 32 and the dataset will be shuﬄed.\n\n# Model Training\n\nDetailed information about model training can be found in [trainer_cat_dog.ipynb](https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb).\n\nThe accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.\n\n# Error analysis (test set)\n\nAlthought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.\n\n![](1.png)\n\nTrue label is dog, but predicted label is cat and the score is high.\n\n![](2.png)\n\nThe Grad-CAM results for these wrong predicted images are:\n\n![](3.png)\n\nWe can ﬁnd that: \n\n1.  Apparent misclassiﬁcations tend to have a larger attention area; \n2. Understandable misclassiﬁcations tend to have a smaller attention area;\n\n# Distribution analysis\n\nTo solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.\n\nBelow, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.\n\n![](4.png)\n\nKernel density estimation for logits is:\n\n![](5.png)\n\n# Analyze the robustness of classiﬁcation using logits kernel density estimation\n\nIt is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.\n\nFor the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.\n\nIf we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not \"seen\" these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.\n\n![](6.png)\n\nIn the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.\n\n![](7.png)\n\n# Conclusion\n- The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; \n- The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; \n- The proposed approach is explainable and can be understand visually;\n\n# More resources\n- [Github](https://github.com/xiaoyuxie-vico/ExplainableAI)\n- [Youtube Explanation](https://youtu.be/cIIOdQHTQu4)\n\nNote that this project is my final project for EE475 at Northwestern University in 2020 Fall.\n","slug":"Robust-Image-Classification-with-Kernel-Density-Function","published":1,"updated":"2021-01-07T04:16:56.724Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknwqx6s0000qbczw7th6704d","content":"<h1 id=\"Project-Description\"><a href=\"#Project-Description\" class=\"headerlink\" title=\"Project Description\"></a>Project Description</h1><p>For a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.</p>\n<a id=\"more\"></a>\n\n<p>There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.</p>\n<h1 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset\"></a>Dataset</h1><p>The cat and dog dataset used in this project is downloaded from <a href=\"https://www.kaggle.com/tongpython/cat-and-dog\">Kaggel</a>. Several images are shown in the below:</p>\n<p>The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.</p>\n<p>The batch size is 32 and the dataset will be shuﬄed.</p>\n<h1 id=\"Model-Training\"><a href=\"#Model-Training\" class=\"headerlink\" title=\"Model Training\"></a>Model Training</h1><p>Detailed information about model training can be found in <a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb\">trainer_cat_dog.ipynb</a>.</p>\n<p>The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.</p>\n<h1 id=\"Error-analysis-test-set\"><a href=\"#Error-analysis-test-set\" class=\"headerlink\" title=\"Error analysis (test set)\"></a>Error analysis (test set)</h1><p>Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.</p>\n<p><img src=\"1.png\"></p>\n<p>True label is dog, but predicted label is cat and the score is high.</p>\n<p><img src=\"2.png\"></p>\n<p>The Grad-CAM results for these wrong predicted images are:</p>\n<p><img src=\"3.png\"></p>\n<p>We can ﬁnd that: </p>\n<ol>\n<li> Apparent misclassiﬁcations tend to have a larger attention area; </li>\n<li>Understandable misclassiﬁcations tend to have a smaller attention area;</li>\n</ol>\n<h1 id=\"Distribution-analysis\"><a href=\"#Distribution-analysis\" class=\"headerlink\" title=\"Distribution analysis\"></a>Distribution analysis</h1><p>To solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.</p>\n<p>Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.</p>\n<p><img src=\"4.png\"></p>\n<p>Kernel density estimation for logits is:</p>\n<p><img src=\"5.png\"></p>\n<h1 id=\"Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\"><a href=\"#Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\" class=\"headerlink\" title=\"Analyze the robustness of classiﬁcation using logits kernel density estimation\"></a>Analyze the robustness of classiﬁcation using logits kernel density estimation</h1><p>It is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.</p>\n<p>For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.</p>\n<p>If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.</p>\n<p><img src=\"6.png\"></p>\n<p>In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.</p>\n<p><img src=\"7.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><ul>\n<li>The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; </li>\n<li>The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; </li>\n<li>The proposed approach is explainable and can be understand visually;</li>\n</ul>\n<h1 id=\"More-resources\"><a href=\"#More-resources\" class=\"headerlink\" title=\"More resources\"></a>More resources</h1><ul>\n<li><a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI\">Github</a></li>\n<li><a href=\"https://youtu.be/cIIOdQHTQu4\">Youtube Explanation</a></li>\n</ul>\n<p>Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.</p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<h1 id=\"Project-Description\"><a href=\"#Project-Description\" class=\"headerlink\" title=\"Project Description\"></a>Project Description</h1><p>For a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.</p>","more":"<p>There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.</p>\n<h1 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset\"></a>Dataset</h1><p>The cat and dog dataset used in this project is downloaded from <a href=\"https://www.kaggle.com/tongpython/cat-and-dog\">Kaggel</a>. Several images are shown in the below:</p>\n<p>The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.</p>\n<p>The batch size is 32 and the dataset will be shuﬄed.</p>\n<h1 id=\"Model-Training\"><a href=\"#Model-Training\" class=\"headerlink\" title=\"Model Training\"></a>Model Training</h1><p>Detailed information about model training can be found in <a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb\">trainer_cat_dog.ipynb</a>.</p>\n<p>The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.</p>\n<h1 id=\"Error-analysis-test-set\"><a href=\"#Error-analysis-test-set\" class=\"headerlink\" title=\"Error analysis (test set)\"></a>Error analysis (test set)</h1><p>Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.</p>\n<p><img src=\"1.png\"></p>\n<p>True label is dog, but predicted label is cat and the score is high.</p>\n<p><img src=\"2.png\"></p>\n<p>The Grad-CAM results for these wrong predicted images are:</p>\n<p><img src=\"3.png\"></p>\n<p>We can ﬁnd that: </p>\n<ol>\n<li> Apparent misclassiﬁcations tend to have a larger attention area; </li>\n<li>Understandable misclassiﬁcations tend to have a smaller attention area;</li>\n</ol>\n<h1 id=\"Distribution-analysis\"><a href=\"#Distribution-analysis\" class=\"headerlink\" title=\"Distribution analysis\"></a>Distribution analysis</h1><p>To solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.</p>\n<p>Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.</p>\n<p><img src=\"4.png\"></p>\n<p>Kernel density estimation for logits is:</p>\n<p><img src=\"5.png\"></p>\n<h1 id=\"Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\"><a href=\"#Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\" class=\"headerlink\" title=\"Analyze the robustness of classiﬁcation using logits kernel density estimation\"></a>Analyze the robustness of classiﬁcation using logits kernel density estimation</h1><p>It is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.</p>\n<p>For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.</p>\n<p>If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.</p>\n<p><img src=\"6.png\"></p>\n<p>In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.</p>\n<p><img src=\"7.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><ul>\n<li>The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; </li>\n<li>The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; </li>\n<li>The proposed approach is explainable and can be understand visually;</li>\n</ul>\n<h1 id=\"More-resources\"><a href=\"#More-resources\" class=\"headerlink\" title=\"More resources\"></a>More resources</h1><ul>\n<li><a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI\">Github</a></li>\n<li><a href=\"https://youtu.be/cIIOdQHTQu4\">Youtube Explanation</a></li>\n</ul>\n<p>Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.</p>"},{"title":"Applications of Machine Learning for Fluid Mechanics","date":"2021-02-14T12:50:17.000Z","_content":"\nThis blog is the notes for a video called \"[Machine Learning for Fluid Mechanics](https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s)\", which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. \"Machine learning for fluid mechanics.\" Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.\n\n<!-- more -->\n\n# What is Machine Learning (ML)?\n\nML:\n\n- Models from Data via Optimization\n\n> Any sufficiently advanced technology is indistinguishable from magic.\n>\n> -- Arthur C. Clarke\n\nFluid dynamics tasks:\n\n- Reduction\n- Modeling\n- Control\n- Sensing\n- Closure\n\nOptimization problems:\n\n- High-dimensional\n- Non-linear\n- Non-convex\n- Multiscale\n\n# What kind of ML is needed in science and engineering?\n\nWe need Interpretable and Generalizable Machine Learning in science and engineering field.\n\n> Everything should be made as simple as possible, but not simpler.\n>\n> -- Albert Einstein\n\nHow to build a model like $F=ma$?\n\nFeatures for ML in science and engineering:\n\n- Sparse\n- Low-dimensional\n- Robust\n\n# Schematic: ML + CFD\n\n![Schematic](image-20210214173356481.png)\n\n\n\n# Why ML could work?\n\nBecause patterns exist in fluid flow.\n\n![pattern_FM](image-20210214173840928.png)\n\n# Applications\n\n## Fluid flow decomposition\n\nPCA (Shallow, linear) -> Autoencoder (Deep)\n\n![PCA_FM](image-20210214174003005.png)\n\n![](image-20210214175022879.png)\n\n## Denoise for Fluid Flow\n\n![denoise](image-20210214174108194.png)\n\n## Turbulence modeling\n\nPaper: \n\n- Schlatter, Philipp, et al. \"The structure of a turbulent boundary layer studied by numerical simulation.\" arXiv preprint arXiv:1010.4000 (2010).\n\n- Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. \"Turbulence modeling in the age of data.\" *Annual Review of Fluid Mechanics* 51 (2019): 357-377.\n\n![](image-20210214174310749.png)\n\n![](image-20210214174422276.png)\n\n## ML_CFD solver\n\nPaper:\n\n- Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.\" Journal of Fluid Mechanics 807 (2016): 155-166.\n\nAdd physical constraints and achieve accurate and pyhsical.\n\n## Super-resolution\n\nPaper: \n\n- Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. \"Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.\" arXiv preprint arXiv:1905.10866 (2019).\n\nInterpolation and Extrapolation\n\n![](image-20210214174809740.png)\n\n![](image-20210214174913278.png)\n\n## Solve PDEs\n\n![](image-20210214175304898.png)\n\n# Beyond understanding: control\n\n![](image-20210214182334199.png)\n\n# Inspiration from biology\n\n![](image-20210214182520461.png)\n","source":"_posts/notes-ML-FM.md","raw":"---\ntitle: Applications of Machine Learning for Fluid Mechanics \ndate: 2021-02-14 20:50:17\ncategories: Notes\ntags: \n\t- Machine Learning\t\n\t- Deep Learning\n\t- Fluid Mechanics\n---\n\nThis blog is the notes for a video called \"[Machine Learning for Fluid Mechanics](https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s)\", which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. \"Machine learning for fluid mechanics.\" Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.\n\n<!-- more -->\n\n# What is Machine Learning (ML)?\n\nML:\n\n- Models from Data via Optimization\n\n> Any sufficiently advanced technology is indistinguishable from magic.\n>\n> -- Arthur C. Clarke\n\nFluid dynamics tasks:\n\n- Reduction\n- Modeling\n- Control\n- Sensing\n- Closure\n\nOptimization problems:\n\n- High-dimensional\n- Non-linear\n- Non-convex\n- Multiscale\n\n# What kind of ML is needed in science and engineering?\n\nWe need Interpretable and Generalizable Machine Learning in science and engineering field.\n\n> Everything should be made as simple as possible, but not simpler.\n>\n> -- Albert Einstein\n\nHow to build a model like $F=ma$?\n\nFeatures for ML in science and engineering:\n\n- Sparse\n- Low-dimensional\n- Robust\n\n# Schematic: ML + CFD\n\n![Schematic](image-20210214173356481.png)\n\n\n\n# Why ML could work?\n\nBecause patterns exist in fluid flow.\n\n![pattern_FM](image-20210214173840928.png)\n\n# Applications\n\n## Fluid flow decomposition\n\nPCA (Shallow, linear) -> Autoencoder (Deep)\n\n![PCA_FM](image-20210214174003005.png)\n\n![](image-20210214175022879.png)\n\n## Denoise for Fluid Flow\n\n![denoise](image-20210214174108194.png)\n\n## Turbulence modeling\n\nPaper: \n\n- Schlatter, Philipp, et al. \"The structure of a turbulent boundary layer studied by numerical simulation.\" arXiv preprint arXiv:1010.4000 (2010).\n\n- Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. \"Turbulence modeling in the age of data.\" *Annual Review of Fluid Mechanics* 51 (2019): 357-377.\n\n![](image-20210214174310749.png)\n\n![](image-20210214174422276.png)\n\n## ML_CFD solver\n\nPaper:\n\n- Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.\" Journal of Fluid Mechanics 807 (2016): 155-166.\n\nAdd physical constraints and achieve accurate and pyhsical.\n\n## Super-resolution\n\nPaper: \n\n- Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. \"Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.\" arXiv preprint arXiv:1905.10866 (2019).\n\nInterpolation and Extrapolation\n\n![](image-20210214174809740.png)\n\n![](image-20210214174913278.png)\n\n## Solve PDEs\n\n![](image-20210214175304898.png)\n\n# Beyond understanding: control\n\n![](image-20210214182334199.png)\n\n# Inspiration from biology\n\n![](image-20210214182520461.png)\n","slug":"notes-ML-FM","published":1,"updated":"2021-02-17T13:13:26.433Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknwqx6s1000rbczwbqqs856l","content":"<p>This blog is the notes for a video called “<a href=\"https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s\">Machine Learning for Fluid Mechanics</a>“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.</p>\n<a id=\"more\"></a>\n\n<h1 id=\"What-is-Machine-Learning-ML\"><a href=\"#What-is-Machine-Learning-ML\" class=\"headerlink\" title=\"What is Machine Learning (ML)?\"></a>What is Machine Learning (ML)?</h1><p>ML:</p>\n<ul>\n<li>Models from Data via Optimization</li>\n</ul>\n<blockquote>\n<p>Any sufficiently advanced technology is indistinguishable from magic.</p>\n<p>– Arthur C. Clarke</p>\n</blockquote>\n<p>Fluid dynamics tasks:</p>\n<ul>\n<li>Reduction</li>\n<li>Modeling</li>\n<li>Control</li>\n<li>Sensing</li>\n<li>Closure</li>\n</ul>\n<p>Optimization problems:</p>\n<ul>\n<li>High-dimensional</li>\n<li>Non-linear</li>\n<li>Non-convex</li>\n<li>Multiscale</li>\n</ul>\n<h1 id=\"What-kind-of-ML-is-needed-in-science-and-engineering\"><a href=\"#What-kind-of-ML-is-needed-in-science-and-engineering\" class=\"headerlink\" title=\"What kind of ML is needed in science and engineering?\"></a>What kind of ML is needed in science and engineering?</h1><p>We need Interpretable and Generalizable Machine Learning in science and engineering field.</p>\n<blockquote>\n<p>Everything should be made as simple as possible, but not simpler.</p>\n<p>– Albert Einstein</p>\n</blockquote>\n<p>How to build a model like $F=ma$?</p>\n<p>Features for ML in science and engineering:</p>\n<ul>\n<li>Sparse</li>\n<li>Low-dimensional</li>\n<li>Robust</li>\n</ul>\n<h1 id=\"Schematic-ML-CFD\"><a href=\"#Schematic-ML-CFD\" class=\"headerlink\" title=\"Schematic: ML + CFD\"></a>Schematic: ML + CFD</h1><p><img src=\"image-20210214173356481.png\" alt=\"Schematic\"></p>\n<h1 id=\"Why-ML-could-work\"><a href=\"#Why-ML-could-work\" class=\"headerlink\" title=\"Why ML could work?\"></a>Why ML could work?</h1><p>Because patterns exist in fluid flow.</p>\n<p><img src=\"image-20210214173840928.png\" alt=\"pattern_FM\"></p>\n<h1 id=\"Applications\"><a href=\"#Applications\" class=\"headerlink\" title=\"Applications\"></a>Applications</h1><h2 id=\"Fluid-flow-decomposition\"><a href=\"#Fluid-flow-decomposition\" class=\"headerlink\" title=\"Fluid flow decomposition\"></a>Fluid flow decomposition</h2><p>PCA (Shallow, linear) -&gt; Autoencoder (Deep)</p>\n<p><img src=\"image-20210214174003005.png\" alt=\"PCA_FM\"></p>\n<p><img src=\"image-20210214175022879.png\"></p>\n<h2 id=\"Denoise-for-Fluid-Flow\"><a href=\"#Denoise-for-Fluid-Flow\" class=\"headerlink\" title=\"Denoise for Fluid Flow\"></a>Denoise for Fluid Flow</h2><p><img src=\"image-20210214174108194.png\" alt=\"denoise\"></p>\n<h2 id=\"Turbulence-modeling\"><a href=\"#Turbulence-modeling\" class=\"headerlink\" title=\"Turbulence modeling\"></a>Turbulence modeling</h2><p>Paper: </p>\n<ul>\n<li><p>Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010).</p>\n</li>\n<li><p>Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” <em>Annual Review of Fluid Mechanics</em> 51 (2019): 357-377.</p>\n</li>\n</ul>\n<p><img src=\"image-20210214174310749.png\"></p>\n<p><img src=\"image-20210214174422276.png\"></p>\n<h2 id=\"ML-CFD-solver\"><a href=\"#ML-CFD-solver\" class=\"headerlink\" title=\"ML_CFD solver\"></a>ML_CFD solver</h2><p>Paper:</p>\n<ul>\n<li>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166.</li>\n</ul>\n<p>Add physical constraints and achieve accurate and pyhsical.</p>\n<h2 id=\"Super-resolution\"><a href=\"#Super-resolution\" class=\"headerlink\" title=\"Super-resolution\"></a>Super-resolution</h2><p>Paper: </p>\n<ul>\n<li>Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019).</li>\n</ul>\n<p>Interpolation and Extrapolation</p>\n<p><img src=\"image-20210214174809740.png\"></p>\n<p><img src=\"image-20210214174913278.png\"></p>\n<h2 id=\"Solve-PDEs\"><a href=\"#Solve-PDEs\" class=\"headerlink\" title=\"Solve PDEs\"></a>Solve PDEs</h2><p><img src=\"image-20210214175304898.png\"></p>\n<h1 id=\"Beyond-understanding-control\"><a href=\"#Beyond-understanding-control\" class=\"headerlink\" title=\"Beyond understanding: control\"></a>Beyond understanding: control</h1><p><img src=\"image-20210214182334199.png\"></p>\n<h1 id=\"Inspiration-from-biology\"><a href=\"#Inspiration-from-biology\" class=\"headerlink\" title=\"Inspiration from biology\"></a>Inspiration from biology</h1><p><img src=\"image-20210214182520461.png\"></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<p>This blog is the notes for a video called “<a href=\"https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s\">Machine Learning for Fluid Mechanics</a>“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.</p>","more":"<h1 id=\"What-is-Machine-Learning-ML\"><a href=\"#What-is-Machine-Learning-ML\" class=\"headerlink\" title=\"What is Machine Learning (ML)?\"></a>What is Machine Learning (ML)?</h1><p>ML:</p>\n<ul>\n<li>Models from Data via Optimization</li>\n</ul>\n<blockquote>\n<p>Any sufficiently advanced technology is indistinguishable from magic.</p>\n<p>– Arthur C. Clarke</p>\n</blockquote>\n<p>Fluid dynamics tasks:</p>\n<ul>\n<li>Reduction</li>\n<li>Modeling</li>\n<li>Control</li>\n<li>Sensing</li>\n<li>Closure</li>\n</ul>\n<p>Optimization problems:</p>\n<ul>\n<li>High-dimensional</li>\n<li>Non-linear</li>\n<li>Non-convex</li>\n<li>Multiscale</li>\n</ul>\n<h1 id=\"What-kind-of-ML-is-needed-in-science-and-engineering\"><a href=\"#What-kind-of-ML-is-needed-in-science-and-engineering\" class=\"headerlink\" title=\"What kind of ML is needed in science and engineering?\"></a>What kind of ML is needed in science and engineering?</h1><p>We need Interpretable and Generalizable Machine Learning in science and engineering field.</p>\n<blockquote>\n<p>Everything should be made as simple as possible, but not simpler.</p>\n<p>– Albert Einstein</p>\n</blockquote>\n<p>How to build a model like $F=ma$?</p>\n<p>Features for ML in science and engineering:</p>\n<ul>\n<li>Sparse</li>\n<li>Low-dimensional</li>\n<li>Robust</li>\n</ul>\n<h1 id=\"Schematic-ML-CFD\"><a href=\"#Schematic-ML-CFD\" class=\"headerlink\" title=\"Schematic: ML + CFD\"></a>Schematic: ML + CFD</h1><p><img src=\"image-20210214173356481.png\" alt=\"Schematic\"></p>\n<h1 id=\"Why-ML-could-work\"><a href=\"#Why-ML-could-work\" class=\"headerlink\" title=\"Why ML could work?\"></a>Why ML could work?</h1><p>Because patterns exist in fluid flow.</p>\n<p><img src=\"image-20210214173840928.png\" alt=\"pattern_FM\"></p>\n<h1 id=\"Applications\"><a href=\"#Applications\" class=\"headerlink\" title=\"Applications\"></a>Applications</h1><h2 id=\"Fluid-flow-decomposition\"><a href=\"#Fluid-flow-decomposition\" class=\"headerlink\" title=\"Fluid flow decomposition\"></a>Fluid flow decomposition</h2><p>PCA (Shallow, linear) -&gt; Autoencoder (Deep)</p>\n<p><img src=\"image-20210214174003005.png\" alt=\"PCA_FM\"></p>\n<p><img src=\"image-20210214175022879.png\"></p>\n<h2 id=\"Denoise-for-Fluid-Flow\"><a href=\"#Denoise-for-Fluid-Flow\" class=\"headerlink\" title=\"Denoise for Fluid Flow\"></a>Denoise for Fluid Flow</h2><p><img src=\"image-20210214174108194.png\" alt=\"denoise\"></p>\n<h2 id=\"Turbulence-modeling\"><a href=\"#Turbulence-modeling\" class=\"headerlink\" title=\"Turbulence modeling\"></a>Turbulence modeling</h2><p>Paper: </p>\n<ul>\n<li><p>Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010).</p>\n</li>\n<li><p>Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” <em>Annual Review of Fluid Mechanics</em> 51 (2019): 357-377.</p>\n</li>\n</ul>\n<p><img src=\"image-20210214174310749.png\"></p>\n<p><img src=\"image-20210214174422276.png\"></p>\n<h2 id=\"ML-CFD-solver\"><a href=\"#ML-CFD-solver\" class=\"headerlink\" title=\"ML_CFD solver\"></a>ML_CFD solver</h2><p>Paper:</p>\n<ul>\n<li>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166.</li>\n</ul>\n<p>Add physical constraints and achieve accurate and pyhsical.</p>\n<h2 id=\"Super-resolution\"><a href=\"#Super-resolution\" class=\"headerlink\" title=\"Super-resolution\"></a>Super-resolution</h2><p>Paper: </p>\n<ul>\n<li>Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019).</li>\n</ul>\n<p>Interpolation and Extrapolation</p>\n<p><img src=\"image-20210214174809740.png\"></p>\n<p><img src=\"image-20210214174913278.png\"></p>\n<h2 id=\"Solve-PDEs\"><a href=\"#Solve-PDEs\" class=\"headerlink\" title=\"Solve PDEs\"></a>Solve PDEs</h2><p><img src=\"image-20210214175304898.png\"></p>\n<h1 id=\"Beyond-understanding-control\"><a href=\"#Beyond-understanding-control\" class=\"headerlink\" title=\"Beyond understanding: control\"></a>Beyond understanding: control</h1><p><img src=\"image-20210214182334199.png\"></p>\n<h1 id=\"Inspiration-from-biology\"><a href=\"#Inspiration-from-biology\" class=\"headerlink\" title=\"Inspiration from biology\"></a>Inspiration from biology</h1><p><img src=\"image-20210214182520461.png\"></p>"},{"title":"A Survey for Vision Transformers","date":"2021-04-25T05:02:11.000Z","_content":"\n**Note that this is a literature surevy for a course EECS 433. It was writen in Mar. 2021.**\n\n**ABSTRACT:** Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.\n\n<!-- more -->\n\n**KEYWORDS:** *Self-attention, transformer, convolution neural networks, deep neural networks.*\n\n# 1.  Introduction\n\nConvolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.\n\nDespite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. \n\nAt the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. \n\nInspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). \n\nEven though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? \n\nThe structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.\n\n# 2.  Related work\n\nVisio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.\n\n## 2.1 CNN\n\nThere are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. \n\nIn 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. \n\nIn 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. \n\nConsidering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. \n\nIn 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.\n\n![img](clip_image001.png)\n\nFigure 1. DenseNet (G. Huang et al., 2016)\n\nIt is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. \n\n## 2.2 Attention in CNN\n\nConsidering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. \n\n![img](clip_image002.png)\n\nFigure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)\n\nInspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. \n\n![img](clip_image003.png)\n\nFigure 3. Selective Kernel Convolution. (X. Li et al., 2019)\n\nSome researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. \n\nOther researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. \n\n![img](clip_image004.png)\n\nFigure 4. Non-local block. (X. Wang et al., 2018)\n\nAlthough NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. \n\nTo summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.\n\nTable 1. Model comparison for the application of attention in CNN.\n\n\n\n| **Model name** | **Characteristics**                                  |\n| -------------- | ---------------------------------------------------- |\n| SENet          | Spatial aggregation, channel attention               |\n| SKNet          | Adaptive receptive filed, channel attention          |\n| BAM            | Channel attention and spatial attention in parallel  |\n| CBAM           | Channel attention and spatial attention sequentially |\n| NLNet          | Spatial weighted sum per pixel                       |\n| GCNet          | Spatial weighted sum, channel attention              |\n\n \n\n## 2.3 Self-attention\n\nSelf-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. \n\n![img](clip_image005.png)\n\nFigure 5. Self-attention operation.\n\nThe calculation of self-attention can be formulated as a single function: \n\n\n\n| ![img](clip_image007.png) | (1)  |\n| ------------------------- | ---- |\n|                           |      |\n\nThe first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of ![img](clip_image009.png) and ![img](clip_image011.png) aims to calculate the similarity/distance between two different vectors. The score of ![img](clip_image013.png) can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. \n\nIt is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix ![img](clip_image011.png),value matrix ![img](clip_image016.png) and query matrix ![img](clip_image009.png) come from. For the encoder-decoder attention layer, ![img](clip_image019.png)and ![img](clip_image016.png) come from the encoder module and ![img](clip_image009.png) comes from the previous layer. Other operations are the same. \n\nThe limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:\n\n\n\n| ![img](clip_image023.png) | (2)  |\n| ------------------------- | ---- |\n|                           |      |\n\nwhere ![img](clip_image025.png), ![img](clip_image027.png), ![img](clip_image029.png), and ![img](clip_image031.png)are the concatenation of ![img](clip_image033.png). \n\n![img](clip_image034.png)\n\nFigure 6. Multi-head attention. (Vaswani et al., 2017)\n\n## 2.4 Fourier-based network\n\nConvolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. \n\nAnother network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.\n\n![img](clip_image035.png)\n\nFigure 7. Fourier Layer. (Z. Li et al., 2020)\n\nThere are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.\n\n# 3.  Topic and its applications\n\nIn this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. \n\n## 3.1 Transformer\n\nIn the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. \n\n![img](clip_image036.png)\n\nFigure 8. The architecture for transformer. (Vaswani et al., 2017)\n\nThe first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. \n\nThe second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:\n\n\n\n| ![img](clip_image038.png), | (3)  |\n| -------------------------- | ---- |\n|                            |      |\n\nwhere ![img](clip_image040.png)and ![img](clip_image042.png) are weights for the two linear transformation layers, ![img](clip_image044.png) is ReLU activation function. \n\nThe third thing is Add & Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. \n\nThe last thing is the output layer. After passing through ![img](clip_image046.png)decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.\n\nTo summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.\n\n## 3.2 Applications\n\nSince 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.\n\nInspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. \n\n## 3.2.1  Image classification\n\nViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image ![img](clip_image048.png) as a sequence of flattened 2D patches ![img](clip_image048.png), where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. \n\n![img](clip_image050.png)\n\nFigure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)\n\nViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. \n\n![img](clip_image051.png)\n\nFigure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)\n\nViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. \n\nAnother vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.\n\n![img](clip_image053.png)\n\nFigure 11. T2T architecture and feature visualization. (Yuan et al., 2021)\n\nViewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. \n\n![img](clip_image054.png)\n\nFigure 12. TNT block and TNT framework. (Han et al., 2021)\n\nBottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.\n\n![img](clip_image055.png)\n\nFigure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)\n\n## 3.2.2  Object detection\n\nCompared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. \n\n![img](clip_image056.png)\n\nFigure 14. The architecture for DETR. (Carion et al., 2020)\n\nTo solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.\n\nPyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. \n\nAdaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. \n\nSun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.\n\n\n\n## 3.2.3  Segmentation\n\nSegmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. \n\n![img](clip_image057.png)\n\nFigure 15. The architecture for VisTR. (Y. Wang et al., 2020)\n\nPoint Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. \n\n\n\n# 4.  Classification and comparison of various methods\n\nTransformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. \n\n \n\n| **Task**                      | **Category**                            | **Method**                                                  | **Characteristics**                                          |\n| ----------------------------- | --------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |\n| Image classification          | Transformer + global attention          | ViT   (Dosovitskiy  et al., 2020)                           | Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset |\n| Image classification          | Transformer + CNN                       | DeiT   (Touvron  et al., 2021)                              | Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher |\n| Image classification          | Transformer + global attention          | T2T-ViT   (Yuan et  al., 2021)                              | Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet |\n| Image classification          | Transformer + global attention          | TNT   (Han et  al., 2021)                                   | Operate on patch-level and pixel-level, more diverse  feature extraction |\n| Object detection              | Transformer + CNN                       | DETR   (Carion  et al., 2020)                               | End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge |\n| Object detection              | Transformer + CNN + efficient attention | Deformable DETR   (Zhu et  al., 2020)                       | Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time |\n| Segmentation                  | Transformer + CNN                       | MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)             | ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture |\n| Segmentation                  | Transformer + CNN                       | VisTR   (Y. Wang  et al., 2020)                             | instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction |\n| Image processing              | Transformer + CNN                       | Image processing transformer  (IPT) (H. Chen  et al., 2020) | The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning |\n| Image colorization            | Transformer + global attention          | Colorization Transformer (ColTran) (Kumar et  al., 2020)    | Conditional transformer layers, two parallel networks to upsample  low resolution images |\n| Segmentation                  | Transformer + efficient attention       | Criss-Cross Network (CCNet)                                 | Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance, |\n| NLP, architecture exploration | Transformer + efficient attention       | ConvBERT (Jiang et al., 2020)                               | Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies |\n\n \n\n# 5.  Future work\n\nTransformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. \n\nThe first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. \n\nThe second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. \n\nThe third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.\n\nThe fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. \n\n# 6.  Conclusion\n\nIn this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.\n\n# 7.  Course feedbacks\n\nThis course is a great graduate-level class. There are several reasons for that:\n\n\\1.   **Rich course content.** From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. \n\n\\2.   **Excellent lectures.** Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.\n\n\\3.   **Novel and creative illustration**. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.\n\n\\4.   **Excellent reading recommendation**. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.\n\n\\5.   For every lecture, professor always pointed out the “**take away home message**”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.\n\n\\6.   **The slides are clear and well-organized.** I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.\n\n# 8.  Reference\n\n[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. *ArXiv:2012.09958 [Cs]*. http://arxiv.org/abs/2012.09958\n\n[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. *IEEE Transactions on Neural Networks*, *5*(2), 157–166. https://doi.org/10.1109/72.279181\n\n[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). *Language Models are Few-Shot Learners*. https://arxiv.org/abs/2005.14165v4\n\n[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). *GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond*. 0–0. https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\n\n[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. *ArXiv:2005.12872 [Cs]*. http://arxiv.org/abs/2005.12872\n\n[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. *ArXiv:2012.00364 [Cs]*. http://arxiv.org/abs/2012.00364\n\n[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. *International Conference on Machine Learning*, 1691–1703. http://proceedings.mlr.press/v119/chen20s.html\n\n[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. *ArXiv:1606.03657 [Cs, Stat]*. http://arxiv.org/abs/1606.03657\n\n[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. *ArXiv:1601.06733 [Cs]*. http://arxiv.org/abs/1601.06733\n\n[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. *2009 IEEE Conference on Computer Vision and Pattern Recognition*, 248–255.\n\n[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *ArXiv:1810.04805 [Cs]*. http://arxiv.org/abs/1810.04805\n\n[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ArXiv:2010.11929 [Cs]*. http://arxiv.org/abs/2010.11929\n\n[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*, 249–256. http://proceedings.mlr.press/v9/glorot10a.html\n\n[14]         Goodfellow, I. (2016). *Generative Adversarial Networks (GANs)*. 86.\n\n[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep Learning*. MIT Press.\n\n[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. *ArXiv:2103.00112 [Cs]*. http://arxiv.org/abs/2103.00112\n\n[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. *ArXiv:1512.03385 [Cs]*. http://arxiv.org/abs/1512.03385\n\n[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. *ArXiv:1704.04861 [Cs]*. http://arxiv.org/abs/1704.04861\n\n[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. *ArXiv:1709.01507 [Cs]*. http://arxiv.org/abs/1709.01507\n\n[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. *ArXiv:1608.06993 [Cs]*. http://arxiv.org/abs/1608.06993\n\n[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 603–612.\n\n[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. *ArXiv Preprint ArXiv:2008.02496*.\n\n[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), *Advances in Neural Information Processing Systems 25* (pp. 1097–1105). Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). *Colorization Transformer*. International Conference on Learning Representations. https://openreview.net/forum?id=5NA1PinlGFu\n\n[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. *Neural Computation*, *1*(4), 541–551. https://doi.org/10.1162/neco.1989.1.4.541\n\n[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. *ArXiv:1903.06586 [Cs]*. http://arxiv.org/abs/1903.06586\n\n[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). *Fourier Neural Operator for Parametric Partial Differential Equations*. https://arxiv.org/abs/2010.08895v1\n\n[28]         Lu Chi. (2020). Fast Fourier Convolution. *Neural Information Processing Systems*, 767–774. https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\n\n[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. *ArXiv:1508.04025 [Cs]*. http://arxiv.org/abs/1508.04025\n\n[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. *Proceedings of the 27th International Conference on International Conference on Machine Learning*, 807–814.\n\n[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. *ArXiv:1807.06514 [Cs]*. http://arxiv.org/abs/1807.06514\n\n[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *Undefined*. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\n\n[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). *Improving language understanding by generative pre-training*.\n\n[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. *ArXiv:1804.02767 [Cs]*. http://arxiv.org/abs/1804.02767\n\n[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. *ArXiv:1506.01497 [Cs]*. http://arxiv.org/abs/1506.01497\n\n[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. *ArXiv:1409.0575 [Cs]*. http://arxiv.org/abs/1409.0575\n\n[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. *ArXiv:1409.1556 [Cs]*. http://arxiv.org/abs/1409.1556\n\n[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. *ArXiv:2101.11605 [Cs]*. http://arxiv.org/abs/2101.11605\n\n[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. *ArXiv E-Prints*, *1505*, arXiv:1505.00387.\n\n[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. *ArXiv:1707.02968 [Cs]*. http://arxiv.org/abs/1707.02968\n\n[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. *ArXiv:2011.10881 [Cs]*. http://arxiv.org/abs/2011.10881\n\n[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. *ArXiv:1409.4842 [Cs]*. http://arxiv.org/abs/1409.4842\n\n[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). *Rethinking the Inception Architecture for Computer Vision*. 2818–2826. http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\n\n[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers & distillation through attention. *ArXiv:2012.12877 [Cs]*. http://arxiv.org/abs/2012.12877\n\n[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), *Advances in Neural Information Processing Systems 30* (pp. 5998–6008). Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n\n[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. *ArXiv:2012.00759 [Cs]*. http://arxiv.org/abs/2012.00759\n\n[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. *European Conference on Computer Vision*, 108–126.\n\n[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. *2018 IEEE Winter Conference on Applications of Computer Vision (WACV)*, 1451–1460. https://doi.org/10.1109/WACV.2018.00163\n\n[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. *ArXiv:2102.12122 [Cs]*. http://arxiv.org/abs/2102.12122\n\n[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. *ArXiv:1711.07971 [Cs]*. http://arxiv.org/abs/1711.07971\n\n[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. *ArXiv:2011.14503 [Cs]*. http://arxiv.org/abs/2011.14503\n\n[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). *CBAM: Convolutional Block Attention Module*. 3–19. https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\n\n[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. *ArXiv:2006.04139 [Cs]*. http://arxiv.org/abs/2006.04139\n\n[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. *ArXiv:2101.11986 [Cs]*. http://arxiv.org/abs/2101.11986\n\n[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. *ArXiv:2007.09451 [Cs]*. http://arxiv.org/abs/2007.09451\n\n[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. *ArXiv:2012.09164 [Cs]*. http://arxiv.org/abs/2012.09164\n\n[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. *ArXiv:2011.09315 [Cs]*. http://arxiv.org/abs/2011.09315\n\n[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. *ArXiv:2012.15840 [Cs]*. http://arxiv.org/abs/2012.15840\n\n[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. *ArXiv:2010.04159 [Cs]*. http://arxiv.org/abs/2010.04159\n\n \n\n \n\n","source":"_posts/Survey-vision-transformer.md","raw":"---\ntitle: A Survey for Vision Transformers\ndate: 2021-04-25 13:02:11\ntags: \n    - Survey\n    - Vision transformer\n    - Computer vision\n    - Course project\n---\n\n**Note that this is a literature surevy for a course EECS 433. It was writen in Mar. 2021.**\n\n**ABSTRACT:** Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.\n\n<!-- more -->\n\n**KEYWORDS:** *Self-attention, transformer, convolution neural networks, deep neural networks.*\n\n# 1.  Introduction\n\nConvolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.\n\nDespite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. \n\nAt the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. \n\nInspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). \n\nEven though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? \n\nThe structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.\n\n# 2.  Related work\n\nVisio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.\n\n## 2.1 CNN\n\nThere are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. \n\nIn 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. \n\nIn 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. \n\nConsidering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. \n\nIn 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.\n\n![img](clip_image001.png)\n\nFigure 1. DenseNet (G. Huang et al., 2016)\n\nIt is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. \n\n## 2.2 Attention in CNN\n\nConsidering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. \n\n![img](clip_image002.png)\n\nFigure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)\n\nInspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. \n\n![img](clip_image003.png)\n\nFigure 3. Selective Kernel Convolution. (X. Li et al., 2019)\n\nSome researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. \n\nOther researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. \n\n![img](clip_image004.png)\n\nFigure 4. Non-local block. (X. Wang et al., 2018)\n\nAlthough NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. \n\nTo summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.\n\nTable 1. Model comparison for the application of attention in CNN.\n\n\n\n| **Model name** | **Characteristics**                                  |\n| -------------- | ---------------------------------------------------- |\n| SENet          | Spatial aggregation, channel attention               |\n| SKNet          | Adaptive receptive filed, channel attention          |\n| BAM            | Channel attention and spatial attention in parallel  |\n| CBAM           | Channel attention and spatial attention sequentially |\n| NLNet          | Spatial weighted sum per pixel                       |\n| GCNet          | Spatial weighted sum, channel attention              |\n\n \n\n## 2.3 Self-attention\n\nSelf-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. \n\n![img](clip_image005.png)\n\nFigure 5. Self-attention operation.\n\nThe calculation of self-attention can be formulated as a single function: \n\n\n\n| ![img](clip_image007.png) | (1)  |\n| ------------------------- | ---- |\n|                           |      |\n\nThe first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of ![img](clip_image009.png) and ![img](clip_image011.png) aims to calculate the similarity/distance between two different vectors. The score of ![img](clip_image013.png) can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. \n\nIt is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix ![img](clip_image011.png),value matrix ![img](clip_image016.png) and query matrix ![img](clip_image009.png) come from. For the encoder-decoder attention layer, ![img](clip_image019.png)and ![img](clip_image016.png) come from the encoder module and ![img](clip_image009.png) comes from the previous layer. Other operations are the same. \n\nThe limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:\n\n\n\n| ![img](clip_image023.png) | (2)  |\n| ------------------------- | ---- |\n|                           |      |\n\nwhere ![img](clip_image025.png), ![img](clip_image027.png), ![img](clip_image029.png), and ![img](clip_image031.png)are the concatenation of ![img](clip_image033.png). \n\n![img](clip_image034.png)\n\nFigure 6. Multi-head attention. (Vaswani et al., 2017)\n\n## 2.4 Fourier-based network\n\nConvolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. \n\nAnother network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.\n\n![img](clip_image035.png)\n\nFigure 7. Fourier Layer. (Z. Li et al., 2020)\n\nThere are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.\n\n# 3.  Topic and its applications\n\nIn this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. \n\n## 3.1 Transformer\n\nIn the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. \n\n![img](clip_image036.png)\n\nFigure 8. The architecture for transformer. (Vaswani et al., 2017)\n\nThe first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. \n\nThe second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:\n\n\n\n| ![img](clip_image038.png), | (3)  |\n| -------------------------- | ---- |\n|                            |      |\n\nwhere ![img](clip_image040.png)and ![img](clip_image042.png) are weights for the two linear transformation layers, ![img](clip_image044.png) is ReLU activation function. \n\nThe third thing is Add & Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. \n\nThe last thing is the output layer. After passing through ![img](clip_image046.png)decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.\n\nTo summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.\n\n## 3.2 Applications\n\nSince 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.\n\nInspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. \n\n## 3.2.1  Image classification\n\nViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image ![img](clip_image048.png) as a sequence of flattened 2D patches ![img](clip_image048.png), where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. \n\n![img](clip_image050.png)\n\nFigure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)\n\nViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. \n\n![img](clip_image051.png)\n\nFigure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)\n\nViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. \n\nAnother vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.\n\n![img](clip_image053.png)\n\nFigure 11. T2T architecture and feature visualization. (Yuan et al., 2021)\n\nViewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. \n\n![img](clip_image054.png)\n\nFigure 12. TNT block and TNT framework. (Han et al., 2021)\n\nBottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.\n\n![img](clip_image055.png)\n\nFigure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)\n\n## 3.2.2  Object detection\n\nCompared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. \n\n![img](clip_image056.png)\n\nFigure 14. The architecture for DETR. (Carion et al., 2020)\n\nTo solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.\n\nPyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. \n\nAdaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. \n\nSun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.\n\n\n\n## 3.2.3  Segmentation\n\nSegmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. \n\n![img](clip_image057.png)\n\nFigure 15. The architecture for VisTR. (Y. Wang et al., 2020)\n\nPoint Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. \n\n\n\n# 4.  Classification and comparison of various methods\n\nTransformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. \n\n \n\n| **Task**                      | **Category**                            | **Method**                                                  | **Characteristics**                                          |\n| ----------------------------- | --------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |\n| Image classification          | Transformer + global attention          | ViT   (Dosovitskiy  et al., 2020)                           | Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset |\n| Image classification          | Transformer + CNN                       | DeiT   (Touvron  et al., 2021)                              | Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher |\n| Image classification          | Transformer + global attention          | T2T-ViT   (Yuan et  al., 2021)                              | Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet |\n| Image classification          | Transformer + global attention          | TNT   (Han et  al., 2021)                                   | Operate on patch-level and pixel-level, more diverse  feature extraction |\n| Object detection              | Transformer + CNN                       | DETR   (Carion  et al., 2020)                               | End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge |\n| Object detection              | Transformer + CNN + efficient attention | Deformable DETR   (Zhu et  al., 2020)                       | Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time |\n| Segmentation                  | Transformer + CNN                       | MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)             | ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture |\n| Segmentation                  | Transformer + CNN                       | VisTR   (Y. Wang  et al., 2020)                             | instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction |\n| Image processing              | Transformer + CNN                       | Image processing transformer  (IPT) (H. Chen  et al., 2020) | The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning |\n| Image colorization            | Transformer + global attention          | Colorization Transformer (ColTran) (Kumar et  al., 2020)    | Conditional transformer layers, two parallel networks to upsample  low resolution images |\n| Segmentation                  | Transformer + efficient attention       | Criss-Cross Network (CCNet)                                 | Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance, |\n| NLP, architecture exploration | Transformer + efficient attention       | ConvBERT (Jiang et al., 2020)                               | Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies |\n\n \n\n# 5.  Future work\n\nTransformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. \n\nThe first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. \n\nThe second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. \n\nThe third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.\n\nThe fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. \n\n# 6.  Conclusion\n\nIn this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.\n\n# 7.  Course feedbacks\n\nThis course is a great graduate-level class. There are several reasons for that:\n\n\\1.   **Rich course content.** From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. \n\n\\2.   **Excellent lectures.** Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.\n\n\\3.   **Novel and creative illustration**. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.\n\n\\4.   **Excellent reading recommendation**. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.\n\n\\5.   For every lecture, professor always pointed out the “**take away home message**”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.\n\n\\6.   **The slides are clear and well-organized.** I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.\n\n# 8.  Reference\n\n[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. *ArXiv:2012.09958 [Cs]*. http://arxiv.org/abs/2012.09958\n\n[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. *IEEE Transactions on Neural Networks*, *5*(2), 157–166. https://doi.org/10.1109/72.279181\n\n[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). *Language Models are Few-Shot Learners*. https://arxiv.org/abs/2005.14165v4\n\n[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). *GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond*. 0–0. https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\n\n[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. *ArXiv:2005.12872 [Cs]*. http://arxiv.org/abs/2005.12872\n\n[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. *ArXiv:2012.00364 [Cs]*. http://arxiv.org/abs/2012.00364\n\n[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. *International Conference on Machine Learning*, 1691–1703. http://proceedings.mlr.press/v119/chen20s.html\n\n[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. *ArXiv:1606.03657 [Cs, Stat]*. http://arxiv.org/abs/1606.03657\n\n[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. *ArXiv:1601.06733 [Cs]*. http://arxiv.org/abs/1601.06733\n\n[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. *2009 IEEE Conference on Computer Vision and Pattern Recognition*, 248–255.\n\n[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *ArXiv:1810.04805 [Cs]*. http://arxiv.org/abs/1810.04805\n\n[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ArXiv:2010.11929 [Cs]*. http://arxiv.org/abs/2010.11929\n\n[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*, 249–256. http://proceedings.mlr.press/v9/glorot10a.html\n\n[14]         Goodfellow, I. (2016). *Generative Adversarial Networks (GANs)*. 86.\n\n[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep Learning*. MIT Press.\n\n[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. *ArXiv:2103.00112 [Cs]*. http://arxiv.org/abs/2103.00112\n\n[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. *ArXiv:1512.03385 [Cs]*. http://arxiv.org/abs/1512.03385\n\n[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. *ArXiv:1704.04861 [Cs]*. http://arxiv.org/abs/1704.04861\n\n[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. *ArXiv:1709.01507 [Cs]*. http://arxiv.org/abs/1709.01507\n\n[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. *ArXiv:1608.06993 [Cs]*. http://arxiv.org/abs/1608.06993\n\n[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 603–612.\n\n[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. *ArXiv Preprint ArXiv:2008.02496*.\n\n[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), *Advances in Neural Information Processing Systems 25* (pp. 1097–1105). Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). *Colorization Transformer*. International Conference on Learning Representations. https://openreview.net/forum?id=5NA1PinlGFu\n\n[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. *Neural Computation*, *1*(4), 541–551. https://doi.org/10.1162/neco.1989.1.4.541\n\n[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. *ArXiv:1903.06586 [Cs]*. http://arxiv.org/abs/1903.06586\n\n[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). *Fourier Neural Operator for Parametric Partial Differential Equations*. https://arxiv.org/abs/2010.08895v1\n\n[28]         Lu Chi. (2020). Fast Fourier Convolution. *Neural Information Processing Systems*, 767–774. https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\n\n[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. *ArXiv:1508.04025 [Cs]*. http://arxiv.org/abs/1508.04025\n\n[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. *Proceedings of the 27th International Conference on International Conference on Machine Learning*, 807–814.\n\n[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. *ArXiv:1807.06514 [Cs]*. http://arxiv.org/abs/1807.06514\n\n[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *Undefined*. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\n\n[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). *Improving language understanding by generative pre-training*.\n\n[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. *ArXiv:1804.02767 [Cs]*. http://arxiv.org/abs/1804.02767\n\n[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. *ArXiv:1506.01497 [Cs]*. http://arxiv.org/abs/1506.01497\n\n[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. *ArXiv:1409.0575 [Cs]*. http://arxiv.org/abs/1409.0575\n\n[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. *ArXiv:1409.1556 [Cs]*. http://arxiv.org/abs/1409.1556\n\n[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. *ArXiv:2101.11605 [Cs]*. http://arxiv.org/abs/2101.11605\n\n[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. *ArXiv E-Prints*, *1505*, arXiv:1505.00387.\n\n[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. *ArXiv:1707.02968 [Cs]*. http://arxiv.org/abs/1707.02968\n\n[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. *ArXiv:2011.10881 [Cs]*. http://arxiv.org/abs/2011.10881\n\n[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. *ArXiv:1409.4842 [Cs]*. http://arxiv.org/abs/1409.4842\n\n[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). *Rethinking the Inception Architecture for Computer Vision*. 2818–2826. http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\n\n[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers & distillation through attention. *ArXiv:2012.12877 [Cs]*. http://arxiv.org/abs/2012.12877\n\n[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), *Advances in Neural Information Processing Systems 30* (pp. 5998–6008). Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n\n[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. *ArXiv:2012.00759 [Cs]*. http://arxiv.org/abs/2012.00759\n\n[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. *European Conference on Computer Vision*, 108–126.\n\n[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. *2018 IEEE Winter Conference on Applications of Computer Vision (WACV)*, 1451–1460. https://doi.org/10.1109/WACV.2018.00163\n\n[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. *ArXiv:2102.12122 [Cs]*. http://arxiv.org/abs/2102.12122\n\n[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. *ArXiv:1711.07971 [Cs]*. http://arxiv.org/abs/1711.07971\n\n[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. *ArXiv:2011.14503 [Cs]*. http://arxiv.org/abs/2011.14503\n\n[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). *CBAM: Convolutional Block Attention Module*. 3–19. https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\n\n[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. *ArXiv:2006.04139 [Cs]*. http://arxiv.org/abs/2006.04139\n\n[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. *ArXiv:2101.11986 [Cs]*. http://arxiv.org/abs/2101.11986\n\n[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. *ArXiv:2007.09451 [Cs]*. http://arxiv.org/abs/2007.09451\n\n[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. *ArXiv:2012.09164 [Cs]*. http://arxiv.org/abs/2012.09164\n\n[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. *ArXiv:2011.09315 [Cs]*. http://arxiv.org/abs/2011.09315\n\n[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. *ArXiv:2012.15840 [Cs]*. http://arxiv.org/abs/2012.15840\n\n[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. *ArXiv:2010.04159 [Cs]*. http://arxiv.org/abs/2010.04159\n\n \n\n \n\n","slug":"Survey-vision-transformer","published":1,"updated":"2021-04-25T05:42:53.111Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknwqx6s60013bczwf7jyd5xa","content":"<p><strong>Note that this is a literature surevy for a course EECS 433. It was writen in Mar. 2021.</strong></p>\n<p><strong>ABSTRACT:</strong> Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.</p>\n<a id=\"more\"></a>\n\n<p><strong>KEYWORDS:</strong> <em>Self-attention, transformer, convolution neural networks, deep neural networks.</em></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.  Introduction\"></a>1.  Introduction</h1><p>Convolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.</p>\n<p>Despite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. </p>\n<p>At the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. </p>\n<p>Inspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). </p>\n<p>Even though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? </p>\n<p>The structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.</p>\n<h1 id=\"2-Related-work\"><a href=\"#2-Related-work\" class=\"headerlink\" title=\"2.  Related work\"></a>2.  Related work</h1><p>Visio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.</p>\n<h2 id=\"2-1-CNN\"><a href=\"#2-1-CNN\" class=\"headerlink\" title=\"2.1 CNN\"></a>2.1 CNN</h2><p>There are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. </p>\n<p>In 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. </p>\n<p>In 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. </p>\n<p>Considering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. </p>\n<p>In 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.</p>\n<p><img src=\"clip_image001.png\" alt=\"img\"></p>\n<p>Figure 1. DenseNet (G. Huang et al., 2016)</p>\n<p>It is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. </p>\n<h2 id=\"2-2-Attention-in-CNN\"><a href=\"#2-2-Attention-in-CNN\" class=\"headerlink\" title=\"2.2 Attention in CNN\"></a>2.2 Attention in CNN</h2><p>Considering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. </p>\n<p><img src=\"clip_image002.png\" alt=\"img\"></p>\n<p>Figure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)</p>\n<p>Inspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. </p>\n<p><img src=\"clip_image003.png\" alt=\"img\"></p>\n<p>Figure 3. Selective Kernel Convolution. (X. Li et al., 2019)</p>\n<p>Some researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. </p>\n<p>Other researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. </p>\n<p><img src=\"clip_image004.png\" alt=\"img\"></p>\n<p>Figure 4. Non-local block. (X. Wang et al., 2018)</p>\n<p>Although NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. </p>\n<p>To summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.</p>\n<p>Table 1. Model comparison for the application of attention in CNN.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Model name</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SENet</td>\n<td>Spatial aggregation, channel attention</td>\n</tr>\n<tr>\n<td>SKNet</td>\n<td>Adaptive receptive filed, channel attention</td>\n</tr>\n<tr>\n<td>BAM</td>\n<td>Channel attention and spatial attention in parallel</td>\n</tr>\n<tr>\n<td>CBAM</td>\n<td>Channel attention and spatial attention sequentially</td>\n</tr>\n<tr>\n<td>NLNet</td>\n<td>Spatial weighted sum per pixel</td>\n</tr>\n<tr>\n<td>GCNet</td>\n<td>Spatial weighted sum, channel attention</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-3-Self-attention\"><a href=\"#2-3-Self-attention\" class=\"headerlink\" title=\"2.3 Self-attention\"></a>2.3 Self-attention</h2><p>Self-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. </p>\n<p><img src=\"clip_image005.png\" alt=\"img\"></p>\n<p>Figure 5. Self-attention operation.</p>\n<p>The calculation of self-attention can be formulated as a single function: </p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image007.png\" alt=\"img\"></th>\n<th>(1)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of <img src=\"clip_image009.png\" alt=\"img\"> and <img src=\"clip_image011.png\" alt=\"img\"> aims to calculate the similarity/distance between two different vectors. The score of <img src=\"clip_image013.png\" alt=\"img\"> can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. </p>\n<p>It is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix <img src=\"clip_image011.png\" alt=\"img\">,value matrix <img src=\"clip_image016.png\" alt=\"img\"> and query matrix <img src=\"clip_image009.png\" alt=\"img\"> come from. For the encoder-decoder attention layer, <img src=\"clip_image019.png\" alt=\"img\">and <img src=\"clip_image016.png\" alt=\"img\"> come from the encoder module and <img src=\"clip_image009.png\" alt=\"img\"> comes from the previous layer. Other operations are the same. </p>\n<p>The limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image023.png\" alt=\"img\"></th>\n<th>(2)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image025.png\" alt=\"img\">, <img src=\"clip_image027.png\" alt=\"img\">, <img src=\"clip_image029.png\" alt=\"img\">, and <img src=\"clip_image031.png\" alt=\"img\">are the concatenation of <img src=\"clip_image033.png\" alt=\"img\">. </p>\n<p><img src=\"clip_image034.png\" alt=\"img\"></p>\n<p>Figure 6. Multi-head attention. (Vaswani et al., 2017)</p>\n<h2 id=\"2-4-Fourier-based-network\"><a href=\"#2-4-Fourier-based-network\" class=\"headerlink\" title=\"2.4 Fourier-based network\"></a>2.4 Fourier-based network</h2><p>Convolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. </p>\n<p>Another network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.</p>\n<p><img src=\"clip_image035.png\" alt=\"img\"></p>\n<p>Figure 7. Fourier Layer. (Z. Li et al., 2020)</p>\n<p>There are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.</p>\n<h1 id=\"3-Topic-and-its-applications\"><a href=\"#3-Topic-and-its-applications\" class=\"headerlink\" title=\"3.  Topic and its applications\"></a>3.  Topic and its applications</h1><p>In this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. </p>\n<h2 id=\"3-1-Transformer\"><a href=\"#3-1-Transformer\" class=\"headerlink\" title=\"3.1 Transformer\"></a>3.1 Transformer</h2><p>In the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. </p>\n<p><img src=\"clip_image036.png\" alt=\"img\"></p>\n<p>Figure 8. The architecture for transformer. (Vaswani et al., 2017)</p>\n<p>The first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. </p>\n<p>The second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image038.png\" alt=\"img\">,</th>\n<th>(3)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image040.png\" alt=\"img\">and <img src=\"clip_image042.png\" alt=\"img\"> are weights for the two linear transformation layers, <img src=\"clip_image044.png\" alt=\"img\"> is ReLU activation function. </p>\n<p>The third thing is Add &amp; Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. </p>\n<p>The last thing is the output layer. After passing through <img src=\"clip_image046.png\" alt=\"img\">decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.</p>\n<p>To summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.</p>\n<h2 id=\"3-2-Applications\"><a href=\"#3-2-Applications\" class=\"headerlink\" title=\"3.2 Applications\"></a>3.2 Applications</h2><p>Since 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.</p>\n<p>Inspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. </p>\n<h2 id=\"3-2-1-Image-classification\"><a href=\"#3-2-1-Image-classification\" class=\"headerlink\" title=\"3.2.1  Image classification\"></a>3.2.1  Image classification</h2><p>ViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image <img src=\"clip_image048.png\" alt=\"img\"> as a sequence of flattened 2D patches <img src=\"clip_image048.png\" alt=\"img\">, where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. </p>\n<p><img src=\"clip_image050.png\" alt=\"img\"></p>\n<p>Figure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)</p>\n<p>ViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. </p>\n<p><img src=\"clip_image051.png\" alt=\"img\"></p>\n<p>Figure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)</p>\n<p>ViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. </p>\n<p>Another vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.</p>\n<p><img src=\"clip_image053.png\" alt=\"img\"></p>\n<p>Figure 11. T2T architecture and feature visualization. (Yuan et al., 2021)</p>\n<p>Viewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. </p>\n<p><img src=\"clip_image054.png\" alt=\"img\"></p>\n<p>Figure 12. TNT block and TNT framework. (Han et al., 2021)</p>\n<p>Bottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.</p>\n<p><img src=\"clip_image055.png\" alt=\"img\"></p>\n<p>Figure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)</p>\n<h2 id=\"3-2-2-Object-detection\"><a href=\"#3-2-2-Object-detection\" class=\"headerlink\" title=\"3.2.2  Object detection\"></a>3.2.2  Object detection</h2><p>Compared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. </p>\n<p><img src=\"clip_image056.png\" alt=\"img\"></p>\n<p>Figure 14. The architecture for DETR. (Carion et al., 2020)</p>\n<p>To solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.</p>\n<p>Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. </p>\n<p>Adaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. </p>\n<p>Sun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.</p>\n<h2 id=\"3-2-3-Segmentation\"><a href=\"#3-2-3-Segmentation\" class=\"headerlink\" title=\"3.2.3  Segmentation\"></a>3.2.3  Segmentation</h2><p>Segmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. </p>\n<p><img src=\"clip_image057.png\" alt=\"img\"></p>\n<p>Figure 15. The architecture for VisTR. (Y. Wang et al., 2020)</p>\n<p>Point Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. </p>\n<h1 id=\"4-Classification-and-comparison-of-various-methods\"><a href=\"#4-Classification-and-comparison-of-various-methods\" class=\"headerlink\" title=\"4.  Classification and comparison of various methods\"></a>4.  Classification and comparison of various methods</h1><p>Transformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. </p>\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong>Category</strong></th>\n<th><strong>Method</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>ViT   (Dosovitskiy  et al., 2020)</td>\n<td>Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + CNN</td>\n<td>DeiT   (Touvron  et al., 2021)</td>\n<td>Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>T2T-ViT   (Yuan et  al., 2021)</td>\n<td>Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>TNT   (Han et  al., 2021)</td>\n<td>Operate on patch-level and pixel-level, more diverse  feature extraction</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN</td>\n<td>DETR   (Carion  et al., 2020)</td>\n<td>End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN + efficient attention</td>\n<td>Deformable DETR   (Zhu et  al., 2020)</td>\n<td>Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)</td>\n<td>ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>VisTR   (Y. Wang  et al., 2020)</td>\n<td>instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction</td>\n</tr>\n<tr>\n<td>Image processing</td>\n<td>Transformer + CNN</td>\n<td>Image processing transformer  (IPT) (H. Chen  et al., 2020)</td>\n<td>The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning</td>\n</tr>\n<tr>\n<td>Image colorization</td>\n<td>Transformer + global attention</td>\n<td>Colorization Transformer (ColTran) (Kumar et  al., 2020)</td>\n<td>Conditional transformer layers, two parallel networks to upsample  low resolution images</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + efficient attention</td>\n<td>Criss-Cross Network (CCNet)</td>\n<td>Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance,</td>\n</tr>\n<tr>\n<td>NLP, architecture exploration</td>\n<td>Transformer + efficient attention</td>\n<td>ConvBERT (Jiang et al., 2020)</td>\n<td>Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies</td>\n</tr>\n</tbody></table>\n<h1 id=\"5-Future-work\"><a href=\"#5-Future-work\" class=\"headerlink\" title=\"5.  Future work\"></a>5.  Future work</h1><p>Transformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. </p>\n<p>The first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. </p>\n<p>The second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. </p>\n<p>The third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.</p>\n<p>The fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. </p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.  Conclusion\"></a>6.  Conclusion</h1><p>In this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.</p>\n<h1 id=\"7-Course-feedbacks\"><a href=\"#7-Course-feedbacks\" class=\"headerlink\" title=\"7.  Course feedbacks\"></a>7.  Course feedbacks</h1><p>This course is a great graduate-level class. There are several reasons for that:</p>\n<p>\\1.   <strong>Rich course content.</strong> From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. </p>\n<p>\\2.   <strong>Excellent lectures.</strong> Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.</p>\n<p>\\3.   <strong>Novel and creative illustration</strong>. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.</p>\n<p>\\4.   <strong>Excellent reading recommendation</strong>. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.</p>\n<p>\\5.   For every lecture, professor always pointed out the “<strong>take away home message</strong>”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.</p>\n<p>\\6.   <strong>The slides are clear and well-organized.</strong> I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.</p>\n<h1 id=\"8-Reference\"><a href=\"#8-Reference\" class=\"headerlink\" title=\"8.  Reference\"></a>8.  Reference</h1><p>[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. <em>ArXiv:2012.09958 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09958\">http://arxiv.org/abs/2012.09958</a></p>\n<p>[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks</em>, <em>5</em>(2), 157–166. <a href=\"https://doi.org/10.1109/72.279181\">https://doi.org/10.1109/72.279181</a></p>\n<p>[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). <em>Language Models are Few-Shot Learners</em>. <a href=\"https://arxiv.org/abs/2005.14165v4\">https://arxiv.org/abs/2005.14165v4</a></p>\n<p>[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). <em>GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond</em>. 0–0. <a href=\"https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\">https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html</a></p>\n<p>[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. <em>ArXiv:2005.12872 [Cs]</em>. <a href=\"http://arxiv.org/abs/2005.12872\">http://arxiv.org/abs/2005.12872</a></p>\n<p>[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. <em>ArXiv:2012.00364 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00364\">http://arxiv.org/abs/2012.00364</a></p>\n<p>[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. <em>International Conference on Machine Learning</em>, 1691–1703. <a href=\"http://proceedings.mlr.press/v119/chen20s.html\">http://proceedings.mlr.press/v119/chen20s.html</a></p>\n<p>[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. <em>ArXiv:1606.03657 [Cs, Stat]</em>. <a href=\"http://arxiv.org/abs/1606.03657\">http://arxiv.org/abs/1606.03657</a></p>\n<p>[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. <em>ArXiv:1601.06733 [Cs]</em>. <a href=\"http://arxiv.org/abs/1601.06733\">http://arxiv.org/abs/1601.06733</a></p>\n<p>[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255.</p>\n<p>[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>ArXiv:1810.04805 [Cs]</em>. <a href=\"http://arxiv.org/abs/1810.04805\">http://arxiv.org/abs/1810.04805</a></p>\n<p>[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>ArXiv:2010.11929 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.11929\">http://arxiv.org/abs/2010.11929</a></p>\n<p>[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–256. <a href=\"http://proceedings.mlr.press/v9/glorot10a.html\">http://proceedings.mlr.press/v9/glorot10a.html</a></p>\n<p>[14]         Goodfellow, I. (2016). <em>Generative Adversarial Networks (GANs)</em>. 86.</p>\n<p>[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>\n<p>[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. <em>ArXiv:2103.00112 [Cs]</em>. <a href=\"http://arxiv.org/abs/2103.00112\">http://arxiv.org/abs/2103.00112</a></p>\n<p>[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. <em>ArXiv:1512.03385 [Cs]</em>. <a href=\"http://arxiv.org/abs/1512.03385\">http://arxiv.org/abs/1512.03385</a></p>\n<p>[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. <em>ArXiv:1704.04861 [Cs]</em>. <a href=\"http://arxiv.org/abs/1704.04861\">http://arxiv.org/abs/1704.04861</a></p>\n<p>[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. <em>ArXiv:1709.01507 [Cs]</em>. <a href=\"http://arxiv.org/abs/1709.01507\">http://arxiv.org/abs/1709.01507</a></p>\n<p>[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. <em>ArXiv:1608.06993 [Cs]</em>. <a href=\"http://arxiv.org/abs/1608.06993\">http://arxiv.org/abs/1608.06993</a></p>\n<p>[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 603–612.</p>\n<p>[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. <em>ArXiv Preprint ArXiv:2008.02496</em>.</p>\n<p>[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), <em>Advances in Neural Information Processing Systems 25</em> (pp. 1097–1105). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<p>[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). <em>Colorization Transformer</em>. International Conference on Learning Representations. <a href=\"https://openreview.net/forum?id=5NA1PinlGFu\">https://openreview.net/forum?id=5NA1PinlGFu</a></p>\n<p>[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. <em>Neural Computation</em>, <em>1</em>(4), 541–551. <a href=\"https://doi.org/10.1162/neco.1989.1.4.541\">https://doi.org/10.1162/neco.1989.1.4.541</a></p>\n<p>[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. <em>ArXiv:1903.06586 [Cs]</em>. <a href=\"http://arxiv.org/abs/1903.06586\">http://arxiv.org/abs/1903.06586</a></p>\n<p>[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). <em>Fourier Neural Operator for Parametric Partial Differential Equations</em>. <a href=\"https://arxiv.org/abs/2010.08895v1\">https://arxiv.org/abs/2010.08895v1</a></p>\n<p>[28]         Lu Chi. (2020). Fast Fourier Convolution. <em>Neural Information Processing Systems</em>, 767–774. <a href=\"https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\">https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf</a></p>\n<p>[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. <em>ArXiv:1508.04025 [Cs]</em>. <a href=\"http://arxiv.org/abs/1508.04025\">http://arxiv.org/abs/1508.04025</a></p>\n<p>[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>, 807–814.</p>\n<p>[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. <em>ArXiv:1807.06514 [Cs]</em>. <a href=\"http://arxiv.org/abs/1807.06514\">http://arxiv.org/abs/1807.06514</a></p>\n<p>[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. <em>Undefined</em>. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</p>\n<p>[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em>Improving language understanding by generative pre-training</em>.</p>\n<p>[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. <em>ArXiv:1804.02767 [Cs]</em>. <a href=\"http://arxiv.org/abs/1804.02767\">http://arxiv.org/abs/1804.02767</a></p>\n<p>[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>ArXiv:1506.01497 [Cs]</em>. <a href=\"http://arxiv.org/abs/1506.01497\">http://arxiv.org/abs/1506.01497</a></p>\n<p>[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. <em>ArXiv:1409.0575 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.0575\">http://arxiv.org/abs/1409.0575</a></p>\n<p>[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>ArXiv:1409.1556 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.1556\">http://arxiv.org/abs/1409.1556</a></p>\n<p>[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. <em>ArXiv:2101.11605 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11605\">http://arxiv.org/abs/2101.11605</a></p>\n<p>[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. <em>ArXiv E-Prints</em>, <em>1505</em>, arXiv:1505.00387.</p>\n<p>[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. <em>ArXiv:1707.02968 [Cs]</em>. <a href=\"http://arxiv.org/abs/1707.02968\">http://arxiv.org/abs/1707.02968</a></p>\n<p>[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. <em>ArXiv:2011.10881 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.10881\">http://arxiv.org/abs/2011.10881</a></p>\n<p>[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. <em>ArXiv:1409.4842 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.4842\">http://arxiv.org/abs/1409.4842</a></p>\n<p>[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). <em>Rethinking the Inception Architecture for Computer Vision</em>. 2818–2826. <a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html</a></p>\n<p>[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. <em>ArXiv:2012.12877 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.12877\">http://arxiv.org/abs/2012.12877</a></p>\n<p>[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems 30</em> (pp. 5998–6008). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>\n<p>[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. <em>ArXiv:2012.00759 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00759\">http://arxiv.org/abs/2012.00759</a></p>\n<p>[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. <em>European Conference on Computer Vision</em>, 108–126.</p>\n<p>[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1451–1460. <a href=\"https://doi.org/10.1109/WACV.2018.00163\">https://doi.org/10.1109/WACV.2018.00163</a></p>\n<p>[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. <em>ArXiv:2102.12122 [Cs]</em>. <a href=\"http://arxiv.org/abs/2102.12122\">http://arxiv.org/abs/2102.12122</a></p>\n<p>[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. <em>ArXiv:1711.07971 [Cs]</em>. <a href=\"http://arxiv.org/abs/1711.07971\">http://arxiv.org/abs/1711.07971</a></p>\n<p>[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. <em>ArXiv:2011.14503 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.14503\">http://arxiv.org/abs/2011.14503</a></p>\n<p>[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). <em>CBAM: Convolutional Block Attention Module</em>. 3–19. <a href=\"https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\">https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html</a></p>\n<p>[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. <em>ArXiv:2006.04139 [Cs]</em>. <a href=\"http://arxiv.org/abs/2006.04139\">http://arxiv.org/abs/2006.04139</a></p>\n<p>[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. <em>ArXiv:2101.11986 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11986\">http://arxiv.org/abs/2101.11986</a></p>\n<p>[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. <em>ArXiv:2007.09451 [Cs]</em>. <a href=\"http://arxiv.org/abs/2007.09451\">http://arxiv.org/abs/2007.09451</a></p>\n<p>[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. <em>ArXiv:2012.09164 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09164\">http://arxiv.org/abs/2012.09164</a></p>\n<p>[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. <em>ArXiv:2011.09315 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.09315\">http://arxiv.org/abs/2011.09315</a></p>\n<p>[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. <em>ArXiv:2012.15840 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.15840\">http://arxiv.org/abs/2012.15840</a></p>\n<p>[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. <em>ArXiv:2010.04159 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.04159\">http://arxiv.org/abs/2010.04159</a></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Jave":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<p><strong>Note that this is a literature surevy for a course EECS 433. It was writen in Mar. 2021.</strong></p>\n<p><strong>ABSTRACT:</strong> Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.</p>","more":"<p><strong>KEYWORDS:</strong> <em>Self-attention, transformer, convolution neural networks, deep neural networks.</em></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.  Introduction\"></a>1.  Introduction</h1><p>Convolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.</p>\n<p>Despite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. </p>\n<p>At the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. </p>\n<p>Inspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). </p>\n<p>Even though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? </p>\n<p>The structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.</p>\n<h1 id=\"2-Related-work\"><a href=\"#2-Related-work\" class=\"headerlink\" title=\"2.  Related work\"></a>2.  Related work</h1><p>Visio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.</p>\n<h2 id=\"2-1-CNN\"><a href=\"#2-1-CNN\" class=\"headerlink\" title=\"2.1 CNN\"></a>2.1 CNN</h2><p>There are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. </p>\n<p>In 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. </p>\n<p>In 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. </p>\n<p>Considering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. </p>\n<p>In 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.</p>\n<p><img src=\"clip_image001.png\" alt=\"img\"></p>\n<p>Figure 1. DenseNet (G. Huang et al., 2016)</p>\n<p>It is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. </p>\n<h2 id=\"2-2-Attention-in-CNN\"><a href=\"#2-2-Attention-in-CNN\" class=\"headerlink\" title=\"2.2 Attention in CNN\"></a>2.2 Attention in CNN</h2><p>Considering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. </p>\n<p><img src=\"clip_image002.png\" alt=\"img\"></p>\n<p>Figure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)</p>\n<p>Inspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. </p>\n<p><img src=\"clip_image003.png\" alt=\"img\"></p>\n<p>Figure 3. Selective Kernel Convolution. (X. Li et al., 2019)</p>\n<p>Some researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. </p>\n<p>Other researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. </p>\n<p><img src=\"clip_image004.png\" alt=\"img\"></p>\n<p>Figure 4. Non-local block. (X. Wang et al., 2018)</p>\n<p>Although NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. </p>\n<p>To summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.</p>\n<p>Table 1. Model comparison for the application of attention in CNN.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Model name</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SENet</td>\n<td>Spatial aggregation, channel attention</td>\n</tr>\n<tr>\n<td>SKNet</td>\n<td>Adaptive receptive filed, channel attention</td>\n</tr>\n<tr>\n<td>BAM</td>\n<td>Channel attention and spatial attention in parallel</td>\n</tr>\n<tr>\n<td>CBAM</td>\n<td>Channel attention and spatial attention sequentially</td>\n</tr>\n<tr>\n<td>NLNet</td>\n<td>Spatial weighted sum per pixel</td>\n</tr>\n<tr>\n<td>GCNet</td>\n<td>Spatial weighted sum, channel attention</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-3-Self-attention\"><a href=\"#2-3-Self-attention\" class=\"headerlink\" title=\"2.3 Self-attention\"></a>2.3 Self-attention</h2><p>Self-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. </p>\n<p><img src=\"clip_image005.png\" alt=\"img\"></p>\n<p>Figure 5. Self-attention operation.</p>\n<p>The calculation of self-attention can be formulated as a single function: </p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image007.png\" alt=\"img\"></th>\n<th>(1)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of <img src=\"clip_image009.png\" alt=\"img\"> and <img src=\"clip_image011.png\" alt=\"img\"> aims to calculate the similarity/distance between two different vectors. The score of <img src=\"clip_image013.png\" alt=\"img\"> can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. </p>\n<p>It is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix <img src=\"clip_image011.png\" alt=\"img\">,value matrix <img src=\"clip_image016.png\" alt=\"img\"> and query matrix <img src=\"clip_image009.png\" alt=\"img\"> come from. For the encoder-decoder attention layer, <img src=\"clip_image019.png\" alt=\"img\">and <img src=\"clip_image016.png\" alt=\"img\"> come from the encoder module and <img src=\"clip_image009.png\" alt=\"img\"> comes from the previous layer. Other operations are the same. </p>\n<p>The limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image023.png\" alt=\"img\"></th>\n<th>(2)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image025.png\" alt=\"img\">, <img src=\"clip_image027.png\" alt=\"img\">, <img src=\"clip_image029.png\" alt=\"img\">, and <img src=\"clip_image031.png\" alt=\"img\">are the concatenation of <img src=\"clip_image033.png\" alt=\"img\">. </p>\n<p><img src=\"clip_image034.png\" alt=\"img\"></p>\n<p>Figure 6. Multi-head attention. (Vaswani et al., 2017)</p>\n<h2 id=\"2-4-Fourier-based-network\"><a href=\"#2-4-Fourier-based-network\" class=\"headerlink\" title=\"2.4 Fourier-based network\"></a>2.4 Fourier-based network</h2><p>Convolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. </p>\n<p>Another network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.</p>\n<p><img src=\"clip_image035.png\" alt=\"img\"></p>\n<p>Figure 7. Fourier Layer. (Z. Li et al., 2020)</p>\n<p>There are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.</p>\n<h1 id=\"3-Topic-and-its-applications\"><a href=\"#3-Topic-and-its-applications\" class=\"headerlink\" title=\"3.  Topic and its applications\"></a>3.  Topic and its applications</h1><p>In this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. </p>\n<h2 id=\"3-1-Transformer\"><a href=\"#3-1-Transformer\" class=\"headerlink\" title=\"3.1 Transformer\"></a>3.1 Transformer</h2><p>In the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. </p>\n<p><img src=\"clip_image036.png\" alt=\"img\"></p>\n<p>Figure 8. The architecture for transformer. (Vaswani et al., 2017)</p>\n<p>The first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. </p>\n<p>The second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image038.png\" alt=\"img\">,</th>\n<th>(3)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image040.png\" alt=\"img\">and <img src=\"clip_image042.png\" alt=\"img\"> are weights for the two linear transformation layers, <img src=\"clip_image044.png\" alt=\"img\"> is ReLU activation function. </p>\n<p>The third thing is Add &amp; Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. </p>\n<p>The last thing is the output layer. After passing through <img src=\"clip_image046.png\" alt=\"img\">decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.</p>\n<p>To summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.</p>\n<h2 id=\"3-2-Applications\"><a href=\"#3-2-Applications\" class=\"headerlink\" title=\"3.2 Applications\"></a>3.2 Applications</h2><p>Since 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.</p>\n<p>Inspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. </p>\n<h2 id=\"3-2-1-Image-classification\"><a href=\"#3-2-1-Image-classification\" class=\"headerlink\" title=\"3.2.1  Image classification\"></a>3.2.1  Image classification</h2><p>ViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image <img src=\"clip_image048.png\" alt=\"img\"> as a sequence of flattened 2D patches <img src=\"clip_image048.png\" alt=\"img\">, where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. </p>\n<p><img src=\"clip_image050.png\" alt=\"img\"></p>\n<p>Figure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)</p>\n<p>ViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. </p>\n<p><img src=\"clip_image051.png\" alt=\"img\"></p>\n<p>Figure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)</p>\n<p>ViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. </p>\n<p>Another vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.</p>\n<p><img src=\"clip_image053.png\" alt=\"img\"></p>\n<p>Figure 11. T2T architecture and feature visualization. (Yuan et al., 2021)</p>\n<p>Viewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. </p>\n<p><img src=\"clip_image054.png\" alt=\"img\"></p>\n<p>Figure 12. TNT block and TNT framework. (Han et al., 2021)</p>\n<p>Bottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.</p>\n<p><img src=\"clip_image055.png\" alt=\"img\"></p>\n<p>Figure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)</p>\n<h2 id=\"3-2-2-Object-detection\"><a href=\"#3-2-2-Object-detection\" class=\"headerlink\" title=\"3.2.2  Object detection\"></a>3.2.2  Object detection</h2><p>Compared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. </p>\n<p><img src=\"clip_image056.png\" alt=\"img\"></p>\n<p>Figure 14. The architecture for DETR. (Carion et al., 2020)</p>\n<p>To solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.</p>\n<p>Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. </p>\n<p>Adaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. </p>\n<p>Sun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.</p>\n<h2 id=\"3-2-3-Segmentation\"><a href=\"#3-2-3-Segmentation\" class=\"headerlink\" title=\"3.2.3  Segmentation\"></a>3.2.3  Segmentation</h2><p>Segmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. </p>\n<p><img src=\"clip_image057.png\" alt=\"img\"></p>\n<p>Figure 15. The architecture for VisTR. (Y. Wang et al., 2020)</p>\n<p>Point Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. </p>\n<h1 id=\"4-Classification-and-comparison-of-various-methods\"><a href=\"#4-Classification-and-comparison-of-various-methods\" class=\"headerlink\" title=\"4.  Classification and comparison of various methods\"></a>4.  Classification and comparison of various methods</h1><p>Transformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. </p>\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong>Category</strong></th>\n<th><strong>Method</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>ViT   (Dosovitskiy  et al., 2020)</td>\n<td>Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + CNN</td>\n<td>DeiT   (Touvron  et al., 2021)</td>\n<td>Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>T2T-ViT   (Yuan et  al., 2021)</td>\n<td>Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>TNT   (Han et  al., 2021)</td>\n<td>Operate on patch-level and pixel-level, more diverse  feature extraction</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN</td>\n<td>DETR   (Carion  et al., 2020)</td>\n<td>End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN + efficient attention</td>\n<td>Deformable DETR   (Zhu et  al., 2020)</td>\n<td>Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)</td>\n<td>ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>VisTR   (Y. Wang  et al., 2020)</td>\n<td>instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction</td>\n</tr>\n<tr>\n<td>Image processing</td>\n<td>Transformer + CNN</td>\n<td>Image processing transformer  (IPT) (H. Chen  et al., 2020)</td>\n<td>The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning</td>\n</tr>\n<tr>\n<td>Image colorization</td>\n<td>Transformer + global attention</td>\n<td>Colorization Transformer (ColTran) (Kumar et  al., 2020)</td>\n<td>Conditional transformer layers, two parallel networks to upsample  low resolution images</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + efficient attention</td>\n<td>Criss-Cross Network (CCNet)</td>\n<td>Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance,</td>\n</tr>\n<tr>\n<td>NLP, architecture exploration</td>\n<td>Transformer + efficient attention</td>\n<td>ConvBERT (Jiang et al., 2020)</td>\n<td>Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies</td>\n</tr>\n</tbody></table>\n<h1 id=\"5-Future-work\"><a href=\"#5-Future-work\" class=\"headerlink\" title=\"5.  Future work\"></a>5.  Future work</h1><p>Transformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. </p>\n<p>The first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. </p>\n<p>The second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. </p>\n<p>The third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.</p>\n<p>The fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. </p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.  Conclusion\"></a>6.  Conclusion</h1><p>In this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.</p>\n<h1 id=\"7-Course-feedbacks\"><a href=\"#7-Course-feedbacks\" class=\"headerlink\" title=\"7.  Course feedbacks\"></a>7.  Course feedbacks</h1><p>This course is a great graduate-level class. There are several reasons for that:</p>\n<p>\\1.   <strong>Rich course content.</strong> From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. </p>\n<p>\\2.   <strong>Excellent lectures.</strong> Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.</p>\n<p>\\3.   <strong>Novel and creative illustration</strong>. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.</p>\n<p>\\4.   <strong>Excellent reading recommendation</strong>. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.</p>\n<p>\\5.   For every lecture, professor always pointed out the “<strong>take away home message</strong>”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.</p>\n<p>\\6.   <strong>The slides are clear and well-organized.</strong> I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.</p>\n<h1 id=\"8-Reference\"><a href=\"#8-Reference\" class=\"headerlink\" title=\"8.  Reference\"></a>8.  Reference</h1><p>[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. <em>ArXiv:2012.09958 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09958\">http://arxiv.org/abs/2012.09958</a></p>\n<p>[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks</em>, <em>5</em>(2), 157–166. <a href=\"https://doi.org/10.1109/72.279181\">https://doi.org/10.1109/72.279181</a></p>\n<p>[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). <em>Language Models are Few-Shot Learners</em>. <a href=\"https://arxiv.org/abs/2005.14165v4\">https://arxiv.org/abs/2005.14165v4</a></p>\n<p>[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). <em>GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond</em>. 0–0. <a href=\"https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\">https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html</a></p>\n<p>[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. <em>ArXiv:2005.12872 [Cs]</em>. <a href=\"http://arxiv.org/abs/2005.12872\">http://arxiv.org/abs/2005.12872</a></p>\n<p>[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. <em>ArXiv:2012.00364 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00364\">http://arxiv.org/abs/2012.00364</a></p>\n<p>[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. <em>International Conference on Machine Learning</em>, 1691–1703. <a href=\"http://proceedings.mlr.press/v119/chen20s.html\">http://proceedings.mlr.press/v119/chen20s.html</a></p>\n<p>[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. <em>ArXiv:1606.03657 [Cs, Stat]</em>. <a href=\"http://arxiv.org/abs/1606.03657\">http://arxiv.org/abs/1606.03657</a></p>\n<p>[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. <em>ArXiv:1601.06733 [Cs]</em>. <a href=\"http://arxiv.org/abs/1601.06733\">http://arxiv.org/abs/1601.06733</a></p>\n<p>[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255.</p>\n<p>[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>ArXiv:1810.04805 [Cs]</em>. <a href=\"http://arxiv.org/abs/1810.04805\">http://arxiv.org/abs/1810.04805</a></p>\n<p>[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>ArXiv:2010.11929 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.11929\">http://arxiv.org/abs/2010.11929</a></p>\n<p>[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–256. <a href=\"http://proceedings.mlr.press/v9/glorot10a.html\">http://proceedings.mlr.press/v9/glorot10a.html</a></p>\n<p>[14]         Goodfellow, I. (2016). <em>Generative Adversarial Networks (GANs)</em>. 86.</p>\n<p>[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>\n<p>[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. <em>ArXiv:2103.00112 [Cs]</em>. <a href=\"http://arxiv.org/abs/2103.00112\">http://arxiv.org/abs/2103.00112</a></p>\n<p>[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. <em>ArXiv:1512.03385 [Cs]</em>. <a href=\"http://arxiv.org/abs/1512.03385\">http://arxiv.org/abs/1512.03385</a></p>\n<p>[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. <em>ArXiv:1704.04861 [Cs]</em>. <a href=\"http://arxiv.org/abs/1704.04861\">http://arxiv.org/abs/1704.04861</a></p>\n<p>[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. <em>ArXiv:1709.01507 [Cs]</em>. <a href=\"http://arxiv.org/abs/1709.01507\">http://arxiv.org/abs/1709.01507</a></p>\n<p>[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. <em>ArXiv:1608.06993 [Cs]</em>. <a href=\"http://arxiv.org/abs/1608.06993\">http://arxiv.org/abs/1608.06993</a></p>\n<p>[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 603–612.</p>\n<p>[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. <em>ArXiv Preprint ArXiv:2008.02496</em>.</p>\n<p>[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), <em>Advances in Neural Information Processing Systems 25</em> (pp. 1097–1105). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<p>[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). <em>Colorization Transformer</em>. International Conference on Learning Representations. <a href=\"https://openreview.net/forum?id=5NA1PinlGFu\">https://openreview.net/forum?id=5NA1PinlGFu</a></p>\n<p>[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. <em>Neural Computation</em>, <em>1</em>(4), 541–551. <a href=\"https://doi.org/10.1162/neco.1989.1.4.541\">https://doi.org/10.1162/neco.1989.1.4.541</a></p>\n<p>[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. <em>ArXiv:1903.06586 [Cs]</em>. <a href=\"http://arxiv.org/abs/1903.06586\">http://arxiv.org/abs/1903.06586</a></p>\n<p>[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). <em>Fourier Neural Operator for Parametric Partial Differential Equations</em>. <a href=\"https://arxiv.org/abs/2010.08895v1\">https://arxiv.org/abs/2010.08895v1</a></p>\n<p>[28]         Lu Chi. (2020). Fast Fourier Convolution. <em>Neural Information Processing Systems</em>, 767–774. <a href=\"https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\">https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf</a></p>\n<p>[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. <em>ArXiv:1508.04025 [Cs]</em>. <a href=\"http://arxiv.org/abs/1508.04025\">http://arxiv.org/abs/1508.04025</a></p>\n<p>[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>, 807–814.</p>\n<p>[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. <em>ArXiv:1807.06514 [Cs]</em>. <a href=\"http://arxiv.org/abs/1807.06514\">http://arxiv.org/abs/1807.06514</a></p>\n<p>[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. <em>Undefined</em>. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</p>\n<p>[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em>Improving language understanding by generative pre-training</em>.</p>\n<p>[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. <em>ArXiv:1804.02767 [Cs]</em>. <a href=\"http://arxiv.org/abs/1804.02767\">http://arxiv.org/abs/1804.02767</a></p>\n<p>[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>ArXiv:1506.01497 [Cs]</em>. <a href=\"http://arxiv.org/abs/1506.01497\">http://arxiv.org/abs/1506.01497</a></p>\n<p>[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. <em>ArXiv:1409.0575 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.0575\">http://arxiv.org/abs/1409.0575</a></p>\n<p>[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>ArXiv:1409.1556 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.1556\">http://arxiv.org/abs/1409.1556</a></p>\n<p>[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. <em>ArXiv:2101.11605 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11605\">http://arxiv.org/abs/2101.11605</a></p>\n<p>[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. <em>ArXiv E-Prints</em>, <em>1505</em>, arXiv:1505.00387.</p>\n<p>[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. <em>ArXiv:1707.02968 [Cs]</em>. <a href=\"http://arxiv.org/abs/1707.02968\">http://arxiv.org/abs/1707.02968</a></p>\n<p>[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. <em>ArXiv:2011.10881 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.10881\">http://arxiv.org/abs/2011.10881</a></p>\n<p>[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. <em>ArXiv:1409.4842 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.4842\">http://arxiv.org/abs/1409.4842</a></p>\n<p>[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). <em>Rethinking the Inception Architecture for Computer Vision</em>. 2818–2826. <a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html</a></p>\n<p>[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. <em>ArXiv:2012.12877 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.12877\">http://arxiv.org/abs/2012.12877</a></p>\n<p>[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems 30</em> (pp. 5998–6008). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>\n<p>[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. <em>ArXiv:2012.00759 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00759\">http://arxiv.org/abs/2012.00759</a></p>\n<p>[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. <em>European Conference on Computer Vision</em>, 108–126.</p>\n<p>[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1451–1460. <a href=\"https://doi.org/10.1109/WACV.2018.00163\">https://doi.org/10.1109/WACV.2018.00163</a></p>\n<p>[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. <em>ArXiv:2102.12122 [Cs]</em>. <a href=\"http://arxiv.org/abs/2102.12122\">http://arxiv.org/abs/2102.12122</a></p>\n<p>[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. <em>ArXiv:1711.07971 [Cs]</em>. <a href=\"http://arxiv.org/abs/1711.07971\">http://arxiv.org/abs/1711.07971</a></p>\n<p>[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. <em>ArXiv:2011.14503 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.14503\">http://arxiv.org/abs/2011.14503</a></p>\n<p>[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). <em>CBAM: Convolutional Block Attention Module</em>. 3–19. <a href=\"https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\">https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html</a></p>\n<p>[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. <em>ArXiv:2006.04139 [Cs]</em>. <a href=\"http://arxiv.org/abs/2006.04139\">http://arxiv.org/abs/2006.04139</a></p>\n<p>[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. <em>ArXiv:2101.11986 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11986\">http://arxiv.org/abs/2101.11986</a></p>\n<p>[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. <em>ArXiv:2007.09451 [Cs]</em>. <a href=\"http://arxiv.org/abs/2007.09451\">http://arxiv.org/abs/2007.09451</a></p>\n<p>[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. <em>ArXiv:2012.09164 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09164\">http://arxiv.org/abs/2012.09164</a></p>\n<p>[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. <em>ArXiv:2011.09315 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.09315\">http://arxiv.org/abs/2011.09315</a></p>\n<p>[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. <em>ArXiv:2012.15840 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.15840\">http://arxiv.org/abs/2012.15840</a></p>\n<p>[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. <em>ArXiv:2010.04159 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.04159\">http://arxiv.org/abs/2010.04159</a></p>"}],"PostAsset":[{"_id":"source/_posts/CS61B-Week2/image-20210425113258793.png","post":"cknwqx6rl0001bczw2h2d2tks","slug":"image-20210425113258793.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/1.png","post":"cknwqx6s0000qbczw7th6704d","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/2.png","post":"cknwqx6s0000qbczw7th6704d","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/3.png","post":"cknwqx6s0000qbczw7th6704d","slug":"3.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/4.png","post":"cknwqx6s0000qbczw7th6704d","slug":"4.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/5.png","post":"cknwqx6s0000qbczw7th6704d","slug":"5.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/6.png","post":"cknwqx6s0000qbczw7th6704d","slug":"6.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/7.png","post":"cknwqx6s0000qbczw7th6704d","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214173356481.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214173356481.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214173840928.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214173840928.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174003005.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174003005.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174108194.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174108194.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174310749.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174310749.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174422276.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174422276.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174809740.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174809740.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174913278.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174913278.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174957751.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214174957751.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214175022879.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214175022879.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214175304898.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214175304898.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214182334199.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214182334199.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214182520461.png","post":"cknwqx6s1000rbczwbqqs856l","slug":"image-20210214182520461.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image001.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image001.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image002.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image002.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image003.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image003.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image004.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image004.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image005.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image005.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image007.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image007.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image009.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image009.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image011.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image011.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image013.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image013.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image016.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image016.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image019.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image019.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image023.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image023.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image025.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image025.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image027.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image027.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image029.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image029.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image031.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image031.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image033.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image033.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image034.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image034.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image035.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image035.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image036.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image036.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image038.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image038.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image040.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image040.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image042.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image042.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image044.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image044.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image046.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image046.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image048.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image048.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image050.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image050.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image051.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image051.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image053.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image053.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image054.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image054.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image055.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image055.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image056.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image056.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image057.png","post":"cknwqx6s60013bczwf7jyd5xa","slug":"clip_image057.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cknwqx6rl0001bczw2h2d2tks","category_id":"cknwqx6rq0004bczw6onuhp7h","_id":"cknwqx6rw000abczwe9me789p"},{"post_id":"cknwqx6ro0003bczw232x0w2f","category_id":"cknwqx6rq0004bczw6onuhp7h","_id":"cknwqx6rx000cbczw2h275deg"},{"post_id":"cknwqx6s1000rbczwbqqs856l","category_id":"cknwqx6rq0004bczw6onuhp7h","_id":"cknwqx6s3000wbczwfmqr702j"},{"post_id":"cknwqx6s0000qbczw7th6704d","category_id":"cknwqx6s2000sbczwht4h1uys","_id":"cknwqx6s3000zbczw9qf4d7rx"}],"PostTag":[{"post_id":"cknwqx6rl0001bczw2h2d2tks","tag_id":"cknwqx6rr0005bczw6kc254ml","_id":"cknwqx6rx000fbczwa8sf1v73"},{"post_id":"cknwqx6rl0001bczw2h2d2tks","tag_id":"cknwqx6ru0009bczw0uu0b9uu","_id":"cknwqx6rx000gbczwhrkhbipq"},{"post_id":"cknwqx6rl0001bczw2h2d2tks","tag_id":"cknwqx6rw000bbczw2ga32cq9","_id":"cknwqx6ry000ibczwca6sa8z6"},{"post_id":"cknwqx6rl0001bczw2h2d2tks","tag_id":"cknwqx6rx000dbczwamqv2el8","_id":"cknwqx6ry000jbczwcg0g490b"},{"post_id":"cknwqx6ro0003bczw232x0w2f","tag_id":"cknwqx6rx000ebczwbwhd9gu5","_id":"cknwqx6rz000mbczw84vpcmld"},{"post_id":"cknwqx6ro0003bczw232x0w2f","tag_id":"cknwqx6rx000hbczwb4a351wn","_id":"cknwqx6rz000nbczwbnrn61iq"},{"post_id":"cknwqx6ro0003bczw232x0w2f","tag_id":"cknwqx6ry000kbczwg1gr4xjb","_id":"cknwqx6rz000obczw1wp9g28t"},{"post_id":"cknwqx6ro0003bczw232x0w2f","tag_id":"cknwqx6ry000lbczw75dp1pms","_id":"cknwqx6rz000pbczwh93pddzd"},{"post_id":"cknwqx6s1000rbczwbqqs856l","tag_id":"cknwqx6rx000ebczwbwhd9gu5","_id":"cknwqx6s2000ubczw0zbbetcq"},{"post_id":"cknwqx6s1000rbczwbqqs856l","tag_id":"cknwqx6rx000hbczwb4a351wn","_id":"cknwqx6s3000vbczw4a6lgwff"},{"post_id":"cknwqx6s1000rbczwbqqs856l","tag_id":"cknwqx6ry000kbczwg1gr4xjb","_id":"cknwqx6s3000ybczw2d5v3lxy"},{"post_id":"cknwqx6s0000qbczw7th6704d","tag_id":"cknwqx6s2000tbczwdllrci1k","_id":"cknwqx6s30010bczw085zbjo7"},{"post_id":"cknwqx6s0000qbczw7th6704d","tag_id":"cknwqx6rx000hbczwb4a351wn","_id":"cknwqx6s30011bczw7297en1r"},{"post_id":"cknwqx6s0000qbczw7th6704d","tag_id":"cknwqx6s3000xbczwe7n64o25","_id":"cknwqx6s30012bczw7wdt18jg"},{"post_id":"cknwqx6s60013bczwf7jyd5xa","tag_id":"cknwqx6s80014bczw6fh669ts","_id":"cknwqx6sc0018bczw7oh1hg6p"},{"post_id":"cknwqx6s60013bczwf7jyd5xa","tag_id":"cknwqx6sa0015bczw6wq23p7t","_id":"cknwqx6sc0019bczw2qn82syb"},{"post_id":"cknwqx6s60013bczwf7jyd5xa","tag_id":"cknwqx6sb0016bczwe37d1rzy","_id":"cknwqx6sc001abczw6fqi6viz"},{"post_id":"cknwqx6s60013bczwf7jyd5xa","tag_id":"cknwqx6sc0017bczw28fcai40","_id":"cknwqx6sc001bbczw5hz5enxg"}],"Tag":[{"name":"Online Course","_id":"cknwqx6rr0005bczw6kc254ml"},{"name":"Algorithm","_id":"cknwqx6ru0009bczw0uu0b9uu"},{"name":"Data structure","_id":"cknwqx6rw000bbczw2ga32cq9"},{"name":"Notes","_id":"cknwqx6rx000dbczwamqv2el8"},{"name":"Machine Learning","_id":"cknwqx6rx000ebczwbwhd9gu5"},{"name":"Deep Learning","_id":"cknwqx6rx000hbczwb4a351wn"},{"name":"Fluid Mechanics","_id":"cknwqx6ry000kbczwg1gr4xjb"},{"name":"Partial Differential Equations","_id":"cknwqx6ry000lbczw75dp1pms"},{"name":"Class Project","_id":"cknwqx6s2000tbczwdllrci1k"},{"name":"Computer Vision","_id":"cknwqx6s3000xbczwe7n64o25"},{"name":"Survey","_id":"cknwqx6s80014bczw6fh669ts"},{"name":"Vision transformer","_id":"cknwqx6sa0015bczw6wq23p7t"},{"name":"Computer vision","_id":"cknwqx6sb0016bczwe37d1rzy"},{"name":"Course project","_id":"cknwqx6sc0017bczw28fcai40"}]}}