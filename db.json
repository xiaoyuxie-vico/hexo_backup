{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/raytaylorism/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/weixin_favicon.png","path":"weixin_favicon.png","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/jquery.min.js","path":"js/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/materialize.min.js","path":"js/materialize.min.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/prettify.js","path":"js/prettify.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","path":"css/images/side-user-cover.jpg","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","path":"css/lib/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","path":"css/lib/materialize.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","path":"css/lib/prettify-tomorrow-night-eighties.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","path":"css/font/font-awesome/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","path":"css/font/font-awesome/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","path":"css/font/font-awesome/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","path":"css/font/font-awesome/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","path":"css/font/font-awesome/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","path":"css/font/font-awesome/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","path":"css/font/roboto/Roboto-Bold.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","path":"css/font/roboto/Roboto-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","path":"css/font/roboto/Roboto-Bold.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","path":"css/font/roboto/Roboto-Bold.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","path":"css/font/roboto/Roboto-Light.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","path":"css/font/roboto/Roboto-Light.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","path":"css/font/roboto/Roboto-Light.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","path":"css/font/roboto/Roboto-Light.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","path":"css/font/roboto/Roboto-Medium.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","path":"css/font/roboto/Roboto-Medium.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","path":"css/font/roboto/Roboto-Medium.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","path":"css/font/roboto/Roboto-Medium.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","path":"css/font/roboto/Roboto-Regular.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","path":"css/font/roboto/Roboto-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","path":"css/font/roboto/Roboto-Regular.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","path":"css/font/roboto/Roboto-Regular.woff2","modified":1,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0}],"Cache":[{"_id":"source/_data/about.json","hash":"7408d88c370ee8eff0b6f3d857bd5a72327a1dcf","modified":1619489794615},{"_id":"source/_data/hint.json","hash":"de3a18d3d62276ffa8ad7ddda6ddd1d964425ff7","modified":1609743005904},{"_id":"source/.DS_Store","hash":"af2527dca4b23012597456ea4764a10643bb3d57","modified":1619413171852},{"_id":"source/CNAME","hash":"5dff077b04ab5f4ebffd9b9d2b1d0811c1a796a8","modified":1609744231340},{"_id":"source/_data/link.json","hash":"624a0f86ef9057f9f767fda6bd936c02c735d3fd","modified":1609981748554},{"_id":"source/_data/slider.json","hash":"98fe742cf6b11d824e5a0d82dd6d542a6eaa20c3","modified":1609746178917},{"_id":"source/_posts/.DS_Store","hash":"8395c2e8fb98e897e187e37e55c6f482ab542f24","modified":1619411489711},{"_id":"source/_data/reading.json","hash":"e9622b6ddaa22a86dd5f8ebd614d156a28e1f7ad","modified":1617083565991},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes.md","hash":"9c327ebe40ff18b098ce5d9be653132935ca614a","modified":1619415305065},{"_id":"source/_posts/CS61B-Week2.md","hash":"81d4f3a4ff1f89ad8aba973142da029d52a2533f","modified":1619363700337},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function.md","hash":"220ca891a3c38e38276962fd30eee1ae48f41787","modified":1619330052303},{"_id":"source/_posts/Survey-vision-transformer.md","hash":"6d32730310f6bb46e5e19aa5d26744f207c31798","modified":1619330090539},{"_id":"source/_posts/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers.md","hash":"194c243efcd051a9aef782bbe8c4a2e90ce52b19","modified":1613567620346},{"_id":"source/_posts/notes-ML-FM.md","hash":"7d00d0721b820ea3e6078e86d55e94e8df60fc46","modified":1613567606433},{"_id":"source/about/index.md","hash":"3a4933ee66a724bdc9c6779e244a085e4004bdd4","modified":1619319312153},{"_id":"source/reading/index.md","hash":"a866087ffe1d857e9b2a2c30b2c1c13b1dc4f0f3","modified":1609846070007},{"_id":"source/categories/index.md","hash":"21c01aad3dc05ec6bad883f2fbf4e0d654c26465","modified":1609993317207},{"_id":"source/tags/index.md","hash":"d2445b6751df9d39bd57d243229878830bc1e5f6","modified":1609992490112},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1619413171593},{"_id":"source/_posts/CS61B-Week2/sllist_last_pointer.png","hash":"835f7a96d140d731b35adf801de32a14433b0c8b","modified":1619362673324},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_5.png","hash":"a139ce163027001bd90e10441ab74dbf2faff432","modified":1619382186000},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_959.png","hash":"ed8f5f88c711a4b38c4f6823f756687d5f158970","modified":1619382186000},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_958.png","hash":"0d2dd6b6ea9925d7c5874127ee63b039d0159b05","modified":1619382186000},{"_id":"source/_posts/Survey-vision-transformer/clip_image001.png","hash":"0e13cb32e27691f2b64c82e5a01b6a566d555ee7","modified":1619329308787},{"_id":"source/_posts/Survey-vision-transformer/clip_image002.png","hash":"9abb1a82fb63b3da66b36125c91af33c941bcb63","modified":1619329308787},{"_id":"source/_posts/Survey-vision-transformer/clip_image007.png","hash":"5175998d2d23dc50914ae7360cc30e0854408566","modified":1619329308792},{"_id":"source/_posts/Survey-vision-transformer/clip_image005.png","hash":"dea126817372ad308a68e054dda964ee1f08572d","modified":1619329308791},{"_id":"source/_posts/Survey-vision-transformer/clip_image009.png","hash":"71961dc1647451676df0184ac001693b112b1e26","modified":1619329308793},{"_id":"source/_posts/Survey-vision-transformer/clip_image003.png","hash":"64250098098634ca8b4876edb7d8d94e72f3c07d","modified":1619329308788},{"_id":"source/_posts/Survey-vision-transformer/clip_image011.png","hash":"dc456312ae88d30a55d75d1d1a273ce0aea7c038","modified":1619329308794},{"_id":"source/_posts/Survey-vision-transformer/clip_image013.png","hash":"ba95703381ae371f34732381eb418a2088d65620","modified":1619329308795},{"_id":"source/_posts/Survey-vision-transformer/clip_image016.png","hash":"d8324302e6072cb5ebf229474d0e7d24e52ee76b","modified":1619329308796},{"_id":"source/_posts/Survey-vision-transformer/clip_image019.png","hash":"bf8d4b484a63e658343cac998e0095cecbac262c","modified":1619329308797},{"_id":"source/_posts/Survey-vision-transformer/clip_image023.png","hash":"28bc904909e227f40c1e3d1113f8dc8efbad3d18","modified":1619329308798},{"_id":"source/_posts/Survey-vision-transformer/clip_image025.png","hash":"7e72a1207eeffc4d9d39781f486fba701b1a8bc2","modified":1619329308798},{"_id":"source/_posts/Survey-vision-transformer/clip_image033.png","hash":"e27674f98bad707a921905b503c9fbf6185a4c48","modified":1619329308801},{"_id":"source/_posts/Survey-vision-transformer/clip_image031.png","hash":"5f0dafa83089b1976b07733b2fd8c092d834608e","modified":1619329308801},{"_id":"source/_posts/Survey-vision-transformer/clip_image029.png","hash":"690e6b8f9f178f51a31bdf8dcf33bda8ffd46b16","modified":1619329308800},{"_id":"source/_posts/Survey-vision-transformer/clip_image027.png","hash":"ecec8a2ca638cc1becd1f4d5aba156e7c74ed27c","modified":1619329308799},{"_id":"source/_posts/Survey-vision-transformer/clip_image034.png","hash":"6960d9d25a36b3e2c19ad6bf2cc1fcbe89ebdd47","modified":1619329308802},{"_id":"source/_posts/Survey-vision-transformer/clip_image038.png","hash":"1bef009b19dcf6cd30c48d6a7e20ce7c0eeb09eb","modified":1619329308805},{"_id":"source/_posts/Survey-vision-transformer/clip_image040.png","hash":"cd478ee47a55a10429c82a682691ad01f93fbbd9","modified":1619329308806},{"_id":"source/_posts/Survey-vision-transformer/clip_image035.png","hash":"eeaad5325e0b2a5b9137e2d726224beb7045cf8d","modified":1619329308803},{"_id":"source/_posts/Survey-vision-transformer/clip_image004.png","hash":"a3f243df8db455c891a5434c1d3e0fe3acd5e9bd","modified":1619329308789},{"_id":"source/_posts/Survey-vision-transformer/clip_image042.png","hash":"ebbbf27f816951c5b9337d76f02c6a07f0951bc6","modified":1619329308807},{"_id":"source/_posts/Survey-vision-transformer/clip_image048.png","hash":"38166a000ee6a5794cec4b73281682d511623b9e","modified":1619329308809},{"_id":"source/_posts/Survey-vision-transformer/clip_image044.png","hash":"a3c299e01669750baef8171cf7c9fb50bf960ffc","modified":1619329308807},{"_id":"source/_posts/Survey-vision-transformer/clip_image046.png","hash":"78b01fea1ab08599d564f6a849fa390417c68f12","modified":1619329308808},{"_id":"source/_posts/Survey-vision-transformer/clip_image051.png","hash":"0fd96824cf4901e9a1f7e48148a2d50f77d26097","modified":1619329308810},{"_id":"source/_posts/Survey-vision-transformer/clip_image055.png","hash":"2b44069f3eea7fbe6d3ca7da6e3769e97dac29f4","modified":1619329308813},{"_id":"source/_posts/CS61B-Week2/image-20210425113258793.png","hash":"6e04996ee4c28c81087ae0e8cd03091497d5245d","modified":1619324646031},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/image-20210426120534698.png","hash":"40a18a1a9bdb1587db32a74cfae3a350620936a4","modified":1619409934699},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_4.png","hash":"28d929a8b3da3c1ba791dd75265be4a8cda906d9","modified":1619382186000},{"_id":"source/_posts/Survey-vision-transformer/clip_image036.png","hash":"8e4fda135bebedfe89d7e345ba2199b3b7d5a65f","modified":1619329308804},{"_id":"source/_posts/Survey-vision-transformer/clip_image050.png","hash":"791f67513d849895843434534bdeefc382df896a","modified":1619329308810},{"_id":"source/_posts/Survey-vision-transformer/clip_image053.png","hash":"4160ce8f9d2eabf070cb2473fa277deda06126dd","modified":1619329308811},{"_id":"source/_posts/notes-ML-FM/image-20210214174913278.png","hash":"12553f8cbd22b15068b4cf0bc2173c1f83c16c93","modified":1613296153278},{"_id":"source/_posts/Survey-vision-transformer/clip_image056.png","hash":"a78943be90d65cbfb63565aa43f1a10cb363353c","modified":1619329308814},{"_id":"source/_posts/Survey-vision-transformer/clip_image057.png","hash":"a10f2c36b0e93e183047abcc1dce5d72222f6d79","modified":1619329308815},{"_id":"source/_posts/Survey-vision-transformer/clip_image054.png","hash":"d618ab0543d394ff056d59436aa16b0f7893baf8","modified":1619329308812},{"_id":"source/_posts/notes-ML-FM/image-20210214174809740.png","hash":"55f75f26acb1919ea76e2a618dd71d6f0d2804fc","modified":1613296089740},{"_id":"source/_posts/notes-ML-FM/image-20210214182520461.png","hash":"6fa531b36a88eef90f1aae1f264009a4f022ac8c","modified":1613298320461},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/1.png","hash":"2db678db0d744b6abf0148fc4f7690bcd61635c3","modified":1609844564413},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/2.png","hash":"e27010e5c846311585587427d4dc5716696c0d57","modified":1609844597221},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/4.png","hash":"be1010578082af53f248773a883ac875af904676","modified":1609844682653},{"_id":"source/_posts/notes-ML-FM/image-20210214173356481.png","hash":"24868d2884527b8b00b7288fe2475d8f9a39bca2","modified":1613295236482},{"_id":"source/_posts/notes-ML-FM/image-20210214174422276.png","hash":"588366cb2e4d3ae87401f5972cccd294b4a55bb2","modified":1613295862277},{"_id":"source/_posts/notes-ML-FM/image-20210214182334199.png","hash":"774c69ec91d50792fcf91cf68b5b80414fa6d5fc","modified":1613298214200},{"_id":"source/_posts/notes-ML-FM/image-20210214175022879.png","hash":"08cf0616efc761a32e5e38f9c01856eb1e14c714","modified":1613296222879},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/5.png","hash":"918add3b718d520c1cc6b3c340dfaf58625e44b7","modified":1609844687196},{"_id":"source/_posts/notes-ML-FM/image-20210214175304898.png","hash":"54b725874e2e3cdb8a0f0819902048c955b05083","modified":1613296384898},{"_id":"themes/raytaylorism/Gruntfile.js","hash":"f0c204fc7b3343417cc1a0fdca12ab089394b654","modified":1609740676882},{"_id":"themes/raytaylorism/.gitignore","hash":"3ba1b7b4baceca12575cc0f212e13a504af994a7","modified":1609740676881},{"_id":"themes/raytaylorism/LICENSE","hash":"391dd84c2091363936603d63cc71ba1628a13c86","modified":1609740676882},{"_id":"themes/raytaylorism/README.md","hash":"23151e838b2f0c029c5cc928dad86105b8c06ba4","modified":1609740676882},{"_id":"themes/raytaylorism/_config.yml","hash":"b418dae0235a8369c6f652b60150465405c8f36e","modified":1619419057450},{"_id":"themes/raytaylorism/log.md","hash":"e6b93891f941c723524b143b69b0d303064db54b","modified":1609740676889},{"_id":"themes/raytaylorism/_data/about.json","hash":"068d6827b50f1c0ae7bb7bb1a9032508143a5bc2","modified":1609740676882},{"_id":"themes/raytaylorism/_data/hint.json","hash":"178fe345ea95511364ed70da86186cb834947993","modified":1609740676882},{"_id":"themes/raytaylorism/_data/link.json","hash":"b865125d0440e6717ee3d88d1b518a4ebdb32d61","modified":1609740676882},{"_id":"themes/raytaylorism/_data/reading.json","hash":"e8045e22b2d499a5d813320a8c2b1bccdbedd46d","modified":1609740676883},{"_id":"themes/raytaylorism/_data/slider.json","hash":"361373b57cfb5371027af42000bbaec4e03333a3","modified":1609740676883},{"_id":"themes/raytaylorism/languages/default.yml","hash":"936a72db42d299cd5e912198b9ace4fd64c476fd","modified":1609847014718},{"_id":"themes/raytaylorism/languages/zh-CN.yml","hash":"c5fcb3d2b353d9747238ca78106953301f9a2018","modified":1609740676884},{"_id":"themes/raytaylorism/languages/zh-TW.yml","hash":"ae281c898cea81f4c897c0a69c45e2ce6a4314a6","modified":1609740676884},{"_id":"themes/raytaylorism/layout/about.ejs","hash":"54e74d61dba41f173f111e32deeb58447260f0e3","modified":1609740676889},{"_id":"themes/raytaylorism/layout/archive.ejs","hash":"0a21af8903e95c6d8bb7554b089ac219e8708ad7","modified":1609740676889},{"_id":"themes/raytaylorism/layout/index.ejs","hash":"34cbcb6c75e2eef622fea6fecebfe15fb7522a95","modified":1609740676889},{"_id":"themes/raytaylorism/layout/layout.ejs","hash":"0fbced6bf0129f550ad66d57735d269b70728b49","modified":1609740676889},{"_id":"themes/raytaylorism/layout/page.ejs","hash":"90441f114859ce63ef7c7d93d668dbe5939995c2","modified":1609740676889},{"_id":"themes/raytaylorism/layout/post.ejs","hash":"8e550fd95ef761909294ed3a4aa428ff0509fbf0","modified":1609740676889},{"_id":"themes/raytaylorism/layout/reading.ejs","hash":"52906ee0e7e679d417d5bc385e054e16e9ff0256","modified":1609740676889},{"_id":"themes/raytaylorism/layout/tag.ejs","hash":"42ecab14917abd40c0a38e6ab629f089352a24b1","modified":1609740676889},{"_id":"themes/raytaylorism/source/favicon.png","hash":"d44008b0d6298287cdcfe744a2c8c562569f67ff","modified":1609740676906},{"_id":"themes/raytaylorism/source/weixin_favicon.png","hash":"4a8466bd7d8cf4753cab8fb68647b40b91a246ad","modified":1609740676909},{"_id":"themes/raytaylorism/_md/about/index.md","hash":"ee081f0766b1bbdd72b9a254a934033878dd1571","modified":1609740676883},{"_id":"themes/raytaylorism/_md/reading/index.md","hash":"ffe64363f79a74ca022f15447a03a96808c64794","modified":1609740676883},{"_id":"themes/raytaylorism/layout/_partial/after_footer.ejs","hash":"77476565bc85987d7656751cbc27b473223b0186","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/archive.ejs","hash":"6fc4dc05d153dbf1dd955df4ff19c380692f87e9","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/archive_title.ejs","hash":"37c38ef6972ddd92668ea08983f4b34230b39d52","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/article.ejs","hash":"068cd4f944f8f0810d06bc79d11042da406c7067","modified":1609740676884},{"_id":"themes/raytaylorism/layout/_partial/construction.ejs","hash":"21190b5a0d567ed4ea5d5289459690b72c1452f0","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/feature_guide.ejs","hash":"752d5c0a4a6f2f2228ae99bb6bede195080a15d8","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/float.ejs","hash":"42ad838e39c007226eb4151292a459173e30d8ea","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/footer.ejs","hash":"6634e01d620e2f341c5e3dcda180caf83f042252","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/head.ejs","hash":"406c0bdb3ef224bb5ec375123426a0babae2724a","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/header.ejs","hash":"426eeb374b409a6ab6eb8e21a7213b6a6147d6f9","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/menu_drawer.ejs","hash":"28a46dd851b971216c788ace1ca5609d961c2446","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/pagenav.ejs","hash":"e7ada8faaee878ea4dde267d1b420bb45421670d","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/pagination.ejs","hash":"247c1507971b3e41ed539dd7f8d15af8e21c0d58","modified":1609740676885},{"_id":"themes/raytaylorism/layout/_partial/search.ejs","hash":"1285a8ecb670f6460b31c0fbca9af13b202f5838","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_partial/side_nav.ejs","hash":"b12e72453fb981924d17fa48904af6951f07450f","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_partial/simple_article.ejs","hash":"fdcbb516a3745d0a70c94e565d53510d9f47693c","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_partial/slider.ejs","hash":"bb7b53f6ca9c852808d955fb074f88112e51ea59","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/blogroll.ejs","hash":"cf42209342e51e1729dcc9b01b1e5497f152844f","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/category.ejs","hash":"fb2d42083c97dfaba2717fb2e63e744259ec4530","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/recent_posts.ejs","hash":"0025878eb4cbf17ddc909f82497e9c73e4091c20","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/tag.ejs","hash":"31b93c078a03af98de504eeb0153f9c0dbc74ed9","modified":1609740676888},{"_id":"themes/raytaylorism/layout/_widget/tagcloud.ejs","hash":"1da338304f94f042606b73e235e9c725628c34ad","modified":1609740676888},{"_id":"themes/raytaylorism/source/js/prettify.js","hash":"d24b1da342b5c2d0582f0922118aaf0b2a6840d5","modified":1609740676908},{"_id":"themes/raytaylorism/source/css/style.styl","hash":"2c7ef7179e29084efe77c653d537b56889734a22","modified":1609740676906},{"_id":"themes/raytaylorism/layout/_partial/post/category.ejs","hash":"f48f487dc952f5703d9bc7737fc6eb2e6b439608","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/gallery.ejs","hash":"0adb673358b42b9ac52b2c1f72b92368cdcf5f2e","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/prevnext.ejs","hash":"dbb790cf454fda546c82a411a3b50ebb0129a1e8","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/readtimes.ejs","hash":"c829d0598f9906f663a8ace1c86f2aa6024d642c","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/tablecontents.ejs","hash":"a851061909d4e27321d1792a262f55385529fb2d","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/tag.ejs","hash":"36cbf8f85f93be441d47d1a4a0584afd85480d4f","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/time.ejs","hash":"e11498240ece670129a2fbfb2fed16ff315344d4","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/post/title.ejs","hash":"c6c44ea53bbfd3838c7bf7cc236c6db1a4b9460e","modified":1609740676887},{"_id":"themes/raytaylorism/layout/_partial/plugin/analytics.ejs","hash":"b88303620558f833c6d7505af762d12e21f90f90","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/comment.ejs","hash":"7bdcfd6b3a5b7dee57e9b96ca90a127b7562fc3f","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/google_code_prettify.ejs","hash":"3aecf1e3e706417131918e3f489557e2d5f438af","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/main_javascript.ejs","hash":"cc246117386c6cbde13e3b4316ba5e85af659df6","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/mathjax.ejs","hash":"7d8c369b14e75d2f120c033d319a4eb749392f38","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/noscript.ejs","hash":"182650c8be93b093997ac4d5fe14af2f835b98d9","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/page_stat.ejs","hash":"0dcd6b1185fa311cd5172b25067436f14e6d7429","modified":1609740676886},{"_id":"themes/raytaylorism/layout/_partial/plugin/reward.ejs","hash":"fde8d42347f72f3b3594c36b1f3c94c6d90a31b6","modified":1609740676887},{"_id":"themes/raytaylorism/source/css/_base/icons.css","hash":"ab167f1694ffe10c3c51d18a633efd41be121555","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_base/layout.styl","hash":"14a747f9fce53f586d11ed67a597a8e71a802d17","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_base/lib_customize.styl","hash":"5f25b295a3ad99991952f864573c0f1ccc6a1591","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_base/variable.styl","hash":"6812c6940c7c59b9fab5b41e6b832e89416d11c5","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_partial/about.styl","hash":"b0f80e89e7518d49190b79985c10c8a7b24bfa19","modified":1609740676890},{"_id":"themes/raytaylorism/source/css/_partial/archive.styl","hash":"d912cf297c10e78bd90f3210d596ec87a16f74ad","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/comment.styl","hash":"bfa4d7caedffffab18b29f70db9cbf2a15a5f24b","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/article.styl","hash":"94bdf4d6da4ec154124ac66008c8dff66882c7e4","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/footer.styl","hash":"92e1c226202782e5d429fbe72b98ae4e07fc0233","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/index.styl","hash":"27918d50e5a69899f184280d9e0048576ac3c85d","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/header.styl","hash":"534686e8e9de54e8dd99eb1b064f5ad3a0199a4e","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/link_context.styl","hash":"cd403f732319354781c552d26d37bad7c4450ad5","modified":1609740676891},{"_id":"themes/raytaylorism/source/css/_partial/other.styl","hash":"710eea97e5c98a1426d1a3c0fc8f648279c7a82d","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/reading.styl","hash":"7abecdfc7fd21f7d11f1568d430418296b34945a","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/search.styl","hash":"caef055a95598415656c417e662264397363704b","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/side_nav.styl","hash":"b239b6b55e87e86d038d6aa821beeb66a9cbaf39","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/slider.styl","hash":"8933a67e92680dbdfd6af7c2ecaa8d86346df907","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/syntax.styl","hash":"20df0f8a54729980094514fc726b51591ada1ad7","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/_partial/tablecontents.styl","hash":"e04fa0e7664065077750a7223ae3390cc84a4c56","modified":1609740676892},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1609740676905},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","hash":"683d12731b7429d32ec7de00a6706602e403013f","modified":1609740676905},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","hash":"35e07bd7a4585363060edd558a0e9939e7e68323","modified":1609740676906},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1609740676898},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1609740676899},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1609740676900},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1609740676900},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1609740676903},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1609740676903},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1609740676903},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1609740676904},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1609740676905},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/3.png","hash":"9a52e35f1b8c80691a5b0be4b1ab6a4f2d56a7f6","modified":1609844605115},{"_id":"themes/raytaylorism/source/js/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1609740676907},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","hash":"41d1676b73eec020eaeb9b507dcbcd30069ae8cb","modified":1609740676906},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1609740676893},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1609740676894},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1609740676898},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1609740676898},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1609740676899},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1609740676901},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1609740676902},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1609740676904},{"_id":"source/_posts/notes-ML-FM/image-20210214174108194.png","hash":"cd7d300b322101079d7f26615f1ca358931e9098","modified":1613295668194},{"_id":"source/_posts/notes-ML-FM/image-20210214173840928.png","hash":"53edcce7af1fbaf7d59acf2062316d81e63f723e","modified":1613295520928},{"_id":"themes/raytaylorism/source/js/materialize.min.js","hash":"c9308fbe808a149aa11061af40a4be5f391cccee","modified":1609740676908},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1609740676897},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","hash":"b06b5c8f67fd632cdc62a33b62ae4f74194131b3","modified":1609740676896},{"_id":"source/_posts/notes-ML-FM/image-20210214174957751.png","hash":"f07858cb57086c6becfbf436e569f90302fbb847","modified":1613296197752},{"_id":"source/_posts/notes-ML-FM/image-20210214174003005.png","hash":"c8c21d653e549d5835ad6ee49237cb6a7b0f60d4","modified":1613295603006},{"_id":"source/_posts/notes-ML-FM/image-20210214174310749.png","hash":"040595ce2e24cd22fab14a223baae6d4a217107f","modified":1613295790750},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/image-20210426120448703.png","hash":"d2311bfd7b00103933ba6e1aa52a3f838216a5ac","modified":1619409888706},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/7.png","hash":"cc84be16b5350a69e15af7aeb998acfd9c1fcd5d","modified":1609844697567},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/6.png","hash":"1e4661db20aebb2c3bc315f658beb41e9fe62893","modified":1609844692677},{"_id":"public/content.json","hash":"00e27dc24e5af7f0191cfae6649c8aa2a9f4e049","modified":1619489803871},{"_id":"public/about/index.html","hash":"cd8f415e1a525c1b513810a5ccd4e4a4560d0ac3","modified":1619489803871},{"_id":"public/tags/index.html","hash":"fe87171c937a75dcb5b5840638298538da80478e","modified":1619489803871},{"_id":"public/categories/index.html","hash":"e4a8f3ad33ddcfcfb7eb6a5c1fc3e693274dc585","modified":1619489803871},{"_id":"public/reading/index.html","hash":"82513746141bbd23fef749b519b141291334ca2f","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/index.html","hash":"3ea982dfde3454670026cf81db9cbbf716be2ea0","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/index.html","hash":"4749bb3d6f094c758ed98099607dacb270bdf0fb","modified":1619489803871},{"_id":"public/2021/04/24/CS61B-Week2/index.html","hash":"7e2d8a35c9e577c93c34fe47e72219292c9ece58","modified":1619489803871},{"_id":"public/2021/02/17/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers/index.html","hash":"a032eb79672401914a7303b31a3d915fa7397747","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/index.html","hash":"99ba80044efb6cf45f207ce4b2ee480a2e137604","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/index.html","hash":"0e03eda0d6791e695358b10055420c7e2b2efb80","modified":1619489803871},{"_id":"public/archives/index.html","hash":"d381dbfa5dc7aa2957eafaa476b8241673b100d8","modified":1619489803871},{"_id":"public/archives/2021/index.html","hash":"dc0c1a22a90ac44312c91b06e6199843727a51c8","modified":1619489803871},{"_id":"public/archives/2021/01/index.html","hash":"8523922968b4856eb747ec26a8954be3607debe0","modified":1619489803871},{"_id":"public/archives/2021/02/index.html","hash":"785a4f39f56343bd731195c4e96569b5d0b569a5","modified":1619489803871},{"_id":"public/archives/2021/04/index.html","hash":"6fb76677371587e1955f17ec3bf003eb6f134bbd","modified":1619489803871},{"_id":"public/categories/Class-notes/index.html","hash":"b0bd0de565a01b982d379ae8d52c440d8842cd83","modified":1619489803871},{"_id":"public/categories/Notes/index.html","hash":"a3018daa2188c9754a56bfb400661fc83581795b","modified":1619489803871},{"_id":"public/categories/Course-project/index.html","hash":"cb52819914cb9eb242a827104eae45ac4a07aeba","modified":1619489803871},{"_id":"public/index.html","hash":"29f7d8553b27da4c2642e2597334b768524eeb0d","modified":1619489803871},{"_id":"public/tags/Online-Course/index.html","hash":"bd43c4cc53a35fde13b3f79392917e8477b1ae59","modified":1619489803871},{"_id":"public/tags/Algorithm/index.html","hash":"18146dde92cc543e98ca7f2191bd23a4fd1ed6de","modified":1619489803871},{"_id":"public/tags/Data-structure/index.html","hash":"0214293071231ef1321aa1cb6d6465c9c283bfb6","modified":1619489803871},{"_id":"public/tags/Notes/index.html","hash":"b2aaa10e47c49e7d57acdb0757d202804ce25e9d","modified":1619489803871},{"_id":"public/tags/Machine-Learning/index.html","hash":"dedac29fef1bf7132422155d2ac1f882de2d03d4","modified":1619489803871},{"_id":"public/tags/Deep-Learning/index.html","hash":"36c3e2c616ec2ebbbe4f5deeeb9225a1b636fffd","modified":1619489803871},{"_id":"public/tags/Fluid-Mechanics/index.html","hash":"aa9af193a9a73a575414633d4e0708115dcbe246","modified":1619489803871},{"_id":"public/tags/Partial-Differential-Equations/index.html","hash":"00f21ccecfc041e55e0cb01a37a48c3b02205b55","modified":1619489803871},{"_id":"public/tags/Class-Project/index.html","hash":"9b7d2e97c74591c6f933660582c25d1f94f3e583","modified":1619489803871},{"_id":"public/tags/Computer-Vision/index.html","hash":"a794ce1ca3f2c12a5377a6e0e92e0e9b2f509a2a","modified":1619489803871},{"_id":"public/tags/Online-course/index.html","hash":"8053c86d266bd22a67e9c0ba101692da3445183c","modified":1619489803871},{"_id":"public/tags/GANs/index.html","hash":"a21089c94c77785475a3e90be19ad4d1f12a2ace","modified":1619489803871},{"_id":"public/tags/Survey/index.html","hash":"565e95c30f88908f7d2f8d50cd25196139ea20c4","modified":1619489803871},{"_id":"public/tags/Vision-transformer/index.html","hash":"fd97824781277b08cf3e5a093fabdd682cad4387","modified":1619489803871},{"_id":"public/tags/Computer-vision/index.html","hash":"3742c2ec8f3cda00805c5f6c67c6da235b3216e0","modified":1619489803871},{"_id":"public/tags/Course-project/index.html","hash":"b7f5a2bd4ca53c7dd3b2b2bcf9885365c3e0dcb0","modified":1619489803871},{"_id":"public/favicon.png","hash":"d44008b0d6298287cdcfe744a2c8c562569f67ff","modified":1619489803871},{"_id":"public/weixin_favicon.png","hash":"4a8466bd7d8cf4753cab8fb68647b40b91a246ad","modified":1619489803871},{"_id":"public/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1619489803871},{"_id":"public/CNAME","hash":"5dff077b04ab5f4ebffd9b9d2b1d0811c1a796a8","modified":1619489803871},{"_id":"public/2021/04/24/CS61B-Week2/sllist_last_pointer.png","hash":"835f7a96d140d731b35adf801de32a14433b0c8b","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/output_31_5.png","hash":"a139ce163027001bd90e10441ab74dbf2faff432","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/output_31_958.png","hash":"0d2dd6b6ea9925d7c5874127ee63b039d0159b05","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/output_31_959.png","hash":"ed8f5f88c711a4b38c4f6823f756687d5f158970","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image001.png","hash":"0e13cb32e27691f2b64c82e5a01b6a566d555ee7","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image002.png","hash":"9abb1a82fb63b3da66b36125c91af33c941bcb63","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image003.png","hash":"64250098098634ca8b4876edb7d8d94e72f3c07d","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image004.png","hash":"a3f243df8db455c891a5434c1d3e0fe3acd5e9bd","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image005.png","hash":"dea126817372ad308a68e054dda964ee1f08572d","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image007.png","hash":"5175998d2d23dc50914ae7360cc30e0854408566","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image009.png","hash":"71961dc1647451676df0184ac001693b112b1e26","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image011.png","hash":"dc456312ae88d30a55d75d1d1a273ce0aea7c038","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image013.png","hash":"ba95703381ae371f34732381eb418a2088d65620","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image016.png","hash":"d8324302e6072cb5ebf229474d0e7d24e52ee76b","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image019.png","hash":"bf8d4b484a63e658343cac998e0095cecbac262c","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image023.png","hash":"28bc904909e227f40c1e3d1113f8dc8efbad3d18","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image025.png","hash":"7e72a1207eeffc4d9d39781f486fba701b1a8bc2","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image027.png","hash":"ecec8a2ca638cc1becd1f4d5aba156e7c74ed27c","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image029.png","hash":"690e6b8f9f178f51a31bdf8dcf33bda8ffd46b16","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image031.png","hash":"5f0dafa83089b1976b07733b2fd8c092d834608e","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image033.png","hash":"e27674f98bad707a921905b503c9fbf6185a4c48","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image034.png","hash":"6960d9d25a36b3e2c19ad6bf2cc1fcbe89ebdd47","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image035.png","hash":"eeaad5325e0b2a5b9137e2d726224beb7045cf8d","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image038.png","hash":"1bef009b19dcf6cd30c48d6a7e20ce7c0eeb09eb","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image040.png","hash":"cd478ee47a55a10429c82a682691ad01f93fbbd9","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image042.png","hash":"ebbbf27f816951c5b9337d76f02c6a07f0951bc6","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image044.png","hash":"a3c299e01669750baef8171cf7c9fb50bf960ffc","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image046.png","hash":"78b01fea1ab08599d564f6a849fa390417c68f12","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image048.png","hash":"38166a000ee6a5794cec4b73281682d511623b9e","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image051.png","hash":"0fd96824cf4901e9a1f7e48148a2d50f77d26097","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image055.png","hash":"2b44069f3eea7fbe6d3ca7da6e3769e97dac29f4","modified":1619489803871},{"_id":"public/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1619489803871},{"_id":"public/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1619489803871},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1619489803871},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1619489803871},{"_id":"public/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1619489803871},{"_id":"public/2021/04/24/CS61B-Week2/image-20210425113258793.png","hash":"6e04996ee4c28c81087ae0e8cd03091497d5245d","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174913278.png","hash":"12553f8cbd22b15068b4cf0bc2173c1f83c16c93","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/image-20210426120534698.png","hash":"40a18a1a9bdb1587db32a74cfae3a350620936a4","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/output_31_4.png","hash":"28d929a8b3da3c1ba791dd75265be4a8cda906d9","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image036.png","hash":"8e4fda135bebedfe89d7e345ba2199b3b7d5a65f","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image050.png","hash":"791f67513d849895843434534bdeefc382df896a","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image053.png","hash":"4160ce8f9d2eabf070cb2473fa277deda06126dd","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image056.png","hash":"a78943be90d65cbfb63565aa43f1a10cb363353c","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image057.png","hash":"a10f2c36b0e93e183047abcc1dce5d72222f6d79","modified":1619489803871},{"_id":"public/js/prettify.js","hash":"d24b1da342b5c2d0582f0922118aaf0b2a6840d5","modified":1619489803871},{"_id":"public/css/style.css","hash":"55a917def994d7b8bd35785f6b3d147435bc0b88","modified":1619489803871},{"_id":"public/css/lib/prettify-tomorrow-night-eighties.css","hash":"35e07bd7a4585363060edd558a0e9939e7e68323","modified":1619489803871},{"_id":"public/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214182520461.png","hash":"6fa531b36a88eef90f1aae1f264009a4f022ac8c","modified":1619489803871},{"_id":"public/2021/04/25/Survey-vision-transformer/clip_image054.png","hash":"d618ab0543d394ff056d59436aa16b0f7893baf8","modified":1619489803871},{"_id":"public/css/lib/font-awesome.min.css","hash":"683d12731b7429d32ec7de00a6706602e403013f","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214173356481.png","hash":"24868d2884527b8b00b7288fe2475d8f9a39bca2","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174809740.png","hash":"55f75f26acb1919ea76e2a618dd71d6f0d2804fc","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/1.png","hash":"2db678db0d744b6abf0148fc4f7690bcd61635c3","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/2.png","hash":"e27010e5c846311585587427d4dc5716696c0d57","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214182334199.png","hash":"774c69ec91d50792fcf91cf68b5b80414fa6d5fc","modified":1619489803871},{"_id":"public/css/font/font-awesome/fontawesome-webfont.svg","hash":"b06b5c8f67fd632cdc62a33b62ae4f74194131b3","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/4.png","hash":"be1010578082af53f248773a883ac875af904676","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174422276.png","hash":"588366cb2e4d3ae87401f5972cccd294b4a55bb2","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214175022879.png","hash":"08cf0616efc761a32e5e38f9c01856eb1e14c714","modified":1619489803871},{"_id":"public/js/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/5.png","hash":"918add3b718d520c1cc6b3c340dfaf58625e44b7","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214175304898.png","hash":"54b725874e2e3cdb8a0f0819902048c955b05083","modified":1619489803871},{"_id":"public/css/lib/materialize.min.css","hash":"41d1676b73eec020eaeb9b507dcbcd30069ae8cb","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174108194.png","hash":"cd7d300b322101079d7f26615f1ca358931e9098","modified":1619489803871},{"_id":"public/js/materialize.min.js","hash":"c9308fbe808a149aa11061af40a4be5f391cccee","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/3.png","hash":"9a52e35f1b8c80691a5b0be4b1ab6a4f2d56a7f6","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214173840928.png","hash":"53edcce7af1fbaf7d59acf2062316d81e63f723e","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174003005.png","hash":"c8c21d653e549d5835ad6ee49237cb6a7b0f60d4","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174957751.png","hash":"f07858cb57086c6becfbf436e569f90302fbb847","modified":1619489803871},{"_id":"public/2021/02/14/notes-ML-FM/image-20210214174310749.png","hash":"040595ce2e24cd22fab14a223baae6d4a217107f","modified":1619489803871},{"_id":"public/2021/04/26/GANs-specialization-Week1-Notes-and-codes/image-20210426120448703.png","hash":"d2311bfd7b00103933ba6e1aa52a3f838216a5ac","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/7.png","hash":"cc84be16b5350a69e15af7aeb998acfd9c1fcd5d","modified":1619489803871},{"_id":"public/2021/01/05/Robust-Image-Classification-with-Kernel-Density-Function/6.png","hash":"1e4661db20aebb2c3bc315f658beb41e9fe62893","modified":1619489803871}],"Category":[{"name":"Class notes","_id":"cknzeeyli0004emzwcqogcipu"},{"name":"Notes","_id":"cknzeeylp0009emzwbfve7azk"},{"name":"Course project","_id":"cknzeeylr000cemzwdobw86mg"}],"Data":[{"_id":"about","data":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]}},{"_id":"hint","data":{"new":{"selector":[".menu-about"]}}},{"_id":"reading","data":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}}},{"_id":"link","data":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}}},{"_id":"slider","data":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}],"Page":[{"title":"Xiaoyu Xie","layout":"about","_content":"\n<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n# Background\n\nI am Xiaoyu Xie, a first-year PhD student at Northwestern University. \n\n# Research Interests:\n\n- Computer Vision;\n- Deep Learning in Manufacturing, Mechanics, and Finace;\n- Bayesian Deep Learning;\n","source":"about/index.md","raw":"title: Xiaoyu Xie\nlayout: about\n---\n\n<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n# Background\n\nI am Xiaoyu Xie, a first-year PhD student at Northwestern University. \n\n# Research Interests:\n\n- Computer Vision;\n- Deep Learning in Manufacturing, Mechanics, and Finace;\n- Bayesian Deep Learning;\n","date":"2021-04-25T02:55:12.179Z","updated":"2021-04-25T02:55:12.153Z","path":"about/index.html","comments":1,"_id":"cknzeeyl70000emzw0orya64d","content":"<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>I am Xiaoyu Xie, a first-year PhD student at Northwestern University. </p>\n<h1 id=\"Research-Interests\"><a href=\"#Research-Interests\" class=\"headerlink\" title=\"Research Interests:\"></a>Research Interests:</h1><ul>\n<li>Computer Vision;</li>\n<li>Deep Learning in Manufacturing, Mechanics, and Finace;</li>\n<li>Bayesian Deep Learning;</li>\n</ul>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":"<!-- <img src=\"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg\" width = \"300\" height = \"200\" alt=\"\" align=center /> -->\n\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>I am Xiaoyu Xie, a first-year PhD student at Northwestern University. </p>\n<h1 id=\"Research-Interests\"><a href=\"#Research-Interests\" class=\"headerlink\" title=\"Research Interests:\"></a>Research Interests:</h1><ul>\n<li>Computer Vision;</li>\n<li>Deep Learning in Manufacturing, Mechanics, and Finace;</li>\n<li>Bayesian Deep Learning;</li>\n</ul>\n"},{"title":"tags","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: \"tags\"\ntype: tags\nlayout: \"tags\"\n---","date":"2021-01-07T04:08:10.118Z","updated":"2021-01-07T04:08:10.112Z","path":"tags/index.html","comments":1,"_id":"cknzeeylf0002emzwawf6e7uf","content":"","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":""},{"title":"categories","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: \"categories\"\ntype: categories \nlayout: \"categories\"\n---\n","date":"2021-01-07T04:21:57.215Z","updated":"2021-01-07T04:21:57.207Z","path":"categories/index.html","comments":1,"_id":"cknzeeylm0006emzw5qqw6nn6","content":"","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":""},{"title":"Reading","layout":"reading","_content":"\n","source":"reading/index.md","raw":"title: Reading\nlayout: reading\n---\n\n","date":"2021-01-05T11:27:50.020Z","updated":"2021-01-05T11:27:50.007Z","path":"reading/index.html","comments":1,"_id":"cknzeeylo0008emzweedy0kad","content":"","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"","more":""}],"Post":[{"title":"CS61B-Week2-Notes","date":"2021-04-24T14:26:44.000Z","_content":"\n# Week 2 - Notes\n\n## Exercise B Level\n\n1. Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method `deleteFirst`, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.\n2. Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.\n\n<!-- more -->\n\n```java\n// Code for the question 1 and 2\n\nimport edu.princeton.cs.algs4.In;\n\n//import java.util.Arrays;\n\n// Using sentinel to replace the first\n\npublic class SLList2 {\n    private static class IntNode {\n        public int item;\n        public IntNode next;\n\n        public IntNode(int i, IntNode n) {\n            item = i;\n            next = n;\n        }\n    }\n\n    private IntNode sentinel;\n    private int size;\n\n    public SLList2() {\n        // initialize with no inputs\n        sentinel = new IntNode(63, null);\n        size = 1;\n    }\n\n    public SLList2(int x) {\n        // initialize with a integer\n        sentinel = new IntNode(63, null);\n        sentinel.next = new IntNode(x, null);\n        size = 1;\n    }\n\n    public SLList2(int[] x) {\n        // initialize with an array\n        size = 0;\n        sentinel = new IntNode(63, null);\n        for (int i = 0; i < x.length; i++) {\n            // get the item from array inversely\n            sentinel.next = new IntNode(x[x.length-i-1], null);\n            size += 1;\n        }\n    }\n\n    /** Add the first item in the list */\n    public void addFirst(int x) {\n        sentinel.next = new IntNode(x, sentinel.next);\n        size += 1;\n    }\n\n    /** Returns the first item in the list */\n    public int getFirst() {\n        return sentinel.next.item;\n    }\n\n    /**\n     * Returns the last item in the list\n     * @return the last item\n     */\n    public int getLast() {\n        if (sentinel.next == null) {\n            return sentinel.item;\n        }\n        sentinel = sentinel.next;\n        return getLast();\n    }\n\n    /**\n     * Add an item to a list\n     * @param args int x\n     */\n    public void addLast(int x) {\n        size += 1;\n        IntNode p = sentinel;\n\n        /* Advance p to the end of the list. */\n        while (p.next != null) {\n            p = p.next;\n        }\n        p.next = new IntNode(x, null);\n    }\n\n    public int size() {\n        return size;\n    }\n\n    public int deleteFirst() {\n        /* sentinel.next or sentinel.next.next\n        could be null when size == 0 */\n        if (sentinel.next == null) {\n            return -1;\n        }\n\n        IntNode deleteNode = sentinel.next;\n\n        if (sentinel.next.next == null) {\n            sentinel.next = new IntNode(-1, null);\n            return deleteNode.item;\n        }\n        sentinel.next = sentinel.next.next;\n        return deleteNode.item;\n    }\n\n    public static void main(String[] args) {\n        /** Test the constructor that takes in an array of integers*/\n        int[] arr = new int[]{1,2,3};\n        SLList2 L = new SLList2(arr);\n        System.out.println(L.getFirst());\n      \n        SLList2 L = new SLList2(1);\n        L.addFirst(2);\n        System.out.println(L.getFirst());\n        L.addFirst(3);\n        L.deleteFirst();\n        System.out.print(\"Final: \");\n        System.out.println(L.getFirst());\n//        L.addLast(100);\n//        System.out.println(L.getLast());\n//        System.out.println(L.size());\n    }\n}\n```\n\n# Exercise A Level\n\n![image-20210425113258793](CS61B-Week2/image-20210425113258793.png)\n\n[Problem Link: Osmosis](https://www.kartikkapur.com/documents/mt1.pdf#page=7)\n\n```java\npublic class IntList2 {\n    public int first;\n    public IntList2 rest;\n\n    public IntList2(int f, IntList2 r){\n        first = f;\n        rest = r;\n    }\n\n    // Iterative\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentIter(IntList2 p){\n        /* if p == null, p.rest will no longer execute */\n        if (p.rest == null) {\n            /* size <= 1 */\n            return;\n        }\n\n        /**\n         * p.rest != null\n         * p ends at the last node finally\n         * loop through 1st ~ last 2nd node\n         */\n        while (p.rest != null) { /* p ends at the last node */\n            if (p.first == p.rest.first) {\n                /* merge */\n                p.first *= 2;\n                p.rest = p.rest.rest; /* it's okay if it is null */\n            } else {\n                p = p.rest;\n            }\n        }\n    }\n\n    // recursion\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentRec(IntList2 p) {\n        if (p == null) return;\n        adj(p, p.rest);\n    }\n\n    // helper function - pass previous node recursively\n    private void adj(IntList2 prev, IntList2 current) {\n        if (current == null) return;\n        if (prev.first == current.first) {\n            prev.first *= 2;\n            prev.rest = current.rest; // maybe null\n            adj(prev, prev.rest); // I fixed this part that is wrong in the reference link.\n        } else {\n            adj(current, current.rest);\n        }\n    }\n\n    // Display an IntList\n    public void display(IntList2 L) {\n        while (L.rest != null) {\n            System.out.print(L.first);\n            System.out.print(\", \");\n            L.first = L.rest.first;\n            L.rest = L.rest.rest;\n        }\n        System.out.println(L.first);\n    }\n\n    public static void main(String[] args) {\n        IntList2 L = new IntList2(3, null);\n        L =new IntList2(2, L);\n        L =new IntList2(1, L);\n        L =new IntList2(1, L);\n\n      \t// There are two methods.\n        // Method 1: Recursive\n        L.addAdjacentRec(L);\n\n        // Method 2: Iterative\n//        L.addAdjacentIter(L);\n        System.out.print(\"Final: \");\n        L.display(L);\n    }\n}\n```\n\n# Reading 2.3: The DLList\n\n## addLast\n\n```java\npublic class SLList {\n    private IntNode sentinel;\n    private IntNode last;\n    private int size;    \n\n    public void addLast(int x) {\n        last.next = new IntNode(x, null);\n        last = last.next;\n        size += 1;\n    }\n    ...\n}\n```\n\n\n\n**Exercise 2.3.1:** Consider the box and pointer diagram representing the `SLList` implementation above, which includes the last pointer. Suppose that we'd like to support `addLast`, `getLast`, and `removeLast` operations. Will the structure shown support rapid `addLast`, `getLast`, and `removeLast` operations? If not, which operations are slow?\n\n![sllist_last_pointer.png](sllist_last_pointer.png)\n\n**Answer 2.3.1:** `addLast` and `getLast` will be fast, but `removeLast` will be slow. That's because we have no easy way to get the second-to-last node, to update the `last` pointer, after removing the last node.\n\n## SecondToLast\n\n\n\n\n\n# Reference:\n\n- [CS 61B | Part 1 | List (Linked List & Array List)](https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html)\n","source":"_posts/CS61B-Week2.md","raw":"---\ntitle: CS61B-Week2-Notes\ndate: 2021-04-24 22:26:44\ntags: \n- Online Course\n- Algorithm\n- Data structure\n- Notes\ncategories: Class notes\n---\n\n# Week 2 - Notes\n\n## Exercise B Level\n\n1. Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method `deleteFirst`, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.\n2. Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.\n\n<!-- more -->\n\n```java\n// Code for the question 1 and 2\n\nimport edu.princeton.cs.algs4.In;\n\n//import java.util.Arrays;\n\n// Using sentinel to replace the first\n\npublic class SLList2 {\n    private static class IntNode {\n        public int item;\n        public IntNode next;\n\n        public IntNode(int i, IntNode n) {\n            item = i;\n            next = n;\n        }\n    }\n\n    private IntNode sentinel;\n    private int size;\n\n    public SLList2() {\n        // initialize with no inputs\n        sentinel = new IntNode(63, null);\n        size = 1;\n    }\n\n    public SLList2(int x) {\n        // initialize with a integer\n        sentinel = new IntNode(63, null);\n        sentinel.next = new IntNode(x, null);\n        size = 1;\n    }\n\n    public SLList2(int[] x) {\n        // initialize with an array\n        size = 0;\n        sentinel = new IntNode(63, null);\n        for (int i = 0; i < x.length; i++) {\n            // get the item from array inversely\n            sentinel.next = new IntNode(x[x.length-i-1], null);\n            size += 1;\n        }\n    }\n\n    /** Add the first item in the list */\n    public void addFirst(int x) {\n        sentinel.next = new IntNode(x, sentinel.next);\n        size += 1;\n    }\n\n    /** Returns the first item in the list */\n    public int getFirst() {\n        return sentinel.next.item;\n    }\n\n    /**\n     * Returns the last item in the list\n     * @return the last item\n     */\n    public int getLast() {\n        if (sentinel.next == null) {\n            return sentinel.item;\n        }\n        sentinel = sentinel.next;\n        return getLast();\n    }\n\n    /**\n     * Add an item to a list\n     * @param args int x\n     */\n    public void addLast(int x) {\n        size += 1;\n        IntNode p = sentinel;\n\n        /* Advance p to the end of the list. */\n        while (p.next != null) {\n            p = p.next;\n        }\n        p.next = new IntNode(x, null);\n    }\n\n    public int size() {\n        return size;\n    }\n\n    public int deleteFirst() {\n        /* sentinel.next or sentinel.next.next\n        could be null when size == 0 */\n        if (sentinel.next == null) {\n            return -1;\n        }\n\n        IntNode deleteNode = sentinel.next;\n\n        if (sentinel.next.next == null) {\n            sentinel.next = new IntNode(-1, null);\n            return deleteNode.item;\n        }\n        sentinel.next = sentinel.next.next;\n        return deleteNode.item;\n    }\n\n    public static void main(String[] args) {\n        /** Test the constructor that takes in an array of integers*/\n        int[] arr = new int[]{1,2,3};\n        SLList2 L = new SLList2(arr);\n        System.out.println(L.getFirst());\n      \n        SLList2 L = new SLList2(1);\n        L.addFirst(2);\n        System.out.println(L.getFirst());\n        L.addFirst(3);\n        L.deleteFirst();\n        System.out.print(\"Final: \");\n        System.out.println(L.getFirst());\n//        L.addLast(100);\n//        System.out.println(L.getLast());\n//        System.out.println(L.size());\n    }\n}\n```\n\n# Exercise A Level\n\n![image-20210425113258793](CS61B-Week2/image-20210425113258793.png)\n\n[Problem Link: Osmosis](https://www.kartikkapur.com/documents/mt1.pdf#page=7)\n\n```java\npublic class IntList2 {\n    public int first;\n    public IntList2 rest;\n\n    public IntList2(int f, IntList2 r){\n        first = f;\n        rest = r;\n    }\n\n    // Iterative\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentIter(IntList2 p){\n        /* if p == null, p.rest will no longer execute */\n        if (p.rest == null) {\n            /* size <= 1 */\n            return;\n        }\n\n        /**\n         * p.rest != null\n         * p ends at the last node finally\n         * loop through 1st ~ last 2nd node\n         */\n        while (p.rest != null) { /* p ends at the last node */\n            if (p.first == p.rest.first) {\n                /* merge */\n                p.first *= 2;\n                p.rest = p.rest.rest; /* it's okay if it is null */\n            } else {\n                p = p.rest;\n            }\n        }\n    }\n\n    // recursion\n    // Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\n    public void addAdjacentRec(IntList2 p) {\n        if (p == null) return;\n        adj(p, p.rest);\n    }\n\n    // helper function - pass previous node recursively\n    private void adj(IntList2 prev, IntList2 current) {\n        if (current == null) return;\n        if (prev.first == current.first) {\n            prev.first *= 2;\n            prev.rest = current.rest; // maybe null\n            adj(prev, prev.rest); // I fixed this part that is wrong in the reference link.\n        } else {\n            adj(current, current.rest);\n        }\n    }\n\n    // Display an IntList\n    public void display(IntList2 L) {\n        while (L.rest != null) {\n            System.out.print(L.first);\n            System.out.print(\", \");\n            L.first = L.rest.first;\n            L.rest = L.rest.rest;\n        }\n        System.out.println(L.first);\n    }\n\n    public static void main(String[] args) {\n        IntList2 L = new IntList2(3, null);\n        L =new IntList2(2, L);\n        L =new IntList2(1, L);\n        L =new IntList2(1, L);\n\n      \t// There are two methods.\n        // Method 1: Recursive\n        L.addAdjacentRec(L);\n\n        // Method 2: Iterative\n//        L.addAdjacentIter(L);\n        System.out.print(\"Final: \");\n        L.display(L);\n    }\n}\n```\n\n# Reading 2.3: The DLList\n\n## addLast\n\n```java\npublic class SLList {\n    private IntNode sentinel;\n    private IntNode last;\n    private int size;    \n\n    public void addLast(int x) {\n        last.next = new IntNode(x, null);\n        last = last.next;\n        size += 1;\n    }\n    ...\n}\n```\n\n\n\n**Exercise 2.3.1:** Consider the box and pointer diagram representing the `SLList` implementation above, which includes the last pointer. Suppose that we'd like to support `addLast`, `getLast`, and `removeLast` operations. Will the structure shown support rapid `addLast`, `getLast`, and `removeLast` operations? If not, which operations are slow?\n\n![sllist_last_pointer.png](sllist_last_pointer.png)\n\n**Answer 2.3.1:** `addLast` and `getLast` will be fast, but `removeLast` will be slow. That's because we have no easy way to get the second-to-last node, to update the `last` pointer, after removing the last node.\n\n## SecondToLast\n\n\n\n\n\n# Reference:\n\n- [CS 61B | Part 1 | List (Linked List & Array List)](https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html)\n","slug":"CS61B-Week2","published":1,"updated":"2021-04-25T15:15:00.337Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknzeeylb0001emzw8va12fh8","content":"<h1 id=\"Week-2-Notes\"><a href=\"#Week-2-Notes\" class=\"headerlink\" title=\"Week 2 - Notes\"></a>Week 2 - Notes</h1><h2 id=\"Exercise-B-Level\"><a href=\"#Exercise-B-Level\" class=\"headerlink\" title=\"Exercise B Level\"></a>Exercise B Level</h2><ol>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method <code>deleteFirst</code>, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.</li>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.</li>\n</ol>\n<a id=\"more\"></a>\n\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Code for the question 1 and 2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> edu.princeton.cs.algs4.In;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//import java.util.Arrays;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Using sentinel to replace the first</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SLList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntNode</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> item;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> IntNode next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntNode</span><span class=\"params\">(<span class=\"keyword\">int</span> i, IntNode n)</span> </span>&#123;</span><br><span class=\"line\">            item = i;</span><br><span class=\"line\">            next = n;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode sentinel;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> size;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with no inputs</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with a integer</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span>[] x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with an array</span></span><br><span class=\"line\">        size = <span class=\"number\">0</span>;</span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; x.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// get the item from array inversely</span></span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(x[x.length-i-<span class=\"number\">1</span>], <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Add the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addFirst</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, sentinel.next);</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Returns the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentinel.next.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns the last item in the list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> the last item</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getLast</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> sentinel.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel = sentinel.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> getLast();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Add an item to a list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> args int x</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addLast</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        IntNode p = sentinel;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Advance p to the end of the list. */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            p = p.next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        p.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">size</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> size;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">deleteFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* sentinel.next or sentinel.next.next</span></span><br><span class=\"line\"><span class=\"comment\">        could be null when size == 0 */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        IntNode deleteNode = sentinel.next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(-<span class=\"number\">1</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel.next = sentinel.next.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/** Test the constructor that takes in an array of integers*/</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span>[] arr = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[]&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>&#125;;</span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(arr);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">      </span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(<span class=\"number\">1</span>);</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">2</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">3</span>);</span><br><span class=\"line\">        L.deleteFirst();</span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\"><span class=\"comment\">//        L.addLast(100);</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.getLast());</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.size());</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Exercise-A-Level\"><a href=\"#Exercise-A-Level\" class=\"headerlink\" title=\"Exercise A Level\"></a>Exercise A Level</h1><p><img src=\"CS61B-Week2/image-20210425113258793.png\" alt=\"image-20210425113258793\"></p>\n<p><a href=\"https://www.kartikkapur.com/documents/mt1.pdf#page=7\">Problem Link: Osmosis</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> first;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> IntList2 rest;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntList2</span><span class=\"params\">(<span class=\"keyword\">int</span> f, IntList2 r)</span></span>&#123;</span><br><span class=\"line\">        first = f;</span><br><span class=\"line\">        rest = r;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Iterative</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentIter</span><span class=\"params\">(IntList2 p)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* if p == null, p.rest will no longer execute */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p.rest == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* size &lt;= 1 */</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * p.rest != null</span></span><br><span class=\"line\"><span class=\"comment\">         * p ends at the last node finally</span></span><br><span class=\"line\"><span class=\"comment\">         * loop through 1st ~ last 2nd node</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.rest != <span class=\"keyword\">null</span>) &#123; <span class=\"comment\">/* p ends at the last node */</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (p.first == p.rest.first) &#123;</span><br><span class=\"line\">                <span class=\"comment\">/* merge */</span></span><br><span class=\"line\">                p.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">                p.rest = p.rest.rest; <span class=\"comment\">/* it&#x27;s okay if it is null */</span></span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                p = p.rest;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// recursion</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentRec</span><span class=\"params\">(IntList2 p)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        adj(p, p.rest);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// helper function - pass previous node recursively</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">adj</span><span class=\"params\">(IntList2 prev, IntList2 current)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (current == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prev.first == current.first) &#123;</span><br><span class=\"line\">            prev.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">            prev.rest = current.rest; <span class=\"comment\">// maybe null</span></span><br><span class=\"line\">            adj(prev, prev.rest); <span class=\"comment\">// I fixed this part that is wrong in the reference link.</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            adj(current, current.rest);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Display an IntList</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">display</span><span class=\"params\">(IntList2 L)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (L.rest != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            System.out.print(L.first);</span><br><span class=\"line\">            System.out.print(<span class=\"string\">&quot;, &quot;</span>);</span><br><span class=\"line\">            L.first = L.rest.first;</span><br><span class=\"line\">            L.rest = L.rest.rest;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        System.out.println(L.first);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        IntList2 L = <span class=\"keyword\">new</span> IntList2(<span class=\"number\">3</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">2</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\"></span><br><span class=\"line\">      \t<span class=\"comment\">// There are two methods.</span></span><br><span class=\"line\">        <span class=\"comment\">// Method 1: Recursive</span></span><br><span class=\"line\">        L.addAdjacentRec(L);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Method 2: Iterative</span></span><br><span class=\"line\"><span class=\"comment\">//        L.addAdjacentIter(L);</span></span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        L.display(L);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Reading-2-3-The-DLList\"><a href=\"#Reading-2-3-The-DLList\" class=\"headerlink\" title=\"Reading 2.3: The DLList\"></a>Reading 2.3: The DLList</h1><h2 id=\"addLast\"><a href=\"#addLast\" class=\"headerlink\" title=\"addLast\"></a>addLast</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SLList</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode sentinel;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode last;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> size;    </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addLast</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        last.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        last = last.next;</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>Exercise 2.3.1:</strong> Consider the box and pointer diagram representing the <code>SLList</code> implementation above, which includes the last pointer. Suppose that we’d like to support <code>addLast</code>, <code>getLast</code>, and <code>removeLast</code> operations. Will the structure shown support rapid <code>addLast</code>, <code>getLast</code>, and <code>removeLast</code> operations? If not, which operations are slow?</p>\n<p><img src=\"sllist_last_pointer.png\" alt=\"sllist_last_pointer.png\"></p>\n<p><strong>Answer 2.3.1:</strong> <code>addLast</code> and <code>getLast</code> will be fast, but <code>removeLast</code> will be slow. That’s because we have no easy way to get the second-to-last node, to update the <code>last</code> pointer, after removing the last node.</p>\n<h2 id=\"SecondToLast\"><a href=\"#SecondToLast\" class=\"headerlink\" title=\"SecondToLast\"></a>SecondToLast</h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h1><ul>\n<li><a href=\"https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\">CS 61B | Part 1 | List (Linked List &amp; Array List)</a></li>\n</ul>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<h1 id=\"Week-2-Notes\"><a href=\"#Week-2-Notes\" class=\"headerlink\" title=\"Week 2 - Notes\"></a>Week 2 - Notes</h1><h2 id=\"Exercise-B-Level\"><a href=\"#Exercise-B-Level\" class=\"headerlink\" title=\"Exercise B Level\"></a>Exercise B Level</h2><ol>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement the method <code>deleteFirst</code>, which deletes the first element in your SLList. Don’t forget to maintain the three invariants discussed above.</li>\n<li>Starting from the copy of SLList.java provided to you in the lecture code repository, implement a second constructor that takes in an array of integers, and creates an SLList with those integers. Again, remember to maintain your invariants.</li>\n</ol>","more":"<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Code for the question 1 and 2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> edu.princeton.cs.algs4.In;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//import java.util.Arrays;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Using sentinel to replace the first</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SLList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntNode</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> item;</span><br><span class=\"line\">        <span class=\"keyword\">public</span> IntNode next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntNode</span><span class=\"params\">(<span class=\"keyword\">int</span> i, IntNode n)</span> </span>&#123;</span><br><span class=\"line\">            item = i;</span><br><span class=\"line\">            next = n;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode sentinel;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> size;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with no inputs</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with a integer</span></span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        size = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">SLList2</span><span class=\"params\">(<span class=\"keyword\">int</span>[] x)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// initialize with an array</span></span><br><span class=\"line\">        size = <span class=\"number\">0</span>;</span><br><span class=\"line\">        sentinel = <span class=\"keyword\">new</span> IntNode(<span class=\"number\">63</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; x.length; i++) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// get the item from array inversely</span></span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(x[x.length-i-<span class=\"number\">1</span>], <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Add the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addFirst</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        sentinel.next = <span class=\"keyword\">new</span> IntNode(x, sentinel.next);</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/** Returns the first item in the list */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentinel.next.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Returns the last item in the list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> the last item</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getLast</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> sentinel.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel = sentinel.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> getLast();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">     * Add an item to a list</span></span><br><span class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> args int x</span></span><br><span class=\"line\"><span class=\"comment\">     */</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addLast</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">        IntNode p = sentinel;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/* Advance p to the end of the list. */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.next != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            p = p.next;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        p.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">size</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> size;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">deleteFirst</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* sentinel.next or sentinel.next.next</span></span><br><span class=\"line\"><span class=\"comment\">        could be null when size == 0 */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        IntNode deleteNode = sentinel.next;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (sentinel.next.next == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            sentinel.next = <span class=\"keyword\">new</span> IntNode(-<span class=\"number\">1</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        sentinel.next = sentinel.next.next;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> deleteNode.item;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/** Test the constructor that takes in an array of integers*/</span></span><br><span class=\"line\">        <span class=\"keyword\">int</span>[] arr = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[]&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>&#125;;</span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(arr);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">      </span><br><span class=\"line\">        SLList2 L = <span class=\"keyword\">new</span> SLList2(<span class=\"number\">1</span>);</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">2</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\">        L.addFirst(<span class=\"number\">3</span>);</span><br><span class=\"line\">        L.deleteFirst();</span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        System.out.println(L.getFirst());</span><br><span class=\"line\"><span class=\"comment\">//        L.addLast(100);</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.getLast());</span></span><br><span class=\"line\"><span class=\"comment\">//        System.out.println(L.size());</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Exercise-A-Level\"><a href=\"#Exercise-A-Level\" class=\"headerlink\" title=\"Exercise A Level\"></a>Exercise A Level</h1><p><img src=\"CS61B-Week2/image-20210425113258793.png\" alt=\"image-20210425113258793\"></p>\n<p><a href=\"https://www.kartikkapur.com/documents/mt1.pdf#page=7\">Problem Link: Osmosis</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntList2</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">int</span> first;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> IntList2 rest;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"title\">IntList2</span><span class=\"params\">(<span class=\"keyword\">int</span> f, IntList2 r)</span></span>&#123;</span><br><span class=\"line\">        first = f;</span><br><span class=\"line\">        rest = r;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Iterative</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentIter</span><span class=\"params\">(IntList2 p)</span></span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">/* if p == null, p.rest will no longer execute */</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p.rest == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">/* size &lt;= 1 */</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">         * p.rest != null</span></span><br><span class=\"line\"><span class=\"comment\">         * p ends at the last node finally</span></span><br><span class=\"line\"><span class=\"comment\">         * loop through 1st ~ last 2nd node</span></span><br><span class=\"line\"><span class=\"comment\">         */</span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (p.rest != <span class=\"keyword\">null</span>) &#123; <span class=\"comment\">/* p ends at the last node */</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (p.first == p.rest.first) &#123;</span><br><span class=\"line\">                <span class=\"comment\">/* merge */</span></span><br><span class=\"line\">                p.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">                p.rest = p.rest.rest; <span class=\"comment\">/* it&#x27;s okay if it is null */</span></span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">                p = p.rest;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// recursion</span></span><br><span class=\"line\">    <span class=\"comment\">// Reference: https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addAdjacentRec</span><span class=\"params\">(IntList2 p)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        adj(p, p.rest);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// helper function - pass previous node recursively</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">adj</span><span class=\"params\">(IntList2 prev, IntList2 current)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (current == <span class=\"keyword\">null</span>) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (prev.first == current.first) &#123;</span><br><span class=\"line\">            prev.first *= <span class=\"number\">2</span>;</span><br><span class=\"line\">            prev.rest = current.rest; <span class=\"comment\">// maybe null</span></span><br><span class=\"line\">            adj(prev, prev.rest); <span class=\"comment\">// I fixed this part that is wrong in the reference link.</span></span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">            adj(current, current.rest);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Display an IntList</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">display</span><span class=\"params\">(IntList2 L)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (L.rest != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            System.out.print(L.first);</span><br><span class=\"line\">            System.out.print(<span class=\"string\">&quot;, &quot;</span>);</span><br><span class=\"line\">            L.first = L.rest.first;</span><br><span class=\"line\">            L.rest = L.rest.rest;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        System.out.println(L.first);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">        IntList2 L = <span class=\"keyword\">new</span> IntList2(<span class=\"number\">3</span>, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">2</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\">        L =<span class=\"keyword\">new</span> IntList2(<span class=\"number\">1</span>, L);</span><br><span class=\"line\"></span><br><span class=\"line\">      \t<span class=\"comment\">// There are two methods.</span></span><br><span class=\"line\">        <span class=\"comment\">// Method 1: Recursive</span></span><br><span class=\"line\">        L.addAdjacentRec(L);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Method 2: Iterative</span></span><br><span class=\"line\"><span class=\"comment\">//        L.addAdjacentIter(L);</span></span><br><span class=\"line\">        System.out.print(<span class=\"string\">&quot;Final: &quot;</span>);</span><br><span class=\"line\">        L.display(L);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Reading-2-3-The-DLList\"><a href=\"#Reading-2-3-The-DLList\" class=\"headerlink\" title=\"Reading 2.3: The DLList\"></a>Reading 2.3: The DLList</h1><h2 id=\"addLast\"><a href=\"#addLast\" class=\"headerlink\" title=\"addLast\"></a>addLast</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SLList</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode sentinel;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> IntNode last;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">int</span> size;    </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">addLast</span><span class=\"params\">(<span class=\"keyword\">int</span> x)</span> </span>&#123;</span><br><span class=\"line\">        last.next = <span class=\"keyword\">new</span> IntNode(x, <span class=\"keyword\">null</span>);</span><br><span class=\"line\">        last = last.next;</span><br><span class=\"line\">        size += <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p><strong>Exercise 2.3.1:</strong> Consider the box and pointer diagram representing the <code>SLList</code> implementation above, which includes the last pointer. Suppose that we’d like to support <code>addLast</code>, <code>getLast</code>, and <code>removeLast</code> operations. Will the structure shown support rapid <code>addLast</code>, <code>getLast</code>, and <code>removeLast</code> operations? If not, which operations are slow?</p>\n<p><img src=\"sllist_last_pointer.png\" alt=\"sllist_last_pointer.png\"></p>\n<p><strong>Answer 2.3.1:</strong> <code>addLast</code> and <code>getLast</code> will be fast, but <code>removeLast</code> will be slow. That’s because we have no easy way to get the second-to-last node, to update the <code>last</code> pointer, after removing the last node.</p>\n<h2 id=\"SecondToLast\"><a href=\"#SecondToLast\" class=\"headerlink\" title=\"SecondToLast\"></a>SecondToLast</h2><h1 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference:\"></a>Reference:</h1><ul>\n<li><a href=\"https://www.junhaow.com/studynotes/cs61b/cs61b%20p1.html\">CS 61B | Part 1 | List (Linked List &amp; Array List)</a></li>\n</ul>"},{"title":"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers","date":"2021-02-17T03:38:03.000Z","_content":"\n# Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\n\nRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.\n\nNote that please check the lastest verison in [github](https://github.com/xiaoyuxie-vico/ML_PDE_Resources).\n\n<!-- more -->\n\n# Model Zoo\n\n| Model                   | Relevant Papers                                              | Link                                                         | Notes                 |\n| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |\n| HiDeNN                  | Saha, Sourav, et al. \"**Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.**\" Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452. | [Paper](https://www.sciencedirect.com/science/article/pii/S004578252030637X) |                       |\n| HiTSs                   | Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. \"**Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.**\" arXiv preprint arXiv:2008.09768 (2020). | [Paper](http://arxiv.org/abs/2102.01010), [Code](https://github.com/luckystarufo/multiscale_HiTS), [Video](https://www.youtube.com/watch?v=Jfl3dIlSTrU) |                       |\n|                         | Kochkov, Dmitrii, et al. \"**Machine learning accelerated computational fluid dynamics.**\" arXiv preprint arXiv:2102.01010 (2021). | [Paper](http://arxiv.org/abs/2102.01010)                     | Google                |\n| Fourier Neural Operator | Li, Zongyi, et al. \"**Fourier neural operator for parametric partial differential equations.**\" arXiv preprint arXiv:2010.08895 (2020). | [Paper](https://arxiv.org/abs/2010.08895), [Code](https://github.com/zongyi-li/fourier_neural_operator), [Video](https://www.youtube.com/watch?v=IaS72aHrJKE) |                       |\n| PINNs                   | Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"**Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.**\" Journal of Computational Physics 378 (2019): 686-707; | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ), [Code](https://github.com/maziarraissi/PINNs), [Video](https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA) |                       |\n|                         | Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"**Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.**\" *Journal of Fluid Mechanics* 807 (2016): 155-166. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB), [Code](https://github.com/tbnn/tbnn) | Pure data             |\n|                         | K. Duraisamy, G. Iaccarino, and H. Xiao, **Turbulence modeling in the age of data**, Annual Review of Fluid Mechanics 51, 357 (2019). | [Paper](https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs) | Pure data, Review     |\n|                         | Maulik, Romit, et al. \"**Subgrid modelling for two-dimensional turbulence using neural networks.**\" *Journal of Fluid Mechanics* 858 (2019): 122-144. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1) |                       |\n|                         | Beck, Andrea, David Flad, and Claus-Dieter Munz. \"**Deep neural networks for data-driven LES closure models.**\" *Journal of Computational Physics* 398 (2019): 108910. | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ) |                       |\n|                         | Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. \"**Deep learning for universal linear embeddings of nonlinear dynamics.**\" *Nature communications* 9.1 (2018): 1-10. | [Paper](https://www.nature.com/articles/s41467-018-07210-0)  | Nature communications |\n|                         | Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. \"**DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).**\" *arXiv preprint arXiv:1911.09145* (2019). | [Paper](https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145) |                       |\n|                         | Um, Kiwon, et al. \"**Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.**\" *arXiv preprint arXiv:2007.00016* (2020). | [Paper](https://arxiv.org/abs/2007.00016)                    |                       |\n\n# Videos\n- [2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations](https://www.youtube.com/watch?v=2Ab-8xTI89c)\n- [Steve Brunton: Machine Learning for Fluid Dynamics](https://www.youtube.com/watch?v=20vB4MzAbCwv)\n- [Petros Koumoutsakos: \"Machine Learning for Fluid Mechanics\"](https://www.youtube.com/watch?v=gv20VsKqgpc)\n\n# Blogs\n- [Fourier Neural Operator](https://zongyi-li.github.io/blog/2020/fourier-pde/)\n\n# Research Groups\n- [Brunton Lab: Data-driven dynamics and control](https://www.eigensteve.com/)\n- [Animashree Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/group.html)\n- [Wing Kam Liu Group](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html)\n\n# Contact\nIf you like, please star or fork.\n\nWelcome any comments or feedbacks!\n\nEmail: xiaoyuxie2020@u.northwestern.edu\n\n\n\n","source":"_posts/Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers.md","raw":"---\ntitle: >-\n  Machine Learning / Deep Learning for Partial Differential Equations (PDEs)\n  Solvers\ndate: 2021-02-17 11:38:03\ncategories: Notes\ntags: \n\t- Machine Learning\n\t- Deep Learning\n\t- Fluid Mechanics\n\t- Partial Differential Equations\n---\n\n# Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\n\nRecently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.\n\nNote that please check the lastest verison in [github](https://github.com/xiaoyuxie-vico/ML_PDE_Resources).\n\n<!-- more -->\n\n# Model Zoo\n\n| Model                   | Relevant Papers                                              | Link                                                         | Notes                 |\n| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------------- |\n| HiDeNN                  | Saha, Sourav, et al. \"**Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.**\" Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452. | [Paper](https://www.sciencedirect.com/science/article/pii/S004578252030637X) |                       |\n| HiTSs                   | Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. \"**Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.**\" arXiv preprint arXiv:2008.09768 (2020). | [Paper](http://arxiv.org/abs/2102.01010), [Code](https://github.com/luckystarufo/multiscale_HiTS), [Video](https://www.youtube.com/watch?v=Jfl3dIlSTrU) |                       |\n|                         | Kochkov, Dmitrii, et al. \"**Machine learning accelerated computational fluid dynamics.**\" arXiv preprint arXiv:2102.01010 (2021). | [Paper](http://arxiv.org/abs/2102.01010)                     | Google                |\n| Fourier Neural Operator | Li, Zongyi, et al. \"**Fourier neural operator for parametric partial differential equations.**\" arXiv preprint arXiv:2010.08895 (2020). | [Paper](https://arxiv.org/abs/2010.08895), [Code](https://github.com/zongyi-li/fourier_neural_operator), [Video](https://www.youtube.com/watch?v=IaS72aHrJKE) |                       |\n| PINNs                   | Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"**Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.**\" Journal of Computational Physics 378 (2019): 686-707; | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ), [Code](https://github.com/maziarraissi/PINNs), [Video](https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA) |                       |\n|                         | Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"**Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.**\" *Journal of Fluid Mechanics* 807 (2016): 155-166. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB), [Code](https://github.com/tbnn/tbnn) | Pure data             |\n|                         | K. Duraisamy, G. Iaccarino, and H. Xiao, **Turbulence modeling in the age of data**, Annual Review of Fluid Mechanics 51, 357 (2019). | [Paper](https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs) | Pure data, Review     |\n|                         | Maulik, Romit, et al. \"**Subgrid modelling for two-dimensional turbulence using neural networks.**\" *Journal of Fluid Mechanics* 858 (2019): 122-144. | [Paper](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1) |                       |\n|                         | Beck, Andrea, David Flad, and Claus-Dieter Munz. \"**Deep neural networks for data-driven LES closure models.**\" *Journal of Computational Physics* 398 (2019): 108910. | [Paper](https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ) |                       |\n|                         | Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. \"**Deep learning for universal linear embeddings of nonlinear dynamics.**\" *Nature communications* 9.1 (2018): 1-10. | [Paper](https://www.nature.com/articles/s41467-018-07210-0)  | Nature communications |\n|                         | Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. \"**DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).**\" *arXiv preprint arXiv:1911.09145* (2019). | [Paper](https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145) |                       |\n|                         | Um, Kiwon, et al. \"**Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.**\" *arXiv preprint arXiv:2007.00016* (2020). | [Paper](https://arxiv.org/abs/2007.00016)                    |                       |\n\n# Videos\n- [2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations](https://www.youtube.com/watch?v=2Ab-8xTI89c)\n- [Steve Brunton: Machine Learning for Fluid Dynamics](https://www.youtube.com/watch?v=20vB4MzAbCwv)\n- [Petros Koumoutsakos: \"Machine Learning for Fluid Mechanics\"](https://www.youtube.com/watch?v=gv20VsKqgpc)\n\n# Blogs\n- [Fourier Neural Operator](https://zongyi-li.github.io/blog/2020/fourier-pde/)\n\n# Research Groups\n- [Brunton Lab: Data-driven dynamics and control](https://www.eigensteve.com/)\n- [Animashree Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/group.html)\n- [Wing Kam Liu Group](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html)\n\n# Contact\nIf you like, please star or fork.\n\nWelcome any comments or feedbacks!\n\nEmail: xiaoyuxie2020@u.northwestern.edu\n\n\n\n","slug":"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers","published":1,"updated":"2021-02-17T13:13:40.346Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknzeeylg0003emzwfg7y19ax","content":"<h1 id=\"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\"><a href=\"#Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\" class=\"headerlink\" title=\"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\"></a>Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers</h1><p>Recently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.</p>\n<p>Note that please check the lastest verison in <a href=\"https://github.com/xiaoyuxie-vico/ML_PDE_Resources\">github</a>.</p>\n<a id=\"more\"></a>\n\n<h1 id=\"Model-Zoo\"><a href=\"#Model-Zoo\" class=\"headerlink\" title=\"Model Zoo\"></a>Model Zoo</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Relevant Papers</th>\n<th>Link</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HiDeNN</td>\n<td>Saha, Sourav, et al. “<strong>Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.</strong>“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S004578252030637X\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td>HiTSs</td>\n<td>Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “<strong>Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.</strong>“ arXiv preprint arXiv:2008.09768 (2020).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a>, <a href=\"https://github.com/luckystarufo/multiscale_HiTS\">Code</a>, <a href=\"https://www.youtube.com/watch?v=Jfl3dIlSTrU\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Kochkov, Dmitrii, et al. “<strong>Machine learning accelerated computational fluid dynamics.</strong>“ arXiv preprint arXiv:2102.01010 (2021).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a></td>\n<td>Google</td>\n</tr>\n<tr>\n<td>Fourier Neural Operator</td>\n<td>Li, Zongyi, et al. “<strong>Fourier neural operator for parametric partial differential equations.</strong>“ arXiv preprint arXiv:2010.08895 (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2010.08895\">Paper</a>, <a href=\"https://github.com/zongyi-li/fourier_neural_operator\">Code</a>, <a href=\"https://www.youtube.com/watch?v=IaS72aHrJKE\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td>PINNs</td>\n<td>Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “<strong>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</strong>“ Journal of Computational Physics 378 (2019): 686-707;</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ\">Paper</a>, <a href=\"https://github.com/maziarraissi/PINNs\">Code</a>, <a href=\"https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “<strong>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.</strong>“ <em>Journal of Fluid Mechanics</em> 807 (2016): 155-166.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB\">Paper</a>, <a href=\"https://github.com/tbnn/tbnn\">Code</a></td>\n<td>Pure data</td>\n</tr>\n<tr>\n<td></td>\n<td>K. Duraisamy, G. Iaccarino, and H. Xiao, <strong>Turbulence modeling in the age of data</strong>, Annual Review of Fluid Mechanics 51, 357 (2019).</td>\n<td><a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs\">Paper</a></td>\n<td>Pure data, Review</td>\n</tr>\n<tr>\n<td></td>\n<td>Maulik, Romit, et al. “<strong>Subgrid modelling for two-dimensional turbulence using neural networks.</strong>“ <em>Journal of Fluid Mechanics</em> 858 (2019): 122-144.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Beck, Andrea, David Flad, and Claus-Dieter Munz. “<strong>Deep neural networks for data-driven LES closure models.</strong>“ <em>Journal of Computational Physics</em> 398 (2019): 108910.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “<strong>Deep learning for universal linear embeddings of nonlinear dynamics.</strong>“ <em>Nature communications</em> 9.1 (2018): 1-10.</td>\n<td><a href=\"https://www.nature.com/articles/s41467-018-07210-0\">Paper</a></td>\n<td>Nature communications</td>\n</tr>\n<tr>\n<td></td>\n<td>Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “<strong>DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).</strong>“ <em>arXiv preprint arXiv:1911.09145</em> (2019).</td>\n<td><a href=\"https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Um, Kiwon, et al. “<strong>Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.</strong>“ <em>arXiv preprint arXiv:2007.00016</em> (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2007.00016\">Paper</a></td>\n<td></td>\n</tr>\n</tbody></table>\n<h1 id=\"Videos\"><a href=\"#Videos\" class=\"headerlink\" title=\"Videos\"></a>Videos</h1><ul>\n<li><a href=\"https://www.youtube.com/watch?v=2Ab-8xTI89c\">2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=20vB4MzAbCwv\">Steve Brunton: Machine Learning for Fluid Dynamics</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=gv20VsKqgpc\">Petros Koumoutsakos: “Machine Learning for Fluid Mechanics”</a></li>\n</ul>\n<h1 id=\"Blogs\"><a href=\"#Blogs\" class=\"headerlink\" title=\"Blogs\"></a>Blogs</h1><ul>\n<li><a href=\"https://zongyi-li.github.io/blog/2020/fourier-pde/\">Fourier Neural Operator</a></li>\n</ul>\n<h1 id=\"Research-Groups\"><a href=\"#Research-Groups\" class=\"headerlink\" title=\"Research Groups\"></a>Research Groups</h1><ul>\n<li><a href=\"https://www.eigensteve.com/\">Brunton Lab: Data-driven dynamics and control</a></li>\n<li><a href=\"http://tensorlab.cms.caltech.edu/users/anima/group.html\">Animashree Anandkumar</a></li>\n<li><a href=\"https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html\">Wing Kam Liu Group</a></li>\n</ul>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>If you like, please star or fork.</p>\n<p>Welcome any comments or feedbacks!</p>\n<p>Email: <a href=\"mailto:&#120;&#x69;&#x61;&#x6f;&#121;&#117;&#120;&#x69;&#x65;&#x32;&#48;&#50;&#48;&#x40;&#117;&#x2e;&#110;&#x6f;&#114;&#x74;&#104;&#x77;&#x65;&#115;&#116;&#x65;&#114;&#110;&#46;&#101;&#x64;&#x75;\">&#120;&#x69;&#x61;&#x6f;&#121;&#117;&#120;&#x69;&#x65;&#x32;&#48;&#50;&#48;&#x40;&#117;&#x2e;&#110;&#x6f;&#114;&#x74;&#104;&#x77;&#x65;&#115;&#116;&#x65;&#114;&#110;&#46;&#101;&#x64;&#x75;</a></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<h1 id=\"Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\"><a href=\"#Machine-Learning-Deep-Learning-for-Partial-Differential-Equations-PDEs-Solvers\" class=\"headerlink\" title=\"Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers\"></a>Machine Learning / Deep Learning for Partial Differential Equations (PDEs) Solvers</h1><p>Recently, there are a growing number of papers trying to solve PDEs with Machine Learning. This respository is trying to collect and sort papers, blogs, videos, and any format materials in this field.</p>\n<p>Note that please check the lastest verison in <a href=\"https://github.com/xiaoyuxie-vico/ML_PDE_Resources\">github</a>.</p>","more":"<h1 id=\"Model-Zoo\"><a href=\"#Model-Zoo\" class=\"headerlink\" title=\"Model Zoo\"></a>Model Zoo</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Relevant Papers</th>\n<th>Link</th>\n<th>Notes</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>HiDeNN</td>\n<td>Saha, Sourav, et al. “<strong>Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering.</strong>“ Computer Methods in Applied Mechanics and Engineering 373 (2021): 113452.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S004578252030637X\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td>HiTSs</td>\n<td>Liu, Yuying, J. Nathan Kutz, and Steven L. Brunton. “<strong>Hierarchical Deep Learning of Multiscale Differential Equation Time-Steppers.</strong>“ arXiv preprint arXiv:2008.09768 (2020).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a>, <a href=\"https://github.com/luckystarufo/multiscale_HiTS\">Code</a>, <a href=\"https://www.youtube.com/watch?v=Jfl3dIlSTrU\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Kochkov, Dmitrii, et al. “<strong>Machine learning accelerated computational fluid dynamics.</strong>“ arXiv preprint arXiv:2102.01010 (2021).</td>\n<td><a href=\"http://arxiv.org/abs/2102.01010\">Paper</a></td>\n<td>Google</td>\n</tr>\n<tr>\n<td>Fourier Neural Operator</td>\n<td>Li, Zongyi, et al. “<strong>Fourier neural operator for parametric partial differential equations.</strong>“ arXiv preprint arXiv:2010.08895 (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2010.08895\">Paper</a>, <a href=\"https://github.com/zongyi-li/fourier_neural_operator\">Code</a>, <a href=\"https://www.youtube.com/watch?v=IaS72aHrJKE\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td>PINNs</td>\n<td>Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. “<strong>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</strong>“ Journal of Computational Physics 378 (2019): 686-707;</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999118307125?casa_token=LpL_wvHQ4CIAAAAA:9xVIgdgQV8GJnbMHbNvP7Kv_gMncbyvEcVFUQI16hhdexW6B7Mzx03LJC4QSr9txfUZ3kI2OEQ\">Paper</a>, <a href=\"https://github.com/maziarraissi/PINNs\">Code</a>, <a href=\"https://www.youtube.com/watch?v=ewaIDXjmRJAhttps://www.youtube.com/watch?v=ewaIDXjmRJA\">Video</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “<strong>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.</strong>“ <em>Journal of Fluid Mechanics</em> 807 (2016): 155-166.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/0B280EEE89C74A7BF651C422F8FBD1EB\">Paper</a>, <a href=\"https://github.com/tbnn/tbnn\">Code</a></td>\n<td>Pure data</td>\n</tr>\n<tr>\n<td></td>\n<td>K. Duraisamy, G. Iaccarino, and H. Xiao, <strong>Turbulence modeling in the age of data</strong>, Annual Review of Fluid Mechanics 51, 357 (2019).</td>\n<td><a href=\"https://www.annualreviews.org/doi/abs/10.1146/annurev-fluid-010518-040547?casa_token=7LvRzlm9it8AAAAA:Fa4hR567KaYqPjKY60YzhlfxiPjg2vsQypN49usrW2ero3lYpVWjIDtq7UObUq3KYvEszTmNYnzs\">Paper</a></td>\n<td>Pure data, Review</td>\n</tr>\n<tr>\n<td></td>\n<td>Maulik, Romit, et al. “<strong>Subgrid modelling for two-dimensional turbulence using neural networks.</strong>“ <em>Journal of Fluid Mechanics</em> 858 (2019): 122-144.</td>\n<td><a href=\"https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/subgrid-modelling-for-twodimensional-turbulence-using-neural-networks/10EDED1AEAA52C35F3E3A3BB6DC218C1\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Beck, Andrea, David Flad, and Claus-Dieter Munz. “<strong>Deep neural networks for data-driven LES closure models.</strong>“ <em>Journal of Computational Physics</em> 398 (2019): 108910.</td>\n<td><a href=\"https://www.sciencedirect.com/science/article/pii/S0021999119306151?casa_token=U_vDCbpgaBQAAAAA:c7WGYiEprQZkdOUhMJaMO4o0qjhklsjLl8ApCjdqsjA4mt-pHTIySAIGcUELOn6Zr6ggOAwoQQ\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Lusch, Bethany, J. Nathan Kutz, and Steven L. Brunton. “<strong>Deep learning for universal linear embeddings of nonlinear dynamics.</strong>“ <em>Nature communications</em> 9.1 (2018): 1-10.</td>\n<td><a href=\"https://www.nature.com/articles/s41467-018-07210-0\">Paper</a></td>\n<td>Nature communications</td>\n</tr>\n<tr>\n<td></td>\n<td>Freund, Jonathan B., Jonathan F. MacArt, and Justin Sirignano. “<strong>DPM: A deep learning PDE augmentation method (with application to large-eddy simulation).</strong>“ <em>arXiv preprint arXiv:1911.09145</em> (2019).</td>\n<td><a href=\"https://arxiv.org/abs/1911.09145https://arxiv.org/abs/1911.09145\">Paper</a></td>\n<td></td>\n</tr>\n<tr>\n<td></td>\n<td>Um, Kiwon, et al. “<strong>Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers.</strong>“ <em>arXiv preprint arXiv:2007.00016</em> (2020).</td>\n<td><a href=\"https://arxiv.org/abs/2007.00016\">Paper</a></td>\n<td></td>\n</tr>\n</tbody></table>\n<h1 id=\"Videos\"><a href=\"#Videos\" class=\"headerlink\" title=\"Videos\"></a>Videos</h1><ul>\n<li><a href=\"https://www.youtube.com/watch?v=2Ab-8xTI89c\">2020-10-16 - Jaideep Pathak - Using ML to Augment Coarse-Grid CFD Simulations</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=20vB4MzAbCwv\">Steve Brunton: Machine Learning for Fluid Dynamics</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=gv20VsKqgpc\">Petros Koumoutsakos: “Machine Learning for Fluid Mechanics”</a></li>\n</ul>\n<h1 id=\"Blogs\"><a href=\"#Blogs\" class=\"headerlink\" title=\"Blogs\"></a>Blogs</h1><ul>\n<li><a href=\"https://zongyi-li.github.io/blog/2020/fourier-pde/\">Fourier Neural Operator</a></li>\n</ul>\n<h1 id=\"Research-Groups\"><a href=\"#Research-Groups\" class=\"headerlink\" title=\"Research Groups\"></a>Research Groups</h1><ul>\n<li><a href=\"https://www.eigensteve.com/\">Brunton Lab: Data-driven dynamics and control</a></li>\n<li><a href=\"http://tensorlab.cms.caltech.edu/users/anima/group.html\">Animashree Anandkumar</a></li>\n<li><a href=\"https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-kam-wing.html\">Wing Kam Liu Group</a></li>\n</ul>\n<h1 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h1><p>If you like, please star or fork.</p>\n<p>Welcome any comments or feedbacks!</p>\n<p>Email: <a href=\"mailto:&#120;&#x69;&#x61;&#x6f;&#121;&#117;&#120;&#x69;&#x65;&#x32;&#48;&#50;&#48;&#x40;&#117;&#x2e;&#110;&#x6f;&#114;&#x74;&#104;&#x77;&#x65;&#115;&#116;&#x65;&#114;&#110;&#46;&#101;&#x64;&#x75;\">&#120;&#x69;&#x61;&#x6f;&#121;&#117;&#120;&#x69;&#x65;&#x32;&#48;&#50;&#48;&#x40;&#117;&#x2e;&#110;&#x6f;&#114;&#x74;&#104;&#x77;&#x65;&#115;&#116;&#x65;&#114;&#110;&#46;&#101;&#x64;&#x75;</a></p>"},{"title":"Robust and Explainable Image Classification Based on Logits Kernel Density Estimation","date":"2021-01-05T10:51:00.000Z","_content":"\n# Project Description\n\nFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.\n\n<!-- more -->\n\nThere are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.\n\n# Dataset\n\nThe cat and dog dataset used in this project is downloaded from [Kaggel](https://www.kaggle.com/tongpython/cat-and-dog). Several images are shown in the below:\n\nThe data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.\n\nThe batch size is 32 and the dataset will be shuﬄed.\n\n# Model Training\n\nDetailed information about model training can be found in [trainer_cat_dog.ipynb](https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb).\n\nThe accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.\n\n# Error analysis (test set)\n\nAlthought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.\n\n![](1.png)\n\nTrue label is dog, but predicted label is cat and the score is high.\n\n![](2.png)\n\nThe Grad-CAM results for these wrong predicted images are:\n\n![](3.png)\n\nWe can ﬁnd that: \n\n1.  Apparent misclassiﬁcations tend to have a larger attention area; \n2. Understandable misclassiﬁcations tend to have a smaller attention area;\n\n# Distribution analysis\n\nTo solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.\n\nBelow, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.\n\n![](4.png)\n\nKernel density estimation for logits is:\n\n![](5.png)\n\n# Analyze the robustness of classiﬁcation using logits kernel density estimation\n\nIt is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.\n\nFor the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.\n\nIf we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not \"seen\" these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.\n\n![](6.png)\n\nIn the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.\n\n![](7.png)\n\n# Conclusion\n- The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; \n- The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; \n- The proposed approach is explainable and can be understand visually;\n\n# More resources\n- [Github](https://github.com/xiaoyuxie-vico/ExplainableAI)\n- [Youtube Explanation](https://youtu.be/cIIOdQHTQu4)\n\nNote that this project is my final project for EE475 at Northwestern University in 2020 Fall.\n","source":"_posts/Robust-Image-Classification-with-Kernel-Density-Function.md","raw":"---\ntitle: Robust and Explainable Image Classification Based on Logits Kernel Density Estimation\ndate: 2021-01-05 18:51:00\ncategories: Course project\ntags: \n\t- Class Project\n\t- Deep Learning\n\t- Computer Vision\n---\n\n# Project Description\n\nFor a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.\n\n<!-- more -->\n\nThere are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.\n\n# Dataset\n\nThe cat and dog dataset used in this project is downloaded from [Kaggel](https://www.kaggle.com/tongpython/cat-and-dog). Several images are shown in the below:\n\nThe data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.\n\nThe batch size is 32 and the dataset will be shuﬄed.\n\n# Model Training\n\nDetailed information about model training can be found in [trainer_cat_dog.ipynb](https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb).\n\nThe accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.\n\n# Error analysis (test set)\n\nAlthought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.\n\n![](1.png)\n\nTrue label is dog, but predicted label is cat and the score is high.\n\n![](2.png)\n\nThe Grad-CAM results for these wrong predicted images are:\n\n![](3.png)\n\nWe can ﬁnd that: \n\n1.  Apparent misclassiﬁcations tend to have a larger attention area; \n2. Understandable misclassiﬁcations tend to have a smaller attention area;\n\n# Distribution analysis\n\nTo solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.\n\nBelow, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.\n\n![](4.png)\n\nKernel density estimation for logits is:\n\n![](5.png)\n\n# Analyze the robustness of classiﬁcation using logits kernel density estimation\n\nIt is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.\n\nFor the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.\n\nIf we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not \"seen\" these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.\n\n![](6.png)\n\nIn the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.\n\n![](7.png)\n\n# Conclusion\n- The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; \n- The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; \n- The proposed approach is explainable and can be understand visually;\n\n# More resources\n- [Github](https://github.com/xiaoyuxie-vico/ExplainableAI)\n- [Youtube Explanation](https://youtu.be/cIIOdQHTQu4)\n\nNote that this project is my final project for EE475 at Northwestern University in 2020 Fall.\n","slug":"Robust-Image-Classification-with-Kernel-Density-Function","published":1,"updated":"2021-04-25T05:54:12.303Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknzeeyln0007emzwavll6jqj","content":"<h1 id=\"Project-Description\"><a href=\"#Project-Description\" class=\"headerlink\" title=\"Project Description\"></a>Project Description</h1><p>For a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.</p>\n<a id=\"more\"></a>\n\n<p>There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.</p>\n<h1 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset\"></a>Dataset</h1><p>The cat and dog dataset used in this project is downloaded from <a href=\"https://www.kaggle.com/tongpython/cat-and-dog\">Kaggel</a>. Several images are shown in the below:</p>\n<p>The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.</p>\n<p>The batch size is 32 and the dataset will be shuﬄed.</p>\n<h1 id=\"Model-Training\"><a href=\"#Model-Training\" class=\"headerlink\" title=\"Model Training\"></a>Model Training</h1><p>Detailed information about model training can be found in <a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb\">trainer_cat_dog.ipynb</a>.</p>\n<p>The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.</p>\n<h1 id=\"Error-analysis-test-set\"><a href=\"#Error-analysis-test-set\" class=\"headerlink\" title=\"Error analysis (test set)\"></a>Error analysis (test set)</h1><p>Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.</p>\n<p><img src=\"1.png\"></p>\n<p>True label is dog, but predicted label is cat and the score is high.</p>\n<p><img src=\"2.png\"></p>\n<p>The Grad-CAM results for these wrong predicted images are:</p>\n<p><img src=\"3.png\"></p>\n<p>We can ﬁnd that: </p>\n<ol>\n<li> Apparent misclassiﬁcations tend to have a larger attention area; </li>\n<li>Understandable misclassiﬁcations tend to have a smaller attention area;</li>\n</ol>\n<h1 id=\"Distribution-analysis\"><a href=\"#Distribution-analysis\" class=\"headerlink\" title=\"Distribution analysis\"></a>Distribution analysis</h1><p>To solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.</p>\n<p>Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.</p>\n<p><img src=\"4.png\"></p>\n<p>Kernel density estimation for logits is:</p>\n<p><img src=\"5.png\"></p>\n<h1 id=\"Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\"><a href=\"#Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\" class=\"headerlink\" title=\"Analyze the robustness of classiﬁcation using logits kernel density estimation\"></a>Analyze the robustness of classiﬁcation using logits kernel density estimation</h1><p>It is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.</p>\n<p>For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.</p>\n<p>If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.</p>\n<p><img src=\"6.png\"></p>\n<p>In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.</p>\n<p><img src=\"7.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><ul>\n<li>The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; </li>\n<li>The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; </li>\n<li>The proposed approach is explainable and can be understand visually;</li>\n</ul>\n<h1 id=\"More-resources\"><a href=\"#More-resources\" class=\"headerlink\" title=\"More resources\"></a>More resources</h1><ul>\n<li><a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI\">Github</a></li>\n<li><a href=\"https://youtu.be/cIIOdQHTQu4\">Youtube Explanation</a></li>\n</ul>\n<p>Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.</p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<h1 id=\"Project-Description\"><a href=\"#Project-Description\" class=\"headerlink\" title=\"Project Description\"></a>Project Description</h1><p>For a classiﬁcation problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong inﬂuence on the ﬁnal prediction, especially for these hard samples. So, I used Gradientweighted Class Activation Mapping (Grad-CAM) to explain the results.</p>","more":"<p>There are two contributions in this ﬁnal project. First, I proposed a robust classiﬁcation approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Speciﬁcally, I ﬁnd that apparent misclassiﬁcations tend to have a larger attention area and understandable misclassiﬁcations tend to have a smaller attention are.</p>\n<h1 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset\"></a>Dataset</h1><p>The cat and dog dataset used in this project is downloaded from <a href=\"https://www.kaggle.com/tongpython/cat-and-dog\">Kaggel</a>. Several images are shown in the below:</p>\n<p>The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Aﬃne, Normalization. The test set does not use data augmentation.</p>\n<p>The batch size is 32 and the dataset will be shuﬄed.</p>\n<h1 id=\"Model-Training\"><a href=\"#Model-Training\" class=\"headerlink\" title=\"Model Training\"></a>Model Training</h1><p>Detailed information about model training can be found in <a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI/blob/master/trainer_cat_dog.ipynb\">trainer_cat_dog.ipynb</a>.</p>\n<p>The accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.</p>\n<h1 id=\"Error-analysis-test-set\"><a href=\"#Error-analysis-test-set\" class=\"headerlink\" title=\"Error analysis (test set)\"></a>Error analysis (test set)</h1><p>Althought the accuracy is high (more than 0.98), the trained model still can make some errors. True label is cat, but predicted label is dog and the score is high.</p>\n<p><img src=\"1.png\"></p>\n<p>True label is dog, but predicted label is cat and the score is high.</p>\n<p><img src=\"2.png\"></p>\n<p>The Grad-CAM results for these wrong predicted images are:</p>\n<p><img src=\"3.png\"></p>\n<p>We can ﬁnd that: </p>\n<ol>\n<li> Apparent misclassiﬁcations tend to have a larger attention area; </li>\n<li>Understandable misclassiﬁcations tend to have a smaller attention area;</li>\n</ol>\n<h1 id=\"Distribution-analysis\"><a href=\"#Distribution-analysis\" class=\"headerlink\" title=\"Distribution analysis\"></a>Distribution analysis</h1><p>To solve the problem about apparent misclassiﬁcations and reduce the number of understandable misclassiﬁcations, we want to analyze the distribution of logits and scores for diﬀerent classes.</p>\n<p>Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.</p>\n<p><img src=\"4.png\"></p>\n<p>Kernel density estimation for logits is:</p>\n<p><img src=\"5.png\"></p>\n<h1 id=\"Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\"><a href=\"#Analyze-the-robustness-of-classiﬁcation-using-logits-kernel-density-estimation\" class=\"headerlink\" title=\"Analyze the robustness of classiﬁcation using logits kernel density estimation\"></a>Analyze the robustness of classiﬁcation using logits kernel density estimation</h1><p>It is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can ﬁnd that for the ﬁrst and third image the model give a high score, which means the model make some serious misclassiﬁcations.</p>\n<p>For the fourth and ﬁveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.</p>\n<p>If we see the average of density, we can ﬁnd that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not “seen” these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.</p>\n<p><img src=\"6.png\"></p>\n<p>In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.</p>\n<p><img src=\"7.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><ul>\n<li>The proposed image classiﬁcation approach can identify data out of training set based on logits kernel density estimation; </li>\n<li>The proposed image classiﬁcation approach can help to improve accuracy by identifying data with low density; </li>\n<li>The proposed approach is explainable and can be understand visually;</li>\n</ul>\n<h1 id=\"More-resources\"><a href=\"#More-resources\" class=\"headerlink\" title=\"More resources\"></a>More resources</h1><ul>\n<li><a href=\"https://github.com/xiaoyuxie-vico/ExplainableAI\">Github</a></li>\n<li><a href=\"https://youtu.be/cIIOdQHTQu4\">Youtube Explanation</a></li>\n</ul>\n<p>Note that this project is my final project for EE475 at Northwestern University in 2020 Fall.</p>"},{"title":"Applications of Machine Learning for Fluid Mechanics","date":"2021-02-14T12:50:17.000Z","_content":"\nThis blog is the notes for a video called \"[Machine Learning for Fluid Mechanics](https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s)\", which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. \"Machine learning for fluid mechanics.\" Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.\n\n<!-- more -->\n\n# What is Machine Learning (ML)?\n\nML:\n\n- Models from Data via Optimization\n\n> Any sufficiently advanced technology is indistinguishable from magic.\n>\n> -- Arthur C. Clarke\n\nFluid dynamics tasks:\n\n- Reduction\n- Modeling\n- Control\n- Sensing\n- Closure\n\nOptimization problems:\n\n- High-dimensional\n- Non-linear\n- Non-convex\n- Multiscale\n\n# What kind of ML is needed in science and engineering?\n\nWe need Interpretable and Generalizable Machine Learning in science and engineering field.\n\n> Everything should be made as simple as possible, but not simpler.\n>\n> -- Albert Einstein\n\nHow to build a model like $F=ma$?\n\nFeatures for ML in science and engineering:\n\n- Sparse\n- Low-dimensional\n- Robust\n\n# Schematic: ML + CFD\n\n![Schematic](image-20210214173356481.png)\n\n\n\n# Why ML could work?\n\nBecause patterns exist in fluid flow.\n\n![pattern_FM](image-20210214173840928.png)\n\n# Applications\n\n## Fluid flow decomposition\n\nPCA (Shallow, linear) -> Autoencoder (Deep)\n\n![PCA_FM](image-20210214174003005.png)\n\n![](image-20210214175022879.png)\n\n## Denoise for Fluid Flow\n\n![denoise](image-20210214174108194.png)\n\n## Turbulence modeling\n\nPaper: \n\n- Schlatter, Philipp, et al. \"The structure of a turbulent boundary layer studied by numerical simulation.\" arXiv preprint arXiv:1010.4000 (2010).\n\n- Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. \"Turbulence modeling in the age of data.\" *Annual Review of Fluid Mechanics* 51 (2019): 357-377.\n\n![](image-20210214174310749.png)\n\n![](image-20210214174422276.png)\n\n## ML_CFD solver\n\nPaper:\n\n- Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.\" Journal of Fluid Mechanics 807 (2016): 155-166.\n\nAdd physical constraints and achieve accurate and pyhsical.\n\n## Super-resolution\n\nPaper: \n\n- Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. \"Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.\" arXiv preprint arXiv:1905.10866 (2019).\n\nInterpolation and Extrapolation\n\n![](image-20210214174809740.png)\n\n![](image-20210214174913278.png)\n\n## Solve PDEs\n\n![](image-20210214175304898.png)\n\n# Beyond understanding: control\n\n![](image-20210214182334199.png)\n\n# Inspiration from biology\n\n![](image-20210214182520461.png)\n","source":"_posts/notes-ML-FM.md","raw":"---\ntitle: Applications of Machine Learning for Fluid Mechanics \ndate: 2021-02-14 20:50:17\ncategories: Notes\ntags: \n\t- Machine Learning\t\n\t- Deep Learning\n\t- Fluid Mechanics\n---\n\nThis blog is the notes for a video called \"[Machine Learning for Fluid Mechanics](https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s)\", which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. \"Machine learning for fluid mechanics.\" Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.\n\n<!-- more -->\n\n# What is Machine Learning (ML)?\n\nML:\n\n- Models from Data via Optimization\n\n> Any sufficiently advanced technology is indistinguishable from magic.\n>\n> -- Arthur C. Clarke\n\nFluid dynamics tasks:\n\n- Reduction\n- Modeling\n- Control\n- Sensing\n- Closure\n\nOptimization problems:\n\n- High-dimensional\n- Non-linear\n- Non-convex\n- Multiscale\n\n# What kind of ML is needed in science and engineering?\n\nWe need Interpretable and Generalizable Machine Learning in science and engineering field.\n\n> Everything should be made as simple as possible, but not simpler.\n>\n> -- Albert Einstein\n\nHow to build a model like $F=ma$?\n\nFeatures for ML in science and engineering:\n\n- Sparse\n- Low-dimensional\n- Robust\n\n# Schematic: ML + CFD\n\n![Schematic](image-20210214173356481.png)\n\n\n\n# Why ML could work?\n\nBecause patterns exist in fluid flow.\n\n![pattern_FM](image-20210214173840928.png)\n\n# Applications\n\n## Fluid flow decomposition\n\nPCA (Shallow, linear) -> Autoencoder (Deep)\n\n![PCA_FM](image-20210214174003005.png)\n\n![](image-20210214175022879.png)\n\n## Denoise for Fluid Flow\n\n![denoise](image-20210214174108194.png)\n\n## Turbulence modeling\n\nPaper: \n\n- Schlatter, Philipp, et al. \"The structure of a turbulent boundary layer studied by numerical simulation.\" arXiv preprint arXiv:1010.4000 (2010).\n\n- Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. \"Turbulence modeling in the age of data.\" *Annual Review of Fluid Mechanics* 51 (2019): 357-377.\n\n![](image-20210214174310749.png)\n\n![](image-20210214174422276.png)\n\n## ML_CFD solver\n\nPaper:\n\n- Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. \"Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.\" Journal of Fluid Mechanics 807 (2016): 155-166.\n\nAdd physical constraints and achieve accurate and pyhsical.\n\n## Super-resolution\n\nPaper: \n\n- Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. \"Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.\" arXiv preprint arXiv:1905.10866 (2019).\n\nInterpolation and Extrapolation\n\n![](image-20210214174809740.png)\n\n![](image-20210214174913278.png)\n\n## Solve PDEs\n\n![](image-20210214175304898.png)\n\n# Beyond understanding: control\n\n![](image-20210214182334199.png)\n\n# Inspiration from biology\n\n![](image-20210214182520461.png)\n","slug":"notes-ML-FM","published":1,"updated":"2021-02-17T13:13:26.433Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknzeeym0000zemzw9hzzdocn","content":"<p>This blog is the notes for a video called “<a href=\"https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s\">Machine Learning for Fluid Mechanics</a>“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.</p>\n<a id=\"more\"></a>\n\n<h1 id=\"What-is-Machine-Learning-ML\"><a href=\"#What-is-Machine-Learning-ML\" class=\"headerlink\" title=\"What is Machine Learning (ML)?\"></a>What is Machine Learning (ML)?</h1><p>ML:</p>\n<ul>\n<li>Models from Data via Optimization</li>\n</ul>\n<blockquote>\n<p>Any sufficiently advanced technology is indistinguishable from magic.</p>\n<p>– Arthur C. Clarke</p>\n</blockquote>\n<p>Fluid dynamics tasks:</p>\n<ul>\n<li>Reduction</li>\n<li>Modeling</li>\n<li>Control</li>\n<li>Sensing</li>\n<li>Closure</li>\n</ul>\n<p>Optimization problems:</p>\n<ul>\n<li>High-dimensional</li>\n<li>Non-linear</li>\n<li>Non-convex</li>\n<li>Multiscale</li>\n</ul>\n<h1 id=\"What-kind-of-ML-is-needed-in-science-and-engineering\"><a href=\"#What-kind-of-ML-is-needed-in-science-and-engineering\" class=\"headerlink\" title=\"What kind of ML is needed in science and engineering?\"></a>What kind of ML is needed in science and engineering?</h1><p>We need Interpretable and Generalizable Machine Learning in science and engineering field.</p>\n<blockquote>\n<p>Everything should be made as simple as possible, but not simpler.</p>\n<p>– Albert Einstein</p>\n</blockquote>\n<p>How to build a model like $F=ma$?</p>\n<p>Features for ML in science and engineering:</p>\n<ul>\n<li>Sparse</li>\n<li>Low-dimensional</li>\n<li>Robust</li>\n</ul>\n<h1 id=\"Schematic-ML-CFD\"><a href=\"#Schematic-ML-CFD\" class=\"headerlink\" title=\"Schematic: ML + CFD\"></a>Schematic: ML + CFD</h1><p><img src=\"image-20210214173356481.png\" alt=\"Schematic\"></p>\n<h1 id=\"Why-ML-could-work\"><a href=\"#Why-ML-could-work\" class=\"headerlink\" title=\"Why ML could work?\"></a>Why ML could work?</h1><p>Because patterns exist in fluid flow.</p>\n<p><img src=\"image-20210214173840928.png\" alt=\"pattern_FM\"></p>\n<h1 id=\"Applications\"><a href=\"#Applications\" class=\"headerlink\" title=\"Applications\"></a>Applications</h1><h2 id=\"Fluid-flow-decomposition\"><a href=\"#Fluid-flow-decomposition\" class=\"headerlink\" title=\"Fluid flow decomposition\"></a>Fluid flow decomposition</h2><p>PCA (Shallow, linear) -&gt; Autoencoder (Deep)</p>\n<p><img src=\"image-20210214174003005.png\" alt=\"PCA_FM\"></p>\n<p><img src=\"image-20210214175022879.png\"></p>\n<h2 id=\"Denoise-for-Fluid-Flow\"><a href=\"#Denoise-for-Fluid-Flow\" class=\"headerlink\" title=\"Denoise for Fluid Flow\"></a>Denoise for Fluid Flow</h2><p><img src=\"image-20210214174108194.png\" alt=\"denoise\"></p>\n<h2 id=\"Turbulence-modeling\"><a href=\"#Turbulence-modeling\" class=\"headerlink\" title=\"Turbulence modeling\"></a>Turbulence modeling</h2><p>Paper: </p>\n<ul>\n<li><p>Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010).</p>\n</li>\n<li><p>Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” <em>Annual Review of Fluid Mechanics</em> 51 (2019): 357-377.</p>\n</li>\n</ul>\n<p><img src=\"image-20210214174310749.png\"></p>\n<p><img src=\"image-20210214174422276.png\"></p>\n<h2 id=\"ML-CFD-solver\"><a href=\"#ML-CFD-solver\" class=\"headerlink\" title=\"ML_CFD solver\"></a>ML_CFD solver</h2><p>Paper:</p>\n<ul>\n<li>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166.</li>\n</ul>\n<p>Add physical constraints and achieve accurate and pyhsical.</p>\n<h2 id=\"Super-resolution\"><a href=\"#Super-resolution\" class=\"headerlink\" title=\"Super-resolution\"></a>Super-resolution</h2><p>Paper: </p>\n<ul>\n<li>Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019).</li>\n</ul>\n<p>Interpolation and Extrapolation</p>\n<p><img src=\"image-20210214174809740.png\"></p>\n<p><img src=\"image-20210214174913278.png\"></p>\n<h2 id=\"Solve-PDEs\"><a href=\"#Solve-PDEs\" class=\"headerlink\" title=\"Solve PDEs\"></a>Solve PDEs</h2><p><img src=\"image-20210214175304898.png\"></p>\n<h1 id=\"Beyond-understanding-control\"><a href=\"#Beyond-understanding-control\" class=\"headerlink\" title=\"Beyond understanding: control\"></a>Beyond understanding: control</h1><p><img src=\"image-20210214182334199.png\"></p>\n<h1 id=\"Inspiration-from-biology\"><a href=\"#Inspiration-from-biology\" class=\"headerlink\" title=\"Inspiration from biology\"></a>Inspiration from biology</h1><p><img src=\"image-20210214182520461.png\"></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<p>This blog is the notes for a video called “<a href=\"https://www.youtube.com/watch?v=8e3OT2K99Kw&t=84s\">Machine Learning for Fluid Mechanics</a>“, which is a brief introduction for a paper (Brunton, Steven L., Bernd R. Noack, and Petros Koumoutsakos. “Machine learning for fluid mechanics.” Annual Review of Fluid Mechanics 52 (2020): 477-508.). If you want to know the details, please find the original video and paper.</p>","more":"<h1 id=\"What-is-Machine-Learning-ML\"><a href=\"#What-is-Machine-Learning-ML\" class=\"headerlink\" title=\"What is Machine Learning (ML)?\"></a>What is Machine Learning (ML)?</h1><p>ML:</p>\n<ul>\n<li>Models from Data via Optimization</li>\n</ul>\n<blockquote>\n<p>Any sufficiently advanced technology is indistinguishable from magic.</p>\n<p>– Arthur C. Clarke</p>\n</blockquote>\n<p>Fluid dynamics tasks:</p>\n<ul>\n<li>Reduction</li>\n<li>Modeling</li>\n<li>Control</li>\n<li>Sensing</li>\n<li>Closure</li>\n</ul>\n<p>Optimization problems:</p>\n<ul>\n<li>High-dimensional</li>\n<li>Non-linear</li>\n<li>Non-convex</li>\n<li>Multiscale</li>\n</ul>\n<h1 id=\"What-kind-of-ML-is-needed-in-science-and-engineering\"><a href=\"#What-kind-of-ML-is-needed-in-science-and-engineering\" class=\"headerlink\" title=\"What kind of ML is needed in science and engineering?\"></a>What kind of ML is needed in science and engineering?</h1><p>We need Interpretable and Generalizable Machine Learning in science and engineering field.</p>\n<blockquote>\n<p>Everything should be made as simple as possible, but not simpler.</p>\n<p>– Albert Einstein</p>\n</blockquote>\n<p>How to build a model like $F=ma$?</p>\n<p>Features for ML in science and engineering:</p>\n<ul>\n<li>Sparse</li>\n<li>Low-dimensional</li>\n<li>Robust</li>\n</ul>\n<h1 id=\"Schematic-ML-CFD\"><a href=\"#Schematic-ML-CFD\" class=\"headerlink\" title=\"Schematic: ML + CFD\"></a>Schematic: ML + CFD</h1><p><img src=\"image-20210214173356481.png\" alt=\"Schematic\"></p>\n<h1 id=\"Why-ML-could-work\"><a href=\"#Why-ML-could-work\" class=\"headerlink\" title=\"Why ML could work?\"></a>Why ML could work?</h1><p>Because patterns exist in fluid flow.</p>\n<p><img src=\"image-20210214173840928.png\" alt=\"pattern_FM\"></p>\n<h1 id=\"Applications\"><a href=\"#Applications\" class=\"headerlink\" title=\"Applications\"></a>Applications</h1><h2 id=\"Fluid-flow-decomposition\"><a href=\"#Fluid-flow-decomposition\" class=\"headerlink\" title=\"Fluid flow decomposition\"></a>Fluid flow decomposition</h2><p>PCA (Shallow, linear) -&gt; Autoencoder (Deep)</p>\n<p><img src=\"image-20210214174003005.png\" alt=\"PCA_FM\"></p>\n<p><img src=\"image-20210214175022879.png\"></p>\n<h2 id=\"Denoise-for-Fluid-Flow\"><a href=\"#Denoise-for-Fluid-Flow\" class=\"headerlink\" title=\"Denoise for Fluid Flow\"></a>Denoise for Fluid Flow</h2><p><img src=\"image-20210214174108194.png\" alt=\"denoise\"></p>\n<h2 id=\"Turbulence-modeling\"><a href=\"#Turbulence-modeling\" class=\"headerlink\" title=\"Turbulence modeling\"></a>Turbulence modeling</h2><p>Paper: </p>\n<ul>\n<li><p>Schlatter, Philipp, et al. “The structure of a turbulent boundary layer studied by numerical simulation.” arXiv preprint arXiv:1010.4000 (2010).</p>\n</li>\n<li><p>Duraisamy, Karthik, Gianluca Iaccarino, and Heng Xiao. “Turbulence modeling in the age of data.” <em>Annual Review of Fluid Mechanics</em> 51 (2019): 357-377.</p>\n</li>\n</ul>\n<p><img src=\"image-20210214174310749.png\"></p>\n<p><img src=\"image-20210214174422276.png\"></p>\n<h2 id=\"ML-CFD-solver\"><a href=\"#ML-CFD-solver\" class=\"headerlink\" title=\"ML_CFD solver\"></a>ML_CFD solver</h2><p>Paper:</p>\n<ul>\n<li>Ling, Julia, Andrew Kurzawski, and Jeremy Templeton. “Reynolds averaged turbulence modelling using deep neural networks with embedded invariance.” Journal of Fluid Mechanics 807 (2016): 155-166.</li>\n</ul>\n<p>Add physical constraints and achieve accurate and pyhsical.</p>\n<h2 id=\"Super-resolution\"><a href=\"#Super-resolution\" class=\"headerlink\" title=\"Super-resolution\"></a>Super-resolution</h2><p>Paper: </p>\n<ul>\n<li>Erichson, N. Benjamin, Michael Muehlebach, and Michael W. Mahoney. “Physics-informed autoencoders for Lyapunov-stable fluid flow prediction.” arXiv preprint arXiv:1905.10866 (2019).</li>\n</ul>\n<p>Interpolation and Extrapolation</p>\n<p><img src=\"image-20210214174809740.png\"></p>\n<p><img src=\"image-20210214174913278.png\"></p>\n<h2 id=\"Solve-PDEs\"><a href=\"#Solve-PDEs\" class=\"headerlink\" title=\"Solve PDEs\"></a>Solve PDEs</h2><p><img src=\"image-20210214175304898.png\"></p>\n<h1 id=\"Beyond-understanding-control\"><a href=\"#Beyond-understanding-control\" class=\"headerlink\" title=\"Beyond understanding: control\"></a>Beyond understanding: control</h1><p><img src=\"image-20210214182334199.png\"></p>\n<h1 id=\"Inspiration-from-biology\"><a href=\"#Inspiration-from-biology\" class=\"headerlink\" title=\"Inspiration from biology\"></a>Inspiration from biology</h1><p><img src=\"image-20210214182520461.png\"></p>"},{"title":"GANs specialization Week1: Notes and codes","date":"2021-04-26T04:30:40.000Z","_content":"\nThis is week 1 notes for the first course ([Build Basic Generative Adversarial Networks (GANs)](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans))  in the [GANs specialization](https://www.coursera.org/specializations/generative-adversarial-networks-gans). I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality.  I really like DeepLearning.AI's courses. The lectures and assignments are always well-designed.\n\n<!-- more -->\n\n# Notes\n\n- Generator needs more time steps to train\n\n  >  This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes.\n\n- Noise vector 𝑧\n\n  >  The noise vector 𝑧 has the important role of making sure the images generated from the same class 𝑦 don't all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote 𝑧 ~ 𝑁(0,1). The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1.\n\n  > In reality, 𝑧 is usually larger than just 1 value to allow for more combinations of what 𝑧 could be. There's no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it's large enough to contain a lot of possibilities. As a result, you would sample 𝑧 from that many different dimensions (constituting multiple normal distributions).\n\n  > Fun Fact: this is also called a spherical normal and denoted 𝑧 ~ 𝑁(0,𝐼) where the 𝐼 represents the identity matrix and means the variance is 1 in all dimensions.*\n\n- Truncation trick\n\n  > So now that you're a bit familiar with noise vectors, here's another cool concept that people use to tune their outputs. It's called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector 𝑧, you can choose to keep that random 𝑧 or you can sample another one.\n\n  > Why would you want to sample another one?\n\n  > Well, since I'm sampling 𝑧 from a normal distribution, my model will see more of those 𝑧 values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it's likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it's not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images).\n\n  ![image-20210426120534698](image-20210426120534698.png)\n\n  > Image Credit: Modelica\n\n  > What the truncation trick does is resamples the noise vector 𝑧 until it falls within some bounds of the normal distribution. In fact, it samples 𝑧 from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that's not very interesting or successful from a model that's supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability).\n\n  > **truncation**: The positive truncation value. 1 is low truncation (high diversity) and 0 is all truncation except for the mean (high quality/fidelity). A lower value increases fidelity and decreases diversity, and vice versa.\n\n  ![image-20210426120448703](image-20210426120448703.png)\n\n- Playing with code\n\n  interpolation\n\n  ```python\n  z_dim = Gs.input_shape[1]\n  first_noise = rnd.randn(1, z_dim)\n  second_noise = rnd.randn(1, z_dim)\n  percent_first_noise = np.linspace(0, 1, n_interpolation)[:, None]\n  interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise)\n  ```\n\n- Random vector\n\n  > Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible.\n\n- HW\n\n  > Since the generator is needed when calculating the discriminator's loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!\n\n# Week 1 Assignment: Your First GAN\n\n### Goal\n\nIn this notebook, you're going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you're not familiar with this framework, you may find the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) useful. The hints will also often include links to relevant documentation.\n\n### Learning Objectives\n\n1.   Build the generator and discriminator components of a GAN from scratch.\n2.   Create generator and discriminator loss functions.\n3.   Train your GAN and visualize the generated images.\n\n\n## Getting Started\n\nYou will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST # Training dataset\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\ntorch.manual_seed(0) # Set for testing purposes, please do not change!\n\ndef show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n    '''\n    Function for visualizing images: Given a tensor of images, number of images, and\n    size per image, plots and prints the images in a uniform grid.\n    '''\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n```\n\n#### MNIST Dataset\n\nThe training images your discriminator will be using is from a dataset called [MNIST](http://yann.lecun.com/exdb/mnist/). It contains 60,000 images of handwritten digits, from 0 to 9, like these:\n\nYou may notice that the images are quite pixelated -- this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or \"color channel\", is needed to represent them (more on this later in the course).\n\n#### Tensor\n\nYou will represent the data using [tensors](https://pytorch.org/docs/stable/tensors.html). Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.\n\nTensors are easy to manipulate and supported by [PyTorch](https://pytorch.org/), the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!\n\n#### Batches\n\nWhile you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.\n\nThis means that your generator will generate an entire batch of images and receive the discriminator's feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.\n\n## Generator\n\nThe first step is to build the generator component.\n\nYou will start by creating a function to make a single layer/block for the generator's neural network. Each block should include a [linear transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to map to another shape, a [batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) for stabilization, and finally a non-linear activation function (you use a [ReLU here](https://pytorch.org/docs/master/generated/torch.nn.ReLU.html)) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.\n\n\n```python\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_generator_block\ndef get_generator_block(input_dim, output_dim):\n    '''\n    Function for returning a block of the generator's neural network\n    given input and output dimensions.\n    Parameters:\n        input_dim: the dimension of the input vector, a scalar\n        output_dim: the dimension of the output vector, a scalar\n    Returns:\n        a generator neural network layer, with a linear transformation \n          followed by a batch normalization and then a relu activation\n    '''\n    return nn.Sequential(\n        # Hint: Replace all of the \"None\" with the appropriate dimensions.\n        # The documentation may be useful if you're less familiar with PyTorch:\n        # https://pytorch.org/docs/stable/nn.html.\n        #### START CODE HERE ####\n        nn.Linear(input_dim, output_dim),\n        nn.BatchNorm1d(output_dim),\n        nn.ReLU(inplace=True),\n        #### END CODE HERE ####\n    )\n```\n\n\n```python\n# Verify the generator block function\ndef test_gen_block(in_features, out_features, num_test=1000):\n    block = get_generator_block(in_features, out_features)\n\n    # Check the three parts\n    assert len(block) == 3\n    assert type(block[0]) == nn.Linear\n    assert type(block[1]) == nn.BatchNorm1d\n    assert type(block[2]) == nn.ReLU\n    \n    # Check the output shape\n    test_input = torch.randn(num_test, in_features)\n    test_output = block(test_input)\n    assert tuple(test_output.shape) == (num_test, out_features)\n    assert test_output.std() > 0.55\n    assert test_output.std() < 0.65\n\ntest_gen_block(25, 12)\ntest_gen_block(15, 28)\nprint(\"Success!\")\n```\n\n    Success!\n\n\nNow you can build the generator class. It will take 3 values:\n\n*   The noise vector dimension\n*   The image dimension\n*   The initial hidden dimension\n\nUsing these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a [sigmoid function](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html). \n\nFinally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.\n\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hints for <code><font size=\"4\">Generator</font></code></b>\n</font>\n</summary>\n\n1. The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.\n2. [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) and [nn.Sigmoid](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html) will be useful here. \n   </details>\n\n\n\n```python\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Generator\nclass Generator(nn.Module):\n    '''\n    Generator Class\n    Values:\n        z_dim: the dimension of the noise vector, a scalar\n        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n          (MNIST images are 28 x 28 = 784 so that is your default)\n        hidden_dim: the inner dimension, a scalar\n    '''\n    def __init__(self, z_dim=10, im_dim=784, hidden_dim=128):\n        super(Generator, self).__init__()\n        # Build the neural network\n        self.gen = nn.Sequential(\n            get_generator_block(z_dim, hidden_dim),\n            get_generator_block(hidden_dim, hidden_dim * 2),\n            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n            # There is a dropdown with hints if you need them! \n            #### START CODE HERE ####\n            nn.Linear(hidden_dim * 8, im_dim),\n            nn.Sigmoid()\n            #### END CODE HERE ####\n        )\n    def forward(self, noise):\n        '''\n        Function for completing a forward pass of the generator: Given a noise tensor, \n        returns generated images.\n        Parameters:\n            noise: a noise tensor with dimensions (n_samples, z_dim)\n        '''\n        return self.gen(noise)\n    \n    # Needed for grading\n    def get_gen(self):\n        '''\n        Returns:\n            the sequential model\n        '''\n        return self.gen\n```\n\n\n```python\n# Verify the generator class\ndef test_generator(z_dim, im_dim, hidden_dim, num_test=10000):\n    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()\n    \n    # Check there are six modules in the sequential part\n    assert len(gen) == 6\n    test_input = torch.randn(num_test, z_dim)\n    test_output = gen(test_input)\n\n    # Check that the output shape is correct\n    assert tuple(test_output.shape) == (num_test, im_dim)\n    assert test_output.max() < 1, \"Make sure to use a sigmoid\"\n    assert test_output.min() > 0, \"Make sure to use a sigmoid\"\n    assert test_output.min() < 0.5, \"Don't use a block in your solution\"\n    assert test_output.std() > 0.05, \"Don't use batchnorm here\"\n    assert test_output.std() < 0.15, \"Don't use batchnorm here\"\n\ntest_generator(5, 10, 20)\ntest_generator(20, 8, 24)\nprint(\"Success!\")\n```\n\n    Success!\n\n\n## Noise\n\nTo be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don't all look the same -- think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.\n\nNote that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. `torch.ones(3, 3, device=device)`, or move it onto the target device using `torch.ones(3, 3).to(device)`. You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as `torch.ones_like`. In general, use `torch.ones_like` and `torch.zeros_like` instead of `torch.ones` or `torch.zeros` where possible.\n\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hint for <code><font size=\"4\">get_noise</font></code></b>\n</font>\n</summary>\n\n1. You will probably find [torch.randn](https://pytorch.org/docs/master/generated/torch.randn.html) useful here.\n   </details>\n\n\n```python\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_noise\ndef get_noise(n_samples, z_dim, device='cpu'):\n    '''\n    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n    creates a tensor of that shape filled with random numbers from the normal distribution.\n    Parameters:\n        n_samples: the number of samples to generate, a scalar\n        z_dim: the dimension of the noise vector, a scalar\n        device: the device type\n    '''\n    # NOTE: To use this on GPU with device='cuda', make sure to pass the device \n    # argument to the function you use to generate the noise.\n    #### START CODE HERE ####\n    return torch.randn(n_samples, z_dim, device=device)\n    #### END CODE HERE ####\n```\n\n\n```python\n# Verify the noise vector function\ndef test_get_noise(n_samples, z_dim, device='cpu'):\n    noise = get_noise(n_samples, z_dim, device)\n    \n    # Make sure a normal distribution was used\n    assert tuple(noise.shape) == (n_samples, z_dim)\n    assert torch.abs(noise.std() - torch.tensor(1.0)) < 0.01\n    assert str(noise.device).startswith(device)\n\ntest_get_noise(1000, 100, 'cpu')\nif torch.cuda.is_available():\n    test_get_noise(1000, 32, 'cuda')\nprint(\"Success!\")\n```\n\n    Success!\n\n\n## Discriminator\n\nThe second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.\n\n*Note: You use leaky ReLUs to prevent the \"dying ReLU\" problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!* \n\n\n```python\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_discriminator_block\ndef get_discriminator_block(input_dim, output_dim):\n    '''\n    Discriminator Block\n    Function for returning a neural network of the discriminator given input and output dimensions.\n    Parameters:\n        input_dim: the dimension of the input vector, a scalar\n        output_dim: the dimension of the output vector, a scalar\n    Returns:\n        a discriminator neural network layer, with a linear transformation \n          followed by an nn.LeakyReLU activation with negative slope of 0.2 \n          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)\n    '''\n    return nn.Sequential(\n        #### START CODE HERE ####\n        nn.Linear(input_dim, output_dim),\n        nn.LeakyReLU(negative_slope=0.2)\n        #### END CODE HERE ####\n    )\n```\n\n\n```python\n# Verify the discriminator block function\ndef test_disc_block(in_features, out_features, num_test=10000):\n    block = get_discriminator_block(in_features, out_features)\n\n    # Check there are two parts\n    assert len(block) == 2\n    test_input = torch.randn(num_test, in_features)\n    test_output = block(test_input)\n\n    # Check that the shape is right\n    assert tuple(test_output.shape) == (num_test, out_features)\n    \n    # Check that the LeakyReLU slope is about 0.2\n    assert -test_output.min() / test_output.max() > 0.1\n    assert -test_output.min() / test_output.max() < 0.3\n    assert test_output.std() > 0.3\n    assert test_output.std() < 0.5\n\ntest_disc_block(25, 12)\ntest_disc_block(15, 28)\nprint(\"Success!\")\n```\n\n    Success!\n\n\nNow you can use these blocks to make a discriminator! The discriminator class holds 2 values:\n\n*   The image dimension\n*   The hidden dimension\n\nThe discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator's neural network you are given a forward pass function that takes in an image tensor to be classified.\n\n\n\n```python\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Discriminator\nclass Discriminator(nn.Module):\n    '''\n    Discriminator Class\n    Values:\n        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n            (MNIST images are 28x28 = 784 so that is your default)\n        hidden_dim: the inner dimension, a scalar\n    '''\n    def __init__(self, im_dim=784, hidden_dim=128):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            get_discriminator_block(im_dim, hidden_dim * 4),\n            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n            get_discriminator_block(hidden_dim * 2, hidden_dim),\n            # Hint: You want to transform the final output into a single value,\n            #       so add one more linear map.\n            #### START CODE HERE ####\n            nn.Linear(hidden_dim, 1)\n            #### END CODE HERE ####\n        )\n\n    def forward(self, image):\n        '''\n        Function for completing a forward pass of the discriminator: Given an image tensor, \n        returns a 1-dimension tensor representing fake/real.\n        Parameters:\n            image: a flattened image tensor with dimension (im_dim)\n        '''\n        return self.disc(image)\n    \n    # Needed for grading\n    def get_disc(self):\n        '''\n        Returns:\n            the sequential model\n        '''\n        return self.disc\n```\n\n\n```python\n# Verify the discriminator class\ndef test_discriminator(z_dim, hidden_dim, num_test=100):\n    \n    disc = Discriminator(z_dim, hidden_dim).get_disc()\n\n    # Check there are three parts\n    assert len(disc) == 4\n\n    # Check the linear layer is correct\n    test_input = torch.randn(num_test, z_dim)\n    test_output = disc(test_input)\n    assert tuple(test_output.shape) == (num_test, 1)\n    \n    # Don't use a block\n    assert not isinstance(disc[-1], nn.Sequential)\n\ntest_discriminator(5, 10)\ntest_discriminator(20, 8)\nprint(\"Success!\")\n```\n\n    Success!\n\n\n## Training\n\nNow you can put it all together!\nFirst, you will set your parameters:\n\n  *   criterion: the loss function\n  *   n_epochs: the number of times you iterate through the entire dataset when training\n  *   z_dim: the dimension of the noise vector\n  *   display_step: how often to display/visualize the images\n  *   batch_size: the number of images per forward/backward pass\n  *   lr: the learning rate\n  *   device: the device type, here using a GPU (which runs CUDA), not CPU\n\nNext, you will load the MNIST dataset as tensors using a dataloader.\n\n\n\n\n```python\n# Set your parameters\n\"\"\"\nNotes for nn.BCEWithLogitsLoss():\nThis loss combines a Sigmoid layer and the BCELoss in one single class. \nThis version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, \nby combining the operations into one layer, \nwe take advantage of the log-sum-exp trick for numerical stability.\n\"\"\"\ncriterion = nn.BCEWithLogitsLoss()\nn_epochs = 200\nz_dim = 64\ndisplay_step = 500\nbatch_size = 128\nlr = 0.00001\n\n# Load MNIST dataset as tensors\ndataloader = DataLoader(\n    MNIST('.', download=False, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n\n### DO NOT EDIT ###\ndevice = 'cuda'\n```\n\nNow, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.\n\n\n```python\ngen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n```\n\nBefore you train your GAN, you will need to create functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves. **Since the generator is needed when calculating the discriminator's loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!**\n\nRemember that you have already defined a loss function earlier (`criterion`) and you are encouraged to use `torch.ones_like` and `torch.zeros_like` instead of `torch.ones` or `torch.zeros`. If you use `torch.ones` or `torch.zeros`, you'll need to pass `device=device` to them.\n\n\n```python\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_disc_loss\ndef get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n    '''\n    Return the loss of the discriminator given inputs.\n    Parameters:\n        gen: the generator model, which returns an image given z-dimensional noise\n        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n        criterion: the loss function, which should be used to compare \n               the discriminator's predictions to the ground truth reality of the images \n               (e.g. fake = 0, real = 1)\n        real: a batch of real images\n        num_images: the number of images the generator should produce, \n                which is also the length of the real images\n        z_dim: the dimension of the noise vector, a scalar\n        device: the device type\n    Returns:\n        disc_loss: a torch scalar loss value for the current batch\n    '''\n    #     These are the steps you will need to complete:\n    #       1) Create noise vectors and generate a batch (num_images) of fake images. \n    #            Make sure to pass the device argument to the noise.\n    #       2) Get the discriminator's prediction of the fake image \n    #            and calculate the loss. Don't forget to detach the generator!\n    #            (Remember the loss function you set earlier -- criterion. You need a \n    #            'ground truth' tensor in order to calculate the loss. \n    #            For example, a ground truth tensor for a fake image is all zeros.)\n    #       3) Get the discriminator's prediction of the real image and calculate the loss.\n    #       4) Calculate the discriminator's loss by averaging the real and fake loss\n    #            and set it to disc_loss.\n    #     Note: Please do not use concatenation in your solution. The tests are being updated to \n    #           support this, but for now, average the two losses as described in step (4).\n    #     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!\n    #### START CODE HERE ####\n    # step 1\n    noise = get_noise(num_images, z_dim, device)\n    fake_imgs = gen(noise)\n    \n    # step 2\n    disc_output_fake = disc(fake_imgs.detach())\n    ground_truth_fake = torch.zeros_like(disc_output_fake)\n    loss_fake = criterion(disc_output_fake, ground_truth_fake)\n    \n    # step 3\n    disc_output_real = disc(real)\n    ground_truth_real = torch.ones_like(disc_output_real)\n    loss_real = criterion(disc_output_real, ground_truth_real)\n    \n    # 3tep 4\n    disc_loss = (loss_fake + loss_real) / 2.0\n    \n    #### END CODE HERE ####\n    return disc_loss\n```\n\n\n```python\ndef test_disc_reasonable(num_images=10):\n    # Don't use explicit casts to cuda - use the device argument\n    import inspect, re\n    lines = inspect.getsource(get_disc_loss)\n    assert (re.search(r\"to\\(.cuda.\\)\", lines)) is None\n    assert (re.search(r\"\\.cuda\\(\\)\", lines)) is None\n    \n    z_dim = 64\n    gen = torch.zeros_like\n    disc = lambda x: x.mean(1)[:, None]\n    criterion = torch.mul # Multiply\n    real = torch.ones(num_images, z_dim)\n    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')\n    assert torch.all(torch.abs(disc_loss.mean() - 0.5) < 1e-5)\n    \n    gen = torch.ones_like\n    criterion = torch.mul # Multiply\n    real = torch.zeros(num_images, z_dim)\n    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')) < 1e-5)\n    \n    gen = lambda x: torch.ones(num_images, 10)\n    disc = lambda x: x.mean(1)[:, None] + 10\n    criterion = torch.mul # Multiply\n    real = torch.zeros(num_images, 10)\n    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean() - 5) < 1e-5)\n\n    gen = torch.ones_like\n    disc = nn.Linear(64, 1, bias=False)\n    real = torch.ones(num_images, 64) * 0.5\n    disc.weight.data = torch.ones_like(disc.weight.data) * 0.5\n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    criterion = lambda x, y: torch.sum(x) + torch.sum(y)\n    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean()\n    disc_loss.backward()\n    assert torch.isclose(torch.abs(disc.weight.grad.mean() - 11.25), torch.tensor(3.75))\n    \ndef test_disc_loss(max_tests = 10):\n    z_dim = 64\n    gen = Generator(z_dim).to(device)\n    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n    disc = Discriminator().to(device) \n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    num_steps = 0\n    for real, _ in dataloader:\n        cur_batch_size = len(real)\n        real = real.view(cur_batch_size, -1).to(device)\n\n        ### Update discriminator ###\n        # Zero out the gradient before backpropagation\n        disc_opt.zero_grad()\n\n        # Calculate discriminator loss\n        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n        assert (disc_loss - 0.68).abs() < 0.05\n\n        # Update gradients\n        disc_loss.backward(retain_graph=True)\n\n        # Check that they detached correctly\n        assert gen.gen[0][0].weight.grad is None\n\n        # Update optimizer\n        old_weight = disc.disc[0][0].weight.data.clone()\n        disc_opt.step()\n        new_weight = disc.disc[0][0].weight.data\n        \n        # Check that some discriminator weights changed\n        assert not torch.all(torch.eq(old_weight, new_weight))\n        num_steps += 1\n        if num_steps >= max_tests:\n            break\n\ntest_disc_reasonable()\ntest_disc_loss()\nprint(\"Success!\")\n```\n\n    Success!\n\n\n\n```python\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_gen_loss\ndef get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n    '''\n    Return the loss of the generator given inputs.\n    Parameters:\n        gen: the generator model, which returns an image given z-dimensional noise\n        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n        criterion: the loss function, which should be used to compare \n               the discriminator's predictions to the ground truth reality of the images \n               (e.g. fake = 0, real = 1)\n        num_images: the number of images the generator should produce, \n                which is also the length of the real images\n        z_dim: the dimension of the noise vector, a scalar\n        device: the device type\n    Returns:\n        gen_loss: a torch scalar loss value for the current batch\n    '''\n    #     These are the steps you will need to complete:\n    #       1) Create noise vectors and generate a batch of fake images. \n    #           Remember to pass the device argument to the get_noise function.\n    #       2) Get the discriminator's prediction of the fake image.\n    #       3) Calculate the generator's loss. Remember the generator wants\n    #          the discriminator to think that its fake images are real\n    #     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!\n\n    #### START CODE HERE ####\n    # step 1\n    noise = get_noise(num_images, z_dim, device)\n    gen_output_fake = gen(noise)\n    \n    # step 2\n    disc_output_fake = disc(gen_output_fake)\n    \n    # step 3\n    ground_truth_fake = torch.ones_like(disc_output_fake)\n    gen_loss = criterion(disc_output_fake, ground_truth_fake)\n    \n    #### END CODE HERE ####\n    return gen_loss\n```\n\n\n```python\ndef test_gen_reasonable(num_images=10):\n    # Don't use explicit casts to cuda - use the device argument\n    import inspect, re\n    lines = inspect.getsource(get_gen_loss)\n    assert (re.search(r\"to\\(.cuda.\\)\", lines)) is None\n    assert (re.search(r\"\\.cuda\\(\\)\", lines)) is None\n    \n    z_dim = 64\n    gen = torch.zeros_like\n    disc = nn.Identity()\n    criterion = torch.mul # Multiply\n    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n    assert torch.all(torch.abs(gen_loss_tensor) < 1e-5)\n    #Verify shape. Related to gen_noise parametrization\n    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n\n    gen = torch.ones_like\n    disc = nn.Identity()\n    criterion = torch.mul # Multiply\n    real = torch.zeros(num_images, 1)\n    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n    assert torch.all(torch.abs(gen_loss_tensor - 1) < 1e-5)\n    #Verify shape. Related to gen_noise parametrization\n    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n    \n\ndef test_gen_loss(num_images):\n    z_dim = 64\n    gen = Generator(z_dim).to(device)\n    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n    disc = Discriminator().to(device) \n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    \n    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)\n    \n    # Check that the loss is reasonable\n    assert (gen_loss - 0.7).abs() < 0.1\n    gen_loss.backward()\n    old_weight = gen.gen[0][0].weight.clone()\n    gen_opt.step()\n    new_weight = gen.gen[0][0].weight\n    assert not torch.all(torch.eq(old_weight, new_weight))\n\n\ntest_gen_reasonable(10)\ntest_gen_loss(18)\nprint(\"Success!\")\n```\n\n    Success!\n\n\nFinally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. \n\nIt’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.\n\nAfter you've submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.\n\n<!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: \n\n1.   Download the .ipynb\n2.   Upload it to Google Drive and open it with Google Colab\n3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)\n4.   Replace `device = \"cpu\"` with `device = \"cuda\"`\n5.   Make sure your `get_noise` function uses the right device -->\n\nBut remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.\n\nYou should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:\n\n\n```python\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: \n\ncur_step = 0\nmean_generator_loss = 0\nmean_discriminator_loss = 0\ntest_generator = True # Whether the generator should be tested\ngen_loss = False\nerror = False\nfor epoch in range(n_epochs):\n  \n    # Dataloader returns the batches\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n\n        # Flatten the batch of real images from the dataset\n        real = real.view(cur_batch_size, -1).to(device)\n\n        ### Update discriminator ###\n        # Zero out the gradients before backpropagation\n        disc_opt.zero_grad()\n\n        # Calculate discriminator loss\n        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n\n        # Update gradients\n        disc_loss.backward(retain_graph=True)\n\n        # Update optimizer\n        disc_opt.step()\n\n        # For testing purposes, to keep track of the generator weights\n        if test_generator:\n            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n\n        ### Update generator ###\n        #     Hint: This code will look a lot like the discriminator updates!\n        #     These are the steps you will need to complete:\n        #       1) Zero out the gradients.\n        #       2) Calculate the generator loss, assigning it to gen_loss.\n        #       3) Backprop through the generator: update the gradients and optimizer.\n        #### START CODE HERE ####\n        # step 1\n        gen_opt.zero_grad()\n        \n        # step 2\n        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n        \n        # step 3\n        gen_loss.backward(retain_graph=True)\n        gen_opt.step()\n        #### END CODE HERE ####\n\n        # For testing purposes, to check that your code changes the generator weights\n        if test_generator:\n            try:\n                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n            except:\n                error = True\n                print(\"Runtime tests have failed\")\n\n        # Keep track of the average discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Keep track of the average generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ### Visualization code ###\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n            fake = gen(fake_noise)\n            show_tensor_images(fake)\n            show_tensor_images(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1\n\n```\n\n\n    HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))\n\n    HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))\n\n\n    Epoch 1, step 500: Generator loss: 1.537445880770684, discriminator loss: 0.4010176688432697\n\n\n\n![png](output_31_4.png)\n\n\n\n![png](output_31_5.png)\n\n **Notes that there are only two epoch's results.**\n\n    HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))\n\n\n    Epoch 199, step 93500: Generator loss: 1.095844237446786, discriminator loss: 0.5419520480632782\n\n\n\n![png](output_31_958.png)\n\n\n\n![png](output_31_959.png)","source":"_posts/GANs-specialization-Week1-Notes-and-codes.md","raw":"---\ntitle: 'GANs specialization Week1: Notes and codes'\ndate: 2021-04-26 12:30:40\ntags:\n\t- Online course\n\t- GANs\n\t- Notes\ncategories: Class notes\n---\n\nThis is week 1 notes for the first course ([Build Basic Generative Adversarial Networks (GANs)](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans))  in the [GANs specialization](https://www.coursera.org/specializations/generative-adversarial-networks-gans). I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality.  I really like DeepLearning.AI's courses. The lectures and assignments are always well-designed.\n\n<!-- more -->\n\n# Notes\n\n- Generator needs more time steps to train\n\n  >  This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes.\n\n- Noise vector 𝑧\n\n  >  The noise vector 𝑧 has the important role of making sure the images generated from the same class 𝑦 don't all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote 𝑧 ~ 𝑁(0,1). The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1.\n\n  > In reality, 𝑧 is usually larger than just 1 value to allow for more combinations of what 𝑧 could be. There's no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it's large enough to contain a lot of possibilities. As a result, you would sample 𝑧 from that many different dimensions (constituting multiple normal distributions).\n\n  > Fun Fact: this is also called a spherical normal and denoted 𝑧 ~ 𝑁(0,𝐼) where the 𝐼 represents the identity matrix and means the variance is 1 in all dimensions.*\n\n- Truncation trick\n\n  > So now that you're a bit familiar with noise vectors, here's another cool concept that people use to tune their outputs. It's called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector 𝑧, you can choose to keep that random 𝑧 or you can sample another one.\n\n  > Why would you want to sample another one?\n\n  > Well, since I'm sampling 𝑧 from a normal distribution, my model will see more of those 𝑧 values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it's likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it's not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images).\n\n  ![image-20210426120534698](image-20210426120534698.png)\n\n  > Image Credit: Modelica\n\n  > What the truncation trick does is resamples the noise vector 𝑧 until it falls within some bounds of the normal distribution. In fact, it samples 𝑧 from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that's not very interesting or successful from a model that's supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability).\n\n  > **truncation**: The positive truncation value. 1 is low truncation (high diversity) and 0 is all truncation except for the mean (high quality/fidelity). A lower value increases fidelity and decreases diversity, and vice versa.\n\n  ![image-20210426120448703](image-20210426120448703.png)\n\n- Playing with code\n\n  interpolation\n\n  ```python\n  z_dim = Gs.input_shape[1]\n  first_noise = rnd.randn(1, z_dim)\n  second_noise = rnd.randn(1, z_dim)\n  percent_first_noise = np.linspace(0, 1, n_interpolation)[:, None]\n  interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise)\n  ```\n\n- Random vector\n\n  > Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible.\n\n- HW\n\n  > Since the generator is needed when calculating the discriminator's loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!\n\n# Week 1 Assignment: Your First GAN\n\n### Goal\n\nIn this notebook, you're going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you're not familiar with this framework, you may find the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) useful. The hints will also often include links to relevant documentation.\n\n### Learning Objectives\n\n1.   Build the generator and discriminator components of a GAN from scratch.\n2.   Create generator and discriminator loss functions.\n3.   Train your GAN and visualize the generated images.\n\n\n## Getting Started\n\nYou will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.\n\n\n\n```python\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST # Training dataset\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\ntorch.manual_seed(0) # Set for testing purposes, please do not change!\n\ndef show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n    '''\n    Function for visualizing images: Given a tensor of images, number of images, and\n    size per image, plots and prints the images in a uniform grid.\n    '''\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n```\n\n#### MNIST Dataset\n\nThe training images your discriminator will be using is from a dataset called [MNIST](http://yann.lecun.com/exdb/mnist/). It contains 60,000 images of handwritten digits, from 0 to 9, like these:\n\nYou may notice that the images are quite pixelated -- this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or \"color channel\", is needed to represent them (more on this later in the course).\n\n#### Tensor\n\nYou will represent the data using [tensors](https://pytorch.org/docs/stable/tensors.html). Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.\n\nTensors are easy to manipulate and supported by [PyTorch](https://pytorch.org/), the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!\n\n#### Batches\n\nWhile you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.\n\nThis means that your generator will generate an entire batch of images and receive the discriminator's feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.\n\n## Generator\n\nThe first step is to build the generator component.\n\nYou will start by creating a function to make a single layer/block for the generator's neural network. Each block should include a [linear transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to map to another shape, a [batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) for stabilization, and finally a non-linear activation function (you use a [ReLU here](https://pytorch.org/docs/master/generated/torch.nn.ReLU.html)) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.\n\n\n```python\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_generator_block\ndef get_generator_block(input_dim, output_dim):\n    '''\n    Function for returning a block of the generator's neural network\n    given input and output dimensions.\n    Parameters:\n        input_dim: the dimension of the input vector, a scalar\n        output_dim: the dimension of the output vector, a scalar\n    Returns:\n        a generator neural network layer, with a linear transformation \n          followed by a batch normalization and then a relu activation\n    '''\n    return nn.Sequential(\n        # Hint: Replace all of the \"None\" with the appropriate dimensions.\n        # The documentation may be useful if you're less familiar with PyTorch:\n        # https://pytorch.org/docs/stable/nn.html.\n        #### START CODE HERE ####\n        nn.Linear(input_dim, output_dim),\n        nn.BatchNorm1d(output_dim),\n        nn.ReLU(inplace=True),\n        #### END CODE HERE ####\n    )\n```\n\n\n```python\n# Verify the generator block function\ndef test_gen_block(in_features, out_features, num_test=1000):\n    block = get_generator_block(in_features, out_features)\n\n    # Check the three parts\n    assert len(block) == 3\n    assert type(block[0]) == nn.Linear\n    assert type(block[1]) == nn.BatchNorm1d\n    assert type(block[2]) == nn.ReLU\n    \n    # Check the output shape\n    test_input = torch.randn(num_test, in_features)\n    test_output = block(test_input)\n    assert tuple(test_output.shape) == (num_test, out_features)\n    assert test_output.std() > 0.55\n    assert test_output.std() < 0.65\n\ntest_gen_block(25, 12)\ntest_gen_block(15, 28)\nprint(\"Success!\")\n```\n\n    Success!\n\n\nNow you can build the generator class. It will take 3 values:\n\n*   The noise vector dimension\n*   The image dimension\n*   The initial hidden dimension\n\nUsing these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a [sigmoid function](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html). \n\nFinally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.\n\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hints for <code><font size=\"4\">Generator</font></code></b>\n</font>\n</summary>\n\n1. The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.\n2. [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) and [nn.Sigmoid](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html) will be useful here. \n   </details>\n\n\n\n```python\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Generator\nclass Generator(nn.Module):\n    '''\n    Generator Class\n    Values:\n        z_dim: the dimension of the noise vector, a scalar\n        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n          (MNIST images are 28 x 28 = 784 so that is your default)\n        hidden_dim: the inner dimension, a scalar\n    '''\n    def __init__(self, z_dim=10, im_dim=784, hidden_dim=128):\n        super(Generator, self).__init__()\n        # Build the neural network\n        self.gen = nn.Sequential(\n            get_generator_block(z_dim, hidden_dim),\n            get_generator_block(hidden_dim, hidden_dim * 2),\n            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n            # There is a dropdown with hints if you need them! \n            #### START CODE HERE ####\n            nn.Linear(hidden_dim * 8, im_dim),\n            nn.Sigmoid()\n            #### END CODE HERE ####\n        )\n    def forward(self, noise):\n        '''\n        Function for completing a forward pass of the generator: Given a noise tensor, \n        returns generated images.\n        Parameters:\n            noise: a noise tensor with dimensions (n_samples, z_dim)\n        '''\n        return self.gen(noise)\n    \n    # Needed for grading\n    def get_gen(self):\n        '''\n        Returns:\n            the sequential model\n        '''\n        return self.gen\n```\n\n\n```python\n# Verify the generator class\ndef test_generator(z_dim, im_dim, hidden_dim, num_test=10000):\n    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()\n    \n    # Check there are six modules in the sequential part\n    assert len(gen) == 6\n    test_input = torch.randn(num_test, z_dim)\n    test_output = gen(test_input)\n\n    # Check that the output shape is correct\n    assert tuple(test_output.shape) == (num_test, im_dim)\n    assert test_output.max() < 1, \"Make sure to use a sigmoid\"\n    assert test_output.min() > 0, \"Make sure to use a sigmoid\"\n    assert test_output.min() < 0.5, \"Don't use a block in your solution\"\n    assert test_output.std() > 0.05, \"Don't use batchnorm here\"\n    assert test_output.std() < 0.15, \"Don't use batchnorm here\"\n\ntest_generator(5, 10, 20)\ntest_generator(20, 8, 24)\nprint(\"Success!\")\n```\n\n    Success!\n\n\n## Noise\n\nTo be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don't all look the same -- think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.\n\nNote that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. `torch.ones(3, 3, device=device)`, or move it onto the target device using `torch.ones(3, 3).to(device)`. You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as `torch.ones_like`. In general, use `torch.ones_like` and `torch.zeros_like` instead of `torch.ones` or `torch.zeros` where possible.\n\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hint for <code><font size=\"4\">get_noise</font></code></b>\n</font>\n</summary>\n\n1. You will probably find [torch.randn](https://pytorch.org/docs/master/generated/torch.randn.html) useful here.\n   </details>\n\n\n```python\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_noise\ndef get_noise(n_samples, z_dim, device='cpu'):\n    '''\n    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n    creates a tensor of that shape filled with random numbers from the normal distribution.\n    Parameters:\n        n_samples: the number of samples to generate, a scalar\n        z_dim: the dimension of the noise vector, a scalar\n        device: the device type\n    '''\n    # NOTE: To use this on GPU with device='cuda', make sure to pass the device \n    # argument to the function you use to generate the noise.\n    #### START CODE HERE ####\n    return torch.randn(n_samples, z_dim, device=device)\n    #### END CODE HERE ####\n```\n\n\n```python\n# Verify the noise vector function\ndef test_get_noise(n_samples, z_dim, device='cpu'):\n    noise = get_noise(n_samples, z_dim, device)\n    \n    # Make sure a normal distribution was used\n    assert tuple(noise.shape) == (n_samples, z_dim)\n    assert torch.abs(noise.std() - torch.tensor(1.0)) < 0.01\n    assert str(noise.device).startswith(device)\n\ntest_get_noise(1000, 100, 'cpu')\nif torch.cuda.is_available():\n    test_get_noise(1000, 32, 'cuda')\nprint(\"Success!\")\n```\n\n    Success!\n\n\n## Discriminator\n\nThe second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.\n\n*Note: You use leaky ReLUs to prevent the \"dying ReLU\" problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!* \n\n\n```python\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_discriminator_block\ndef get_discriminator_block(input_dim, output_dim):\n    '''\n    Discriminator Block\n    Function for returning a neural network of the discriminator given input and output dimensions.\n    Parameters:\n        input_dim: the dimension of the input vector, a scalar\n        output_dim: the dimension of the output vector, a scalar\n    Returns:\n        a discriminator neural network layer, with a linear transformation \n          followed by an nn.LeakyReLU activation with negative slope of 0.2 \n          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)\n    '''\n    return nn.Sequential(\n        #### START CODE HERE ####\n        nn.Linear(input_dim, output_dim),\n        nn.LeakyReLU(negative_slope=0.2)\n        #### END CODE HERE ####\n    )\n```\n\n\n```python\n# Verify the discriminator block function\ndef test_disc_block(in_features, out_features, num_test=10000):\n    block = get_discriminator_block(in_features, out_features)\n\n    # Check there are two parts\n    assert len(block) == 2\n    test_input = torch.randn(num_test, in_features)\n    test_output = block(test_input)\n\n    # Check that the shape is right\n    assert tuple(test_output.shape) == (num_test, out_features)\n    \n    # Check that the LeakyReLU slope is about 0.2\n    assert -test_output.min() / test_output.max() > 0.1\n    assert -test_output.min() / test_output.max() < 0.3\n    assert test_output.std() > 0.3\n    assert test_output.std() < 0.5\n\ntest_disc_block(25, 12)\ntest_disc_block(15, 28)\nprint(\"Success!\")\n```\n\n    Success!\n\n\nNow you can use these blocks to make a discriminator! The discriminator class holds 2 values:\n\n*   The image dimension\n*   The hidden dimension\n\nThe discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator's neural network you are given a forward pass function that takes in an image tensor to be classified.\n\n\n\n```python\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Discriminator\nclass Discriminator(nn.Module):\n    '''\n    Discriminator Class\n    Values:\n        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n            (MNIST images are 28x28 = 784 so that is your default)\n        hidden_dim: the inner dimension, a scalar\n    '''\n    def __init__(self, im_dim=784, hidden_dim=128):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            get_discriminator_block(im_dim, hidden_dim * 4),\n            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n            get_discriminator_block(hidden_dim * 2, hidden_dim),\n            # Hint: You want to transform the final output into a single value,\n            #       so add one more linear map.\n            #### START CODE HERE ####\n            nn.Linear(hidden_dim, 1)\n            #### END CODE HERE ####\n        )\n\n    def forward(self, image):\n        '''\n        Function for completing a forward pass of the discriminator: Given an image tensor, \n        returns a 1-dimension tensor representing fake/real.\n        Parameters:\n            image: a flattened image tensor with dimension (im_dim)\n        '''\n        return self.disc(image)\n    \n    # Needed for grading\n    def get_disc(self):\n        '''\n        Returns:\n            the sequential model\n        '''\n        return self.disc\n```\n\n\n```python\n# Verify the discriminator class\ndef test_discriminator(z_dim, hidden_dim, num_test=100):\n    \n    disc = Discriminator(z_dim, hidden_dim).get_disc()\n\n    # Check there are three parts\n    assert len(disc) == 4\n\n    # Check the linear layer is correct\n    test_input = torch.randn(num_test, z_dim)\n    test_output = disc(test_input)\n    assert tuple(test_output.shape) == (num_test, 1)\n    \n    # Don't use a block\n    assert not isinstance(disc[-1], nn.Sequential)\n\ntest_discriminator(5, 10)\ntest_discriminator(20, 8)\nprint(\"Success!\")\n```\n\n    Success!\n\n\n## Training\n\nNow you can put it all together!\nFirst, you will set your parameters:\n\n  *   criterion: the loss function\n  *   n_epochs: the number of times you iterate through the entire dataset when training\n  *   z_dim: the dimension of the noise vector\n  *   display_step: how often to display/visualize the images\n  *   batch_size: the number of images per forward/backward pass\n  *   lr: the learning rate\n  *   device: the device type, here using a GPU (which runs CUDA), not CPU\n\nNext, you will load the MNIST dataset as tensors using a dataloader.\n\n\n\n\n```python\n# Set your parameters\n\"\"\"\nNotes for nn.BCEWithLogitsLoss():\nThis loss combines a Sigmoid layer and the BCELoss in one single class. \nThis version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, \nby combining the operations into one layer, \nwe take advantage of the log-sum-exp trick for numerical stability.\n\"\"\"\ncriterion = nn.BCEWithLogitsLoss()\nn_epochs = 200\nz_dim = 64\ndisplay_step = 500\nbatch_size = 128\nlr = 0.00001\n\n# Load MNIST dataset as tensors\ndataloader = DataLoader(\n    MNIST('.', download=False, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n\n### DO NOT EDIT ###\ndevice = 'cuda'\n```\n\nNow, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.\n\n\n```python\ngen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n```\n\nBefore you train your GAN, you will need to create functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves. **Since the generator is needed when calculating the discriminator's loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!**\n\nRemember that you have already defined a loss function earlier (`criterion`) and you are encouraged to use `torch.ones_like` and `torch.zeros_like` instead of `torch.ones` or `torch.zeros`. If you use `torch.ones` or `torch.zeros`, you'll need to pass `device=device` to them.\n\n\n```python\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_disc_loss\ndef get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n    '''\n    Return the loss of the discriminator given inputs.\n    Parameters:\n        gen: the generator model, which returns an image given z-dimensional noise\n        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n        criterion: the loss function, which should be used to compare \n               the discriminator's predictions to the ground truth reality of the images \n               (e.g. fake = 0, real = 1)\n        real: a batch of real images\n        num_images: the number of images the generator should produce, \n                which is also the length of the real images\n        z_dim: the dimension of the noise vector, a scalar\n        device: the device type\n    Returns:\n        disc_loss: a torch scalar loss value for the current batch\n    '''\n    #     These are the steps you will need to complete:\n    #       1) Create noise vectors and generate a batch (num_images) of fake images. \n    #            Make sure to pass the device argument to the noise.\n    #       2) Get the discriminator's prediction of the fake image \n    #            and calculate the loss. Don't forget to detach the generator!\n    #            (Remember the loss function you set earlier -- criterion. You need a \n    #            'ground truth' tensor in order to calculate the loss. \n    #            For example, a ground truth tensor for a fake image is all zeros.)\n    #       3) Get the discriminator's prediction of the real image and calculate the loss.\n    #       4) Calculate the discriminator's loss by averaging the real and fake loss\n    #            and set it to disc_loss.\n    #     Note: Please do not use concatenation in your solution. The tests are being updated to \n    #           support this, but for now, average the two losses as described in step (4).\n    #     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!\n    #### START CODE HERE ####\n    # step 1\n    noise = get_noise(num_images, z_dim, device)\n    fake_imgs = gen(noise)\n    \n    # step 2\n    disc_output_fake = disc(fake_imgs.detach())\n    ground_truth_fake = torch.zeros_like(disc_output_fake)\n    loss_fake = criterion(disc_output_fake, ground_truth_fake)\n    \n    # step 3\n    disc_output_real = disc(real)\n    ground_truth_real = torch.ones_like(disc_output_real)\n    loss_real = criterion(disc_output_real, ground_truth_real)\n    \n    # 3tep 4\n    disc_loss = (loss_fake + loss_real) / 2.0\n    \n    #### END CODE HERE ####\n    return disc_loss\n```\n\n\n```python\ndef test_disc_reasonable(num_images=10):\n    # Don't use explicit casts to cuda - use the device argument\n    import inspect, re\n    lines = inspect.getsource(get_disc_loss)\n    assert (re.search(r\"to\\(.cuda.\\)\", lines)) is None\n    assert (re.search(r\"\\.cuda\\(\\)\", lines)) is None\n    \n    z_dim = 64\n    gen = torch.zeros_like\n    disc = lambda x: x.mean(1)[:, None]\n    criterion = torch.mul # Multiply\n    real = torch.ones(num_images, z_dim)\n    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')\n    assert torch.all(torch.abs(disc_loss.mean() - 0.5) < 1e-5)\n    \n    gen = torch.ones_like\n    criterion = torch.mul # Multiply\n    real = torch.zeros(num_images, z_dim)\n    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')) < 1e-5)\n    \n    gen = lambda x: torch.ones(num_images, 10)\n    disc = lambda x: x.mean(1)[:, None] + 10\n    criterion = torch.mul # Multiply\n    real = torch.zeros(num_images, 10)\n    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean() - 5) < 1e-5)\n\n    gen = torch.ones_like\n    disc = nn.Linear(64, 1, bias=False)\n    real = torch.ones(num_images, 64) * 0.5\n    disc.weight.data = torch.ones_like(disc.weight.data) * 0.5\n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    criterion = lambda x, y: torch.sum(x) + torch.sum(y)\n    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean()\n    disc_loss.backward()\n    assert torch.isclose(torch.abs(disc.weight.grad.mean() - 11.25), torch.tensor(3.75))\n    \ndef test_disc_loss(max_tests = 10):\n    z_dim = 64\n    gen = Generator(z_dim).to(device)\n    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n    disc = Discriminator().to(device) \n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    num_steps = 0\n    for real, _ in dataloader:\n        cur_batch_size = len(real)\n        real = real.view(cur_batch_size, -1).to(device)\n\n        ### Update discriminator ###\n        # Zero out the gradient before backpropagation\n        disc_opt.zero_grad()\n\n        # Calculate discriminator loss\n        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n        assert (disc_loss - 0.68).abs() < 0.05\n\n        # Update gradients\n        disc_loss.backward(retain_graph=True)\n\n        # Check that they detached correctly\n        assert gen.gen[0][0].weight.grad is None\n\n        # Update optimizer\n        old_weight = disc.disc[0][0].weight.data.clone()\n        disc_opt.step()\n        new_weight = disc.disc[0][0].weight.data\n        \n        # Check that some discriminator weights changed\n        assert not torch.all(torch.eq(old_weight, new_weight))\n        num_steps += 1\n        if num_steps >= max_tests:\n            break\n\ntest_disc_reasonable()\ntest_disc_loss()\nprint(\"Success!\")\n```\n\n    Success!\n\n\n\n```python\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_gen_loss\ndef get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n    '''\n    Return the loss of the generator given inputs.\n    Parameters:\n        gen: the generator model, which returns an image given z-dimensional noise\n        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n        criterion: the loss function, which should be used to compare \n               the discriminator's predictions to the ground truth reality of the images \n               (e.g. fake = 0, real = 1)\n        num_images: the number of images the generator should produce, \n                which is also the length of the real images\n        z_dim: the dimension of the noise vector, a scalar\n        device: the device type\n    Returns:\n        gen_loss: a torch scalar loss value for the current batch\n    '''\n    #     These are the steps you will need to complete:\n    #       1) Create noise vectors and generate a batch of fake images. \n    #           Remember to pass the device argument to the get_noise function.\n    #       2) Get the discriminator's prediction of the fake image.\n    #       3) Calculate the generator's loss. Remember the generator wants\n    #          the discriminator to think that its fake images are real\n    #     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!\n\n    #### START CODE HERE ####\n    # step 1\n    noise = get_noise(num_images, z_dim, device)\n    gen_output_fake = gen(noise)\n    \n    # step 2\n    disc_output_fake = disc(gen_output_fake)\n    \n    # step 3\n    ground_truth_fake = torch.ones_like(disc_output_fake)\n    gen_loss = criterion(disc_output_fake, ground_truth_fake)\n    \n    #### END CODE HERE ####\n    return gen_loss\n```\n\n\n```python\ndef test_gen_reasonable(num_images=10):\n    # Don't use explicit casts to cuda - use the device argument\n    import inspect, re\n    lines = inspect.getsource(get_gen_loss)\n    assert (re.search(r\"to\\(.cuda.\\)\", lines)) is None\n    assert (re.search(r\"\\.cuda\\(\\)\", lines)) is None\n    \n    z_dim = 64\n    gen = torch.zeros_like\n    disc = nn.Identity()\n    criterion = torch.mul # Multiply\n    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n    assert torch.all(torch.abs(gen_loss_tensor) < 1e-5)\n    #Verify shape. Related to gen_noise parametrization\n    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n\n    gen = torch.ones_like\n    disc = nn.Identity()\n    criterion = torch.mul # Multiply\n    real = torch.zeros(num_images, 1)\n    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n    assert torch.all(torch.abs(gen_loss_tensor - 1) < 1e-5)\n    #Verify shape. Related to gen_noise parametrization\n    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n    \n\ndef test_gen_loss(num_images):\n    z_dim = 64\n    gen = Generator(z_dim).to(device)\n    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n    disc = Discriminator().to(device) \n    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n    \n    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)\n    \n    # Check that the loss is reasonable\n    assert (gen_loss - 0.7).abs() < 0.1\n    gen_loss.backward()\n    old_weight = gen.gen[0][0].weight.clone()\n    gen_opt.step()\n    new_weight = gen.gen[0][0].weight\n    assert not torch.all(torch.eq(old_weight, new_weight))\n\n\ntest_gen_reasonable(10)\ntest_gen_loss(18)\nprint(\"Success!\")\n```\n\n    Success!\n\n\nFinally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. \n\nIt’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.\n\nAfter you've submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.\n\n<!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: \n\n1.   Download the .ipynb\n2.   Upload it to Google Drive and open it with Google Colab\n3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)\n4.   Replace `device = \"cpu\"` with `device = \"cuda\"`\n5.   Make sure your `get_noise` function uses the right device -->\n\nBut remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.\n\nYou should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:\n\n\n```python\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: \n\ncur_step = 0\nmean_generator_loss = 0\nmean_discriminator_loss = 0\ntest_generator = True # Whether the generator should be tested\ngen_loss = False\nerror = False\nfor epoch in range(n_epochs):\n  \n    # Dataloader returns the batches\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n\n        # Flatten the batch of real images from the dataset\n        real = real.view(cur_batch_size, -1).to(device)\n\n        ### Update discriminator ###\n        # Zero out the gradients before backpropagation\n        disc_opt.zero_grad()\n\n        # Calculate discriminator loss\n        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n\n        # Update gradients\n        disc_loss.backward(retain_graph=True)\n\n        # Update optimizer\n        disc_opt.step()\n\n        # For testing purposes, to keep track of the generator weights\n        if test_generator:\n            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n\n        ### Update generator ###\n        #     Hint: This code will look a lot like the discriminator updates!\n        #     These are the steps you will need to complete:\n        #       1) Zero out the gradients.\n        #       2) Calculate the generator loss, assigning it to gen_loss.\n        #       3) Backprop through the generator: update the gradients and optimizer.\n        #### START CODE HERE ####\n        # step 1\n        gen_opt.zero_grad()\n        \n        # step 2\n        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n        \n        # step 3\n        gen_loss.backward(retain_graph=True)\n        gen_opt.step()\n        #### END CODE HERE ####\n\n        # For testing purposes, to check that your code changes the generator weights\n        if test_generator:\n            try:\n                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n            except:\n                error = True\n                print(\"Runtime tests have failed\")\n\n        # Keep track of the average discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Keep track of the average generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ### Visualization code ###\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n            fake = gen(fake_noise)\n            show_tensor_images(fake)\n            show_tensor_images(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1\n\n```\n\n\n    HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))\n\n    HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))\n\n\n    Epoch 1, step 500: Generator loss: 1.537445880770684, discriminator loss: 0.4010176688432697\n\n\n\n![png](output_31_4.png)\n\n\n\n![png](output_31_5.png)\n\n **Notes that there are only two epoch's results.**\n\n    HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))\n\n\n    Epoch 199, step 93500: Generator loss: 1.095844237446786, discriminator loss: 0.5419520480632782\n\n\n\n![png](output_31_958.png)\n\n\n\n![png](output_31_959.png)","slug":"GANs-specialization-Week1-Notes-and-codes","published":1,"updated":"2021-04-26T05:35:05.065Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknzeeym40014emzwa34u08b4","content":"<p>This is week 1 notes for the first course (<a href=\"https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans\">Build Basic Generative Adversarial Networks (GANs)</a>)  in the <a href=\"https://www.coursera.org/specializations/generative-adversarial-networks-gans\">GANs specialization</a>. I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality.  I really like DeepLearning.AI’s courses. The lectures and assignments are always well-designed.</p>\n<a id=\"more\"></a>\n\n<h1 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h1><ul>\n<li><p>Generator needs more time steps to train</p>\n<blockquote>\n<p> This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes.</p>\n</blockquote>\n</li>\n<li><p>Noise vector 𝑧</p>\n<blockquote>\n<p> The noise vector 𝑧 has the important role of making sure the images generated from the same class 𝑦 don’t all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote 𝑧 ~ 𝑁(0,1). The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1.</p>\n</blockquote>\n<blockquote>\n<p>In reality, 𝑧 is usually larger than just 1 value to allow for more combinations of what 𝑧 could be. There’s no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it’s large enough to contain a lot of possibilities. As a result, you would sample 𝑧 from that many different dimensions (constituting multiple normal distributions).</p>\n</blockquote>\n<blockquote>\n<p>Fun Fact: this is also called a spherical normal and denoted 𝑧 ~ 𝑁(0,𝐼) where the 𝐼 represents the identity matrix and means the variance is 1 in all dimensions.*</p>\n</blockquote>\n</li>\n<li><p>Truncation trick</p>\n<blockquote>\n<p>So now that you’re a bit familiar with noise vectors, here’s another cool concept that people use to tune their outputs. It’s called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector 𝑧, you can choose to keep that random 𝑧 or you can sample another one.</p>\n</blockquote>\n<blockquote>\n<p>Why would you want to sample another one?</p>\n</blockquote>\n<blockquote>\n<p>Well, since I’m sampling 𝑧 from a normal distribution, my model will see more of those 𝑧 values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it’s likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it’s not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images).</p>\n</blockquote>\n<p><img src=\"image-20210426120534698.png\" alt=\"image-20210426120534698\"></p>\n<blockquote>\n<p>Image Credit: Modelica</p>\n</blockquote>\n<blockquote>\n<p>What the truncation trick does is resamples the noise vector 𝑧 until it falls within some bounds of the normal distribution. In fact, it samples 𝑧 from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that’s not very interesting or successful from a model that’s supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability).</p>\n</blockquote>\n<blockquote>\n<p><strong>truncation</strong>: The positive truncation value. 1 is low truncation (high diversity) and 0 is all truncation except for the mean (high quality/fidelity). A lower value increases fidelity and decreases diversity, and vice versa.</p>\n</blockquote>\n<p><img src=\"image-20210426120448703.png\" alt=\"image-20210426120448703\"></p>\n</li>\n<li><p>Playing with code</p>\n<p>interpolation</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z_dim = Gs.input_shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">first_noise = rnd.randn(<span class=\"number\">1</span>, z_dim)</span><br><span class=\"line\">second_noise = rnd.randn(<span class=\"number\">1</span>, z_dim)</span><br><span class=\"line\">percent_first_noise = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">1</span>, n_interpolation)[:, <span class=\"literal\">None</span>]</span><br><span class=\"line\">interpolation_noise = first_noise * percent_first_noise + second_noise * (<span class=\"number\">1</span> - percent_first_noise)</span><br></pre></td></tr></table></figure></li>\n<li><p>Random vector</p>\n<blockquote>\n<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible.</p>\n</blockquote>\n</li>\n<li><p>HW</p>\n<blockquote>\n<p>Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"Week-1-Assignment-Your-First-GAN\"><a href=\"#Week-1-Assignment-Your-First-GAN\" class=\"headerlink\" title=\"Week 1 Assignment: Your First GAN\"></a>Week 1 Assignment: Your First GAN</h1><h3 id=\"Goal\"><a href=\"#Goal\" class=\"headerlink\" title=\"Goal\"></a>Goal</h3><p>In this notebook, you’re going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you’re not familiar with this framework, you may find the <a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch documentation</a> useful. The hints will also often include links to relevant documentation.</p>\n<h3 id=\"Learning-Objectives\"><a href=\"#Learning-Objectives\" class=\"headerlink\" title=\"Learning Objectives\"></a>Learning Objectives</h3><ol>\n<li>  Build the generator and discriminator components of a GAN from scratch.</li>\n<li>  Create generator and discriminator loss functions.</li>\n<li>  Train your GAN and visualize the generated images.</li>\n</ol>\n<h2 id=\"Getting-Started\"><a href=\"#Getting-Started\" class=\"headerlink\" title=\"Getting Started\"></a>Getting Started</h2><p>You will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.auto <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision.datasets <span class=\"keyword\">import</span> MNIST <span class=\"comment\"># Training dataset</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision.utils <span class=\"keyword\">import</span> make_grid</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>) <span class=\"comment\"># Set for testing purposes, please do not change!</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show_tensor_images</span>(<span class=\"params\">image_tensor, num_images=<span class=\"number\">25</span>, size=(<span class=\"params\"><span class=\"number\">1</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span></span>)</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class=\"line\"><span class=\"string\">    size per image, plots and prints the images in a uniform grid.</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    image_unflat = image_tensor.detach().cpu().view(-<span class=\"number\">1</span>, *size)</span><br><span class=\"line\">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class=\"number\">5</span>)</span><br><span class=\"line\">    plt.imshow(image_grid.permute(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>).squeeze())</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure>\n<h4 id=\"MNIST-Dataset\"><a href=\"#MNIST-Dataset\" class=\"headerlink\" title=\"MNIST Dataset\"></a>MNIST Dataset</h4><p>The training images your discriminator will be using is from a dataset called <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a>. It contains 60,000 images of handwritten digits, from 0 to 9, like these:</p>\n<p>You may notice that the images are quite pixelated – this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or “color channel”, is needed to represent them (more on this later in the course).</p>\n<h4 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h4><p>You will represent the data using <a href=\"https://pytorch.org/docs/stable/tensors.html\">tensors</a>. Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.</p>\n<p>Tensors are easy to manipulate and supported by <a href=\"https://pytorch.org/\">PyTorch</a>, the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!</p>\n<h4 id=\"Batches\"><a href=\"#Batches\" class=\"headerlink\" title=\"Batches\"></a>Batches</h4><p>While you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.</p>\n<p>This means that your generator will generate an entire batch of images and receive the discriminator’s feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.</p>\n<h2 id=\"Generator\"><a href=\"#Generator\" class=\"headerlink\" title=\"Generator\"></a>Generator</h2><p>The first step is to build the generator component.</p>\n<p>You will start by creating a function to make a single layer/block for the generator’s neural network. Each block should include a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\">linear transformation</a> to map to another shape, a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\">batch normalization</a> for stabilization, and finally a non-linear activation function (you use a <a href=\"https://pytorch.org/docs/master/generated/torch.nn.ReLU.html\">ReLU here</a>) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_generator_block</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_generator_block</span>(<span class=\"params\">input_dim, output_dim</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Function for returning a block of the generator&#x27;s neural network</span></span><br><span class=\"line\"><span class=\"string\">    given input and output dimensions.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        a generator neural network layer, with a linear transformation </span></span><br><span class=\"line\"><span class=\"string\">          followed by a batch normalization and then a relu activation</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        <span class=\"comment\"># Hint: Replace all of the &quot;None&quot; with the appropriate dimensions.</span></span><br><span class=\"line\">        <span class=\"comment\"># The documentation may be useful if you&#x27;re less familiar with PyTorch:</span></span><br><span class=\"line\">        <span class=\"comment\"># https://pytorch.org/docs/stable/nn.html.</span></span><br><span class=\"line\">        <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">        nn.Linear(input_dim, output_dim),</span><br><span class=\"line\">        nn.BatchNorm1d(output_dim),</span><br><span class=\"line\">        nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">        <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the generator block function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_gen_block</span>(<span class=\"params\">in_features, out_features, num_test=<span class=\"number\">1000</span></span>):</span></span><br><span class=\"line\">    block = get_generator_block(in_features, out_features)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check the three parts</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(block) == <span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">type</span>(block[<span class=\"number\">0</span>]) == nn.Linear</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">type</span>(block[<span class=\"number\">1</span>]) == nn.BatchNorm1d</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">type</span>(block[<span class=\"number\">2</span>]) == nn.ReLU</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check the output shape</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, in_features)</span><br><span class=\"line\">    test_output = block(test_input)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, out_features)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &gt; <span class=\"number\">0.55</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &lt; <span class=\"number\">0.65</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_gen_block(<span class=\"number\">25</span>, <span class=\"number\">12</span>)</span><br><span class=\"line\">test_gen_block(<span class=\"number\">15</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<p>Now you can build the generator class. It will take 3 values:</p>\n<ul>\n<li>  The noise vector dimension</li>\n<li>  The image dimension</li>\n<li>  The initial hidden dimension</li>\n</ul>\n<p>Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html\">sigmoid function</a>. </p>\n<p>Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.</p>\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hints for <code><font size=\"4\">Generator</font></code></b>\n</font>\n</summary>\n\n<ol>\n<li>The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.</li>\n<li><a href=\"https://pytorch.org/docs/master/generated/torch.nn.Linear.html\">nn.Linear</a> and <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html\">nn.Sigmoid</a> will be useful here. </details>\n\n\n\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: Generator</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Generator Class</span></span><br><span class=\"line\"><span class=\"string\">    Values:</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class=\"line\"><span class=\"string\">          (MNIST images are 28 x 28 = 784 so that is your default)</span></span><br><span class=\"line\"><span class=\"string\">        hidden_dim: the inner dimension, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, z_dim=<span class=\"number\">10</span>, im_dim=<span class=\"number\">784</span>, hidden_dim=<span class=\"number\">128</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Generator, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># Build the neural network</span></span><br><span class=\"line\">        self.gen = nn.Sequential(</span><br><span class=\"line\">            get_generator_block(z_dim, hidden_dim),</span><br><span class=\"line\">            get_generator_block(hidden_dim, hidden_dim * <span class=\"number\">2</span>),</span><br><span class=\"line\">            get_generator_block(hidden_dim * <span class=\"number\">2</span>, hidden_dim * <span class=\"number\">4</span>),</span><br><span class=\"line\">            get_generator_block(hidden_dim * <span class=\"number\">4</span>, hidden_dim * <span class=\"number\">8</span>),</span><br><span class=\"line\">            <span class=\"comment\"># There is a dropdown with hints if you need them! </span></span><br><span class=\"line\">            <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">            nn.Linear(hidden_dim * <span class=\"number\">8</span>, im_dim),</span><br><span class=\"line\">            nn.Sigmoid()</span><br><span class=\"line\">            <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">        )</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, noise</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class=\"line\"><span class=\"string\">        returns generated images.</span></span><br><span class=\"line\"><span class=\"string\">        Parameters:</span></span><br><span class=\"line\"><span class=\"string\">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.gen(noise)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Needed for grading</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gen</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            the sequential model</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.gen</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the generator class</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_generator</span>(<span class=\"params\">z_dim, im_dim, hidden_dim, num_test=<span class=\"number\">10000</span></span>):</span></span><br><span class=\"line\">    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check there are six modules in the sequential part</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(gen) == <span class=\"number\">6</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, z_dim)</span><br><span class=\"line\">    test_output = gen(test_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check that the output shape is correct</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, im_dim)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.<span class=\"built_in\">max</span>() &lt; <span class=\"number\">1</span>, <span class=\"string\">&quot;Make sure to use a sigmoid&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.<span class=\"built_in\">min</span>() &gt; <span class=\"number\">0</span>, <span class=\"string\">&quot;Make sure to use a sigmoid&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.<span class=\"built_in\">min</span>() &lt; <span class=\"number\">0.5</span>, <span class=\"string\">&quot;Don&#x27;t use a block in your solution&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &gt; <span class=\"number\">0.05</span>, <span class=\"string\">&quot;Don&#x27;t use batchnorm here&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &lt; <span class=\"number\">0.15</span>, <span class=\"string\">&quot;Don&#x27;t use batchnorm here&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_generator(<span class=\"number\">5</span>, <span class=\"number\">10</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">test_generator(<span class=\"number\">20</span>, <span class=\"number\">8</span>, <span class=\"number\">24</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<h2 id=\"Noise\"><a href=\"#Noise\" class=\"headerlink\" title=\"Noise\"></a>Noise</h2><p>To be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don’t all look the same – think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.</p>\n<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. <code>torch.ones(3, 3, device=device)</code>, or move it onto the target device using <code>torch.ones(3, 3).to(device)</code>. You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as <code>torch.ones_like</code>. In general, use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code> where possible.</p>\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hint for <code><font size=\"4\">get_noise</font></code></b>\n</font>\n</summary>\n\n<ol>\n<li>You will probably find <a href=\"https://pytorch.org/docs/master/generated/torch.randn.html\">torch.randn</a> useful here.</details>\n\n\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_noise</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_noise</span>(<span class=\"params\">n_samples, z_dim, device=<span class=\"string\">&#x27;cpu&#x27;</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),</span></span><br><span class=\"line\"><span class=\"string\">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        n_samples: the number of samples to generate, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        device: the device type</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">NOTE:</span> To use this on GPU with device=&#x27;cuda&#x27;, make sure to pass the device </span></span><br><span class=\"line\">    <span class=\"comment\"># argument to the function you use to generate the noise.</span></span><br><span class=\"line\">    <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.randn(n_samples, z_dim, device=device)</span><br><span class=\"line\">    <span class=\"comment\">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the noise vector function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_get_noise</span>(<span class=\"params\">n_samples, z_dim, device=<span class=\"string\">&#x27;cpu&#x27;</span></span>):</span></span><br><span class=\"line\">    noise = get_noise(n_samples, z_dim, device)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Make sure a normal distribution was used</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(noise.shape) == (n_samples, z_dim)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">abs</span>(noise.std() - torch.tensor(<span class=\"number\">1.0</span>)) &lt; <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">str</span>(noise.device).startswith(device)</span><br><span class=\"line\"></span><br><span class=\"line\">test_get_noise(<span class=\"number\">1000</span>, <span class=\"number\">100</span>, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    test_get_noise(<span class=\"number\">1000</span>, <span class=\"number\">32</span>, <span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<h2 id=\"Discriminator\"><a href=\"#Discriminator\" class=\"headerlink\" title=\"Discriminator\"></a>Discriminator</h2><p>The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.</p>\n<p><em>Note: You use leaky ReLUs to prevent the “dying ReLU” problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!</em> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_discriminator_block</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_discriminator_block</span>(<span class=\"params\">input_dim, output_dim</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Discriminator Block</span></span><br><span class=\"line\"><span class=\"string\">    Function for returning a neural network of the discriminator given input and output dimensions.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        a discriminator neural network layer, with a linear transformation </span></span><br><span class=\"line\"><span class=\"string\">          followed by an nn.LeakyReLU activation with negative slope of 0.2 </span></span><br><span class=\"line\"><span class=\"string\">          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">        nn.Linear(input_dim, output_dim),</span><br><span class=\"line\">        nn.LeakyReLU(negative_slope=<span class=\"number\">0.2</span>)</span><br><span class=\"line\">        <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the discriminator block function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_disc_block</span>(<span class=\"params\">in_features, out_features, num_test=<span class=\"number\">10000</span></span>):</span></span><br><span class=\"line\">    block = get_discriminator_block(in_features, out_features)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check there are two parts</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(block) == <span class=\"number\">2</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, in_features)</span><br><span class=\"line\">    test_output = block(test_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check that the shape is right</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, out_features)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check that the LeakyReLU slope is about 0.2</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> -test_output.<span class=\"built_in\">min</span>() / test_output.<span class=\"built_in\">max</span>() &gt; <span class=\"number\">0.1</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> -test_output.<span class=\"built_in\">min</span>() / test_output.<span class=\"built_in\">max</span>() &lt; <span class=\"number\">0.3</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &gt; <span class=\"number\">0.3</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &lt; <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_disc_block(<span class=\"number\">25</span>, <span class=\"number\">12</span>)</span><br><span class=\"line\">test_disc_block(<span class=\"number\">15</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<p>Now you can use these blocks to make a discriminator! The discriminator class holds 2 values:</p>\n<ul>\n<li>  The image dimension</li>\n<li>  The hidden dimension</li>\n</ul>\n<p>The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator’s neural network you are given a forward pass function that takes in an image tensor to be classified.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: Discriminator</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Discriminator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Discriminator Class</span></span><br><span class=\"line\"><span class=\"string\">    Values:</span></span><br><span class=\"line\"><span class=\"string\">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class=\"line\"><span class=\"string\">            (MNIST images are 28x28 = 784 so that is your default)</span></span><br><span class=\"line\"><span class=\"string\">        hidden_dim: the inner dimension, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, im_dim=<span class=\"number\">784</span>, hidden_dim=<span class=\"number\">128</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Discriminator, self).__init__()</span><br><span class=\"line\">        self.disc = nn.Sequential(</span><br><span class=\"line\">            get_discriminator_block(im_dim, hidden_dim * <span class=\"number\">4</span>),</span><br><span class=\"line\">            get_discriminator_block(hidden_dim * <span class=\"number\">4</span>, hidden_dim * <span class=\"number\">2</span>),</span><br><span class=\"line\">            get_discriminator_block(hidden_dim * <span class=\"number\">2</span>, hidden_dim),</span><br><span class=\"line\">            <span class=\"comment\"># Hint: You want to transform the final output into a single value,</span></span><br><span class=\"line\">            <span class=\"comment\">#       so add one more linear map.</span></span><br><span class=\"line\">            <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">            nn.Linear(hidden_dim, <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, image</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class=\"line\"><span class=\"string\">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class=\"line\"><span class=\"string\">        Parameters:</span></span><br><span class=\"line\"><span class=\"string\">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.disc(image)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Needed for grading</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_disc</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            the sequential model</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.disc</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the discriminator class</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_discriminator</span>(<span class=\"params\">z_dim, hidden_dim, num_test=<span class=\"number\">100</span></span>):</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    disc = Discriminator(z_dim, hidden_dim).get_disc()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check there are three parts</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(disc) == <span class=\"number\">4</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check the linear layer is correct</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, z_dim)</span><br><span class=\"line\">    test_output = disc(test_input)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, <span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Don&#x27;t use a block</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(disc[-<span class=\"number\">1</span>], nn.Sequential)</span><br><span class=\"line\"></span><br><span class=\"line\">test_discriminator(<span class=\"number\">5</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">test_discriminator(<span class=\"number\">20</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<h2 id=\"Training\"><a href=\"#Training\" class=\"headerlink\" title=\"Training\"></a>Training</h2><p>Now you can put it all together!<br>First, you will set your parameters:</p>\n<ul>\n<li>  criterion: the loss function</li>\n<li>  n_epochs: the number of times you iterate through the entire dataset when training</li>\n<li>  z_dim: the dimension of the noise vector</li>\n<li>  display_step: how often to display/visualize the images</li>\n<li>  batch_size: the number of images per forward/backward pass</li>\n<li>  lr: the learning rate</li>\n<li>  device: the device type, here using a GPU (which runs CUDA), not CPU</li>\n</ul>\n<p>Next, you will load the MNIST dataset as tensors using a dataloader.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Set your parameters</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Notes for nn.BCEWithLogitsLoss():</span></span><br><span class=\"line\"><span class=\"string\">This loss combines a Sigmoid layer and the BCELoss in one single class. </span></span><br><span class=\"line\"><span class=\"string\">This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, </span></span><br><span class=\"line\"><span class=\"string\">by combining the operations into one layer, </span></span><br><span class=\"line\"><span class=\"string\">we take advantage of the log-sum-exp trick for numerical stability.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\">criterion = nn.BCEWithLogitsLoss()</span><br><span class=\"line\">n_epochs = <span class=\"number\">200</span></span><br><span class=\"line\">z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">display_step = <span class=\"number\">500</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\">lr = <span class=\"number\">0.00001</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Load MNIST dataset as tensors</span></span><br><span class=\"line\">dataloader = DataLoader(</span><br><span class=\"line\">    MNIST(<span class=\"string\">&#x27;.&#x27;</span>, download=<span class=\"literal\">False</span>, transform=transforms.ToTensor()),</span><br><span class=\"line\">    batch_size=batch_size,</span><br><span class=\"line\">    shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### DO NOT EDIT ###</span></span><br><span class=\"line\">device = <span class=\"string\">&#x27;cuda&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gen = Generator(z_dim).to(device)</span><br><span class=\"line\">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class=\"line\">disc = Discriminator().to(device) </span><br><span class=\"line\">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br></pre></td></tr></table></figure>\n<p>Before you train your GAN, you will need to create functions to calculate the discriminator’s loss and the generator’s loss. This is how the discriminator and generator will know how they are doing and improve themselves. <strong>Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</strong></p>\n<p>Remember that you have already defined a loss function earlier (<code>criterion</code>) and you are encouraged to use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code>. If you use <code>torch.ones</code> or <code>torch.zeros</code>, you’ll need to pass <code>device=device</code> to them.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_disc_loss</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_disc_loss</span>(<span class=\"params\">gen, disc, criterion, real, num_images, z_dim, device</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Return the loss of the discriminator given inputs.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class=\"line\"><span class=\"string\">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class=\"line\"><span class=\"string\">        criterion: the loss function, which should be used to compare </span></span><br><span class=\"line\"><span class=\"string\">               the discriminator&#x27;s predictions to the ground truth reality of the images </span></span><br><span class=\"line\"><span class=\"string\">               (e.g. fake = 0, real = 1)</span></span><br><span class=\"line\"><span class=\"string\">        real: a batch of real images</span></span><br><span class=\"line\"><span class=\"string\">        num_images: the number of images the generator should produce, </span></span><br><span class=\"line\"><span class=\"string\">                which is also the length of the real images</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        device: the device type</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        disc_loss: a torch scalar loss value for the current batch</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\">#     These are the steps you will need to complete:</span></span><br><span class=\"line\">    <span class=\"comment\">#       1) Create noise vectors and generate a batch (num_images) of fake images. </span></span><br><span class=\"line\">    <span class=\"comment\">#            Make sure to pass the device argument to the noise.</span></span><br><span class=\"line\">    <span class=\"comment\">#       2) Get the discriminator&#x27;s prediction of the fake image </span></span><br><span class=\"line\">    <span class=\"comment\">#            and calculate the loss. Don&#x27;t forget to detach the generator!</span></span><br><span class=\"line\">    <span class=\"comment\">#            (Remember the loss function you set earlier -- criterion. You need a </span></span><br><span class=\"line\">    <span class=\"comment\">#            &#x27;ground truth&#x27; tensor in order to calculate the loss. </span></span><br><span class=\"line\">    <span class=\"comment\">#            For example, a ground truth tensor for a fake image is all zeros.)</span></span><br><span class=\"line\">    <span class=\"comment\">#       3) Get the discriminator&#x27;s prediction of the real image and calculate the loss.</span></span><br><span class=\"line\">    <span class=\"comment\">#       4) Calculate the discriminator&#x27;s loss by averaging the real and fake loss</span></span><br><span class=\"line\">    <span class=\"comment\">#            and set it to disc_loss.</span></span><br><span class=\"line\">    <span class=\"comment\">#     Note: Please do not use concatenation in your solution. The tests are being updated to </span></span><br><span class=\"line\">    <span class=\"comment\">#           support this, but for now, average the two losses as described in step (4).</span></span><br><span class=\"line\">    <span class=\"comment\">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class=\"line\">    <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1</span></span><br><span class=\"line\">    noise = get_noise(num_images, z_dim, device)</span><br><span class=\"line\">    fake_imgs = gen(noise)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 2</span></span><br><span class=\"line\">    disc_output_fake = disc(fake_imgs.detach())</span><br><span class=\"line\">    ground_truth_fake = torch.zeros_like(disc_output_fake)</span><br><span class=\"line\">    loss_fake = criterion(disc_output_fake, ground_truth_fake)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 3</span></span><br><span class=\"line\">    disc_output_real = disc(real)</span><br><span class=\"line\">    ground_truth_real = torch.ones_like(disc_output_real)</span><br><span class=\"line\">    loss_real = criterion(disc_output_real, ground_truth_real)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 3tep 4</span></span><br><span class=\"line\">    disc_loss = (loss_fake + loss_real) / <span class=\"number\">2.0</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> disc_loss</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_disc_reasonable</span>(<span class=\"params\">num_images=<span class=\"number\">10</span></span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># Don&#x27;t use explicit casts to cuda - use the device argument</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> inspect, re</span><br><span class=\"line\">    lines = inspect.getsource(get_disc_loss)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;to\\(.cuda.\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;\\.cuda\\(\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = torch.zeros_like</span><br><span class=\"line\">    disc = <span class=\"keyword\">lambda</span> x: x.mean(<span class=\"number\">1</span>)[:, <span class=\"literal\">None</span>]</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.ones(num_images, z_dim)</span><br><span class=\"line\">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(disc_loss.mean() - <span class=\"number\">0.5</span>) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    gen = torch.ones_like</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.zeros(num_images, z_dim)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    gen = <span class=\"keyword\">lambda</span> x: torch.ones(num_images, <span class=\"number\">10</span>)</span><br><span class=\"line\">    disc = <span class=\"keyword\">lambda</span> x: x.mean(<span class=\"number\">1</span>)[:, <span class=\"literal\">None</span>] + <span class=\"number\">10</span></span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.zeros(num_images, <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>).mean() - <span class=\"number\">5</span>) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    gen = torch.ones_like</span><br><span class=\"line\">    disc = nn.Linear(<span class=\"number\">64</span>, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    real = torch.ones(num_images, <span class=\"number\">64</span>) * <span class=\"number\">0.5</span></span><br><span class=\"line\">    disc.weight.data = torch.ones_like(disc.weight.data) * <span class=\"number\">0.5</span></span><br><span class=\"line\">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class=\"line\">    criterion = <span class=\"keyword\">lambda</span> x, y: torch.<span class=\"built_in\">sum</span>(x) + torch.<span class=\"built_in\">sum</span>(y)</span><br><span class=\"line\">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>).mean()</span><br><span class=\"line\">    disc_loss.backward()</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.isclose(torch.<span class=\"built_in\">abs</span>(disc.weight.grad.mean() - <span class=\"number\">11.25</span>), torch.tensor(<span class=\"number\">3.75</span>))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_disc_loss</span>(<span class=\"params\">max_tests = <span class=\"number\">10</span></span>):</span></span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = Generator(z_dim).to(device)</span><br><span class=\"line\">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class=\"line\">    disc = Discriminator().to(device) </span><br><span class=\"line\">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class=\"line\">    num_steps = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> real, _ <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">        cur_batch_size = <span class=\"built_in\">len</span>(real)</span><br><span class=\"line\">        real = real.view(cur_batch_size, -<span class=\"number\">1</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Update discriminator ###</span></span><br><span class=\"line\">        <span class=\"comment\"># Zero out the gradient before backpropagation</span></span><br><span class=\"line\">        disc_opt.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate discriminator loss</span></span><br><span class=\"line\">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> (disc_loss - <span class=\"number\">0.68</span>).<span class=\"built_in\">abs</span>() &lt; <span class=\"number\">0.05</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update gradients</span></span><br><span class=\"line\">        disc_loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Check that they detached correctly</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.grad <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update optimizer</span></span><br><span class=\"line\">        old_weight = disc.disc[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.data.clone()</span><br><span class=\"line\">        disc_opt.step()</span><br><span class=\"line\">        new_weight = disc.disc[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.data</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Check that some discriminator weights changed</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> torch.<span class=\"built_in\">all</span>(torch.eq(old_weight, new_weight))</span><br><span class=\"line\">        num_steps += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> num_steps &gt;= max_tests:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_disc_reasonable()</span><br><span class=\"line\">test_disc_loss()</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_gen_loss</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gen_loss</span>(<span class=\"params\">gen, disc, criterion, num_images, z_dim, device</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Return the loss of the generator given inputs.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class=\"line\"><span class=\"string\">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class=\"line\"><span class=\"string\">        criterion: the loss function, which should be used to compare </span></span><br><span class=\"line\"><span class=\"string\">               the discriminator&#x27;s predictions to the ground truth reality of the images </span></span><br><span class=\"line\"><span class=\"string\">               (e.g. fake = 0, real = 1)</span></span><br><span class=\"line\"><span class=\"string\">        num_images: the number of images the generator should produce, </span></span><br><span class=\"line\"><span class=\"string\">                which is also the length of the real images</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        device: the device type</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        gen_loss: a torch scalar loss value for the current batch</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\">#     These are the steps you will need to complete:</span></span><br><span class=\"line\">    <span class=\"comment\">#       1) Create noise vectors and generate a batch of fake images. </span></span><br><span class=\"line\">    <span class=\"comment\">#           Remember to pass the device argument to the get_noise function.</span></span><br><span class=\"line\">    <span class=\"comment\">#       2) Get the discriminator&#x27;s prediction of the fake image.</span></span><br><span class=\"line\">    <span class=\"comment\">#       3) Calculate the generator&#x27;s loss. Remember the generator wants</span></span><br><span class=\"line\">    <span class=\"comment\">#          the discriminator to think that its fake images are real</span></span><br><span class=\"line\">    <span class=\"comment\">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1</span></span><br><span class=\"line\">    noise = get_noise(num_images, z_dim, device)</span><br><span class=\"line\">    gen_output_fake = gen(noise)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 2</span></span><br><span class=\"line\">    disc_output_fake = disc(gen_output_fake)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 3</span></span><br><span class=\"line\">    ground_truth_fake = torch.ones_like(disc_output_fake)</span><br><span class=\"line\">    gen_loss = criterion(disc_output_fake, ground_truth_fake)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> gen_loss</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_gen_reasonable</span>(<span class=\"params\">num_images=<span class=\"number\">10</span></span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># Don&#x27;t use explicit casts to cuda - use the device argument</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> inspect, re</span><br><span class=\"line\">    lines = inspect.getsource(get_gen_loss)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;to\\(.cuda.\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;\\.cuda\\(\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = torch.zeros_like</span><br><span class=\"line\">    disc = nn.Identity()</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(gen_loss_tensor) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    <span class=\"comment\">#Verify shape. Related to gen_noise parametrization</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">    gen = torch.ones_like</span><br><span class=\"line\">    disc = nn.Identity()</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.zeros(num_images, <span class=\"number\">1</span>)</span><br><span class=\"line\">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(gen_loss_tensor - <span class=\"number\">1</span>) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    <span class=\"comment\">#Verify shape. Related to gen_noise parametrization</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_gen_loss</span>(<span class=\"params\">num_images</span>):</span></span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = Generator(z_dim).to(device)</span><br><span class=\"line\">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class=\"line\">    disc = Discriminator().to(device) </span><br><span class=\"line\">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class=\"line\">    </span><br><span class=\"line\">    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check that the loss is reasonable</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (gen_loss - <span class=\"number\">0.7</span>).<span class=\"built_in\">abs</span>() &lt; <span class=\"number\">0.1</span></span><br><span class=\"line\">    gen_loss.backward()</span><br><span class=\"line\">    old_weight = gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.clone()</span><br><span class=\"line\">    gen_opt.step()</span><br><span class=\"line\">    new_weight = gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> torch.<span class=\"built_in\">all</span>(torch.eq(old_weight, new_weight))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">test_gen_reasonable(<span class=\"number\">10</span>)</span><br><span class=\"line\">test_gen_loss(<span class=\"number\">18</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<p>Finally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. </p>\n<p>It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It’s important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.</p>\n<p>After you’ve submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.</p>\n<!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: \n\n1.   Download the .ipynb\n2.   Upload it to Google Drive and open it with Google Colab\n3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)\n4.   Replace `device = \"cpu\"` with `device = \"cuda\"`\n5.   Make sure your `get_noise` function uses the right device -->\n\n<p>But remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.</p>\n<p>You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: </span></span><br><span class=\"line\"></span><br><span class=\"line\">cur_step = <span class=\"number\">0</span></span><br><span class=\"line\">mean_generator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">mean_discriminator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">test_generator = <span class=\"literal\">True</span> <span class=\"comment\"># Whether the generator should be tested</span></span><br><span class=\"line\">gen_loss = <span class=\"literal\">False</span></span><br><span class=\"line\">error = <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n_epochs):</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Dataloader returns the batches</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> real, _ <span class=\"keyword\">in</span> tqdm(dataloader):</span><br><span class=\"line\">        cur_batch_size = <span class=\"built_in\">len</span>(real)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Flatten the batch of real images from the dataset</span></span><br><span class=\"line\">        real = real.view(cur_batch_size, -<span class=\"number\">1</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Update discriminator ###</span></span><br><span class=\"line\">        <span class=\"comment\"># Zero out the gradients before backpropagation</span></span><br><span class=\"line\">        disc_opt.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate discriminator loss</span></span><br><span class=\"line\">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update gradients</span></span><br><span class=\"line\">        disc_loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update optimizer</span></span><br><span class=\"line\">        disc_opt.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># For testing purposes, to keep track of the generator weights</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_generator:</span><br><span class=\"line\">            old_generator_weights = gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.detach().clone()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Update generator ###</span></span><br><span class=\"line\">        <span class=\"comment\">#     Hint: This code will look a lot like the discriminator updates!</span></span><br><span class=\"line\">        <span class=\"comment\">#     These are the steps you will need to complete:</span></span><br><span class=\"line\">        <span class=\"comment\">#       1) Zero out the gradients.</span></span><br><span class=\"line\">        <span class=\"comment\">#       2) Calculate the generator loss, assigning it to gen_loss.</span></span><br><span class=\"line\">        <span class=\"comment\">#       3) Backprop through the generator: update the gradients and optimizer.</span></span><br><span class=\"line\">        <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">        <span class=\"comment\"># step 1</span></span><br><span class=\"line\">        gen_opt.zero_grad()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># step 2</span></span><br><span class=\"line\">        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># step 3</span></span><br><span class=\"line\">        gen_loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        gen_opt.step()</span><br><span class=\"line\">        <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># For testing purposes, to check that your code changes the generator weights</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_generator:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                <span class=\"keyword\">assert</span> lr &gt; <span class=\"number\">0.0000002</span> <span class=\"keyword\">or</span> (gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.grad.<span class=\"built_in\">abs</span>().<span class=\"built_in\">max</span>() &lt; <span class=\"number\">0.0005</span> <span class=\"keyword\">and</span> epoch == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">any</span>(gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.detach().clone() != old_generator_weights)</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                error = <span class=\"literal\">True</span></span><br><span class=\"line\">                print(<span class=\"string\">&quot;Runtime tests have failed&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Keep track of the average discriminator loss</span></span><br><span class=\"line\">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Keep track of the average generator loss</span></span><br><span class=\"line\">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Visualization code ###</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> cur_step % display_step == <span class=\"number\">0</span> <span class=\"keyword\">and</span> cur_step &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">f&quot;Epoch <span class=\"subst\">&#123;epoch&#125;</span>, step <span class=\"subst\">&#123;cur_step&#125;</span>: Generator loss: <span class=\"subst\">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class=\"subst\">&#123;mean_discriminator_loss&#125;</span>&quot;</span>)</span><br><span class=\"line\">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class=\"line\">            fake = gen(fake_noise)</span><br><span class=\"line\">            show_tensor_images(fake)</span><br><span class=\"line\">            show_tensor_images(real)</span><br><span class=\"line\">            mean_generator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">            mean_discriminator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">        cur_step += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))\n\nHBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))\n\n\nEpoch 1, step 500: Generator loss: 1.537445880770684, discriminator loss: 0.4010176688432697</code></pre>\n<p><img src=\"output_31_4.png\" alt=\"png\"></p>\n<p><img src=\"output_31_5.png\" alt=\"png\"></p>\n<p> <strong>Notes that there are only two epoch’s results.</strong></p>\n<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))\n\n\nEpoch 199, step 93500: Generator loss: 1.095844237446786, discriminator loss: 0.5419520480632782</code></pre>\n<p><img src=\"output_31_958.png\" alt=\"png\"></p>\n<p><img src=\"output_31_959.png\" alt=\"png\"></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<p>This is week 1 notes for the first course (<a href=\"https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans\">Build Basic Generative Adversarial Networks (GANs)</a>)  in the <a href=\"https://www.coursera.org/specializations/generative-adversarial-networks-gans\">GANs specialization</a>. I actually leant a lot about some pratical things in doing the assignment, like how to set some hyperparameters and the usage of truncation to balance the diversity and quality.  I really like DeepLearning.AI’s courses. The lectures and assignments are always well-designed.</p>","more":"<h1 id=\"Notes\"><a href=\"#Notes\" class=\"headerlink\" title=\"Notes\"></a>Notes</h1><ul>\n<li><p>Generator needs more time steps to train</p>\n<blockquote>\n<p> This can be a much harder task than discrimination. Typically, you will need the generator to take multiple steps to improve itself for every step the discriminator takes.</p>\n</blockquote>\n</li>\n<li><p>Noise vector 𝑧</p>\n<blockquote>\n<p> The noise vector 𝑧 has the important role of making sure the images generated from the same class 𝑦 don’t all look the same—think of it as a random seed. You generate it randomly, usually by sampling random numbers either between 0 and 1 uniformly, or from the normal distribution, which you can denote 𝑧 ~ 𝑁(0,1). The zero means the normal distribution has a mean of zero, and the 1 means that the normal distribution has a variance of 1.</p>\n</blockquote>\n<blockquote>\n<p>In reality, 𝑧 is usually larger than just 1 value to allow for more combinations of what 𝑧 could be. There’s no special number that determines what works, but 100 is standard. Some researchers might use a power of 2, like 128 or 512, but again, nothing special about the number itself, just that it’s large enough to contain a lot of possibilities. As a result, you would sample 𝑧 from that many different dimensions (constituting multiple normal distributions).</p>\n</blockquote>\n<blockquote>\n<p>Fun Fact: this is also called a spherical normal and denoted 𝑧 ~ 𝑁(0,𝐼) where the 𝐼 represents the identity matrix and means the variance is 1 in all dimensions.*</p>\n</blockquote>\n</li>\n<li><p>Truncation trick</p>\n<blockquote>\n<p>So now that you’re a bit familiar with noise vectors, here’s another cool concept that people use to tune their outputs. It’s called the truncation trick. I like to think of the truncation trick as a way of trading off fidelity (quality) and diversity in the samples. It works like this: when you randomly sample your noise vector 𝑧, you can choose to keep that random 𝑧 or you can sample another one.</p>\n</blockquote>\n<blockquote>\n<p>Why would you want to sample another one?</p>\n</blockquote>\n<blockquote>\n<p>Well, since I’m sampling 𝑧 from a normal distribution, my model will see more of those 𝑧 values within a standard deviation from the mean than those at the tails of the distribution—and this happens during training. This means that while the model is training, it’s likely to be familiar with certain noise vectors and as a result model those areas coming from familiar noise vector regions. In these areas, my model will likely have much more realistic results, but nothing too funky, it’s not taking as many risks in those regions mapped from those familiar noise vectors. This is the trade-off between fidelity (realistic, high quality images) and diversity (variety in images).</p>\n</blockquote>\n<p><img src=\"image-20210426120534698.png\" alt=\"image-20210426120534698\"></p>\n<blockquote>\n<p>Image Credit: Modelica</p>\n</blockquote>\n<blockquote>\n<p>What the truncation trick does is resamples the noise vector 𝑧 until it falls within some bounds of the normal distribution. In fact, it samples 𝑧 from a truncated normal distribution where the tails are cut off at different values (red line in graph is truncated normal, blue is original). You can tune these values and thus tune fidelity/diversity. Recall that having a lot of fidelity is not always the goal—one failure mode of that is that you get one really real image but nothing else (no diversity), and that’s not very interesting or successful from a model that’s supposed to model the realm of all possible human faces or that of all possible coconuts—including that of a cat pouncing after a flying coconut (but with extremely low probability).</p>\n</blockquote>\n<blockquote>\n<p><strong>truncation</strong>: The positive truncation value. 1 is low truncation (high diversity) and 0 is all truncation except for the mean (high quality/fidelity). A lower value increases fidelity and decreases diversity, and vice versa.</p>\n</blockquote>\n<p><img src=\"image-20210426120448703.png\" alt=\"image-20210426120448703\"></p>\n</li>\n<li><p>Playing with code</p>\n<p>interpolation</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z_dim = Gs.input_shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">first_noise = rnd.randn(<span class=\"number\">1</span>, z_dim)</span><br><span class=\"line\">second_noise = rnd.randn(<span class=\"number\">1</span>, z_dim)</span><br><span class=\"line\">percent_first_noise = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">1</span>, n_interpolation)[:, <span class=\"literal\">None</span>]</span><br><span class=\"line\">interpolation_noise = first_noise * percent_first_noise + second_noise * (<span class=\"number\">1</span> - percent_first_noise)</span><br></pre></td></tr></table></figure></li>\n<li><p>Random vector</p>\n<blockquote>\n<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. torch.ones(3, 3, device=device), or move it onto the target device using torch.ones(3, 3).to(device). You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as torch.ones_like. In general, use torch.ones_like and torch.zeros_like instead of torch.ones or torch.zeros where possible.</p>\n</blockquote>\n</li>\n<li><p>HW</p>\n<blockquote>\n<p>Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</p>\n</blockquote>\n</li>\n</ul>\n<h1 id=\"Week-1-Assignment-Your-First-GAN\"><a href=\"#Week-1-Assignment-Your-First-GAN\" class=\"headerlink\" title=\"Week 1 Assignment: Your First GAN\"></a>Week 1 Assignment: Your First GAN</h1><h3 id=\"Goal\"><a href=\"#Goal\" class=\"headerlink\" title=\"Goal\"></a>Goal</h3><p>In this notebook, you’re going to create your first generative adversarial network (GAN) for this course! Specifically, you will build and train a GAN that can generate hand-written images of digits (0-9). You will be using PyTorch in this specialization, so if you’re not familiar with this framework, you may find the <a href=\"https://pytorch.org/docs/stable/index.html\">PyTorch documentation</a> useful. The hints will also often include links to relevant documentation.</p>\n<h3 id=\"Learning-Objectives\"><a href=\"#Learning-Objectives\" class=\"headerlink\" title=\"Learning Objectives\"></a>Learning Objectives</h3><ol>\n<li>  Build the generator and discriminator components of a GAN from scratch.</li>\n<li>  Create generator and discriminator loss functions.</li>\n<li>  Train your GAN and visualize the generated images.</li>\n</ol>\n<h2 id=\"Getting-Started\"><a href=\"#Getting-Started\" class=\"headerlink\" title=\"Getting Started\"></a>Getting Started</h2><p>You will begin by importing some useful packages and the dataset you will use to build and train your GAN. You are also provided with a visualizer function to help you investigate the images your GAN will create.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.auto <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision.datasets <span class=\"keyword\">import</span> MNIST <span class=\"comment\"># Training dataset</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision.utils <span class=\"keyword\">import</span> make_grid</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">0</span>) <span class=\"comment\"># Set for testing purposes, please do not change!</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show_tensor_images</span>(<span class=\"params\">image_tensor, num_images=<span class=\"number\">25</span>, size=(<span class=\"params\"><span class=\"number\">1</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span></span>)</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Function for visualizing images: Given a tensor of images, number of images, and</span></span><br><span class=\"line\"><span class=\"string\">    size per image, plots and prints the images in a uniform grid.</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    image_unflat = image_tensor.detach().cpu().view(-<span class=\"number\">1</span>, *size)</span><br><span class=\"line\">    image_grid = make_grid(image_unflat[:num_images], nrow=<span class=\"number\">5</span>)</span><br><span class=\"line\">    plt.imshow(image_grid.permute(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>).squeeze())</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure>\n<h4 id=\"MNIST-Dataset\"><a href=\"#MNIST-Dataset\" class=\"headerlink\" title=\"MNIST Dataset\"></a>MNIST Dataset</h4><p>The training images your discriminator will be using is from a dataset called <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a>. It contains 60,000 images of handwritten digits, from 0 to 9, like these:</p>\n<p>You may notice that the images are quite pixelated – this is because they are all only 28 x 28! The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or “color channel”, is needed to represent them (more on this later in the course).</p>\n<h4 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h4><p>You will represent the data using <a href=\"https://pytorch.org/docs/stable/tensors.html\">tensors</a>. Tensors are a generalization of matrices: for example, a stack of three matrices with the amounts of red, green, and blue at different locations in a 64 x 64 pixel image is a tensor with the shape 3 x 64 x 64.</p>\n<p>Tensors are easy to manipulate and supported by <a href=\"https://pytorch.org/\">PyTorch</a>, the machine learning library you will be using. Feel free to explore them more, but you can imagine these as multi-dimensional matrices or vectors!</p>\n<h4 id=\"Batches\"><a href=\"#Batches\" class=\"headerlink\" title=\"Batches\"></a>Batches</h4><p>While you could train your model after generating one image, it is extremely inefficient and leads to less stable training. In GANs, and in machine learning in general, you will process multiple images per training step. These are called batches.</p>\n<p>This means that your generator will generate an entire batch of images and receive the discriminator’s feedback on each before updating the model. The same goes for the discriminator, it will calculate its loss on the entire batch of generated images as well as on the reals before the model is updated.</p>\n<h2 id=\"Generator\"><a href=\"#Generator\" class=\"headerlink\" title=\"Generator\"></a>Generator</h2><p>The first step is to build the generator component.</p>\n<p>You will start by creating a function to make a single layer/block for the generator’s neural network. Each block should include a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\">linear transformation</a> to map to another shape, a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\">batch normalization</a> for stabilization, and finally a non-linear activation function (you use a <a href=\"https://pytorch.org/docs/master/generated/torch.nn.ReLU.html\">ReLU here</a>) so the output can be transformed in complex ways. You will learn more about activations and batch normalization later in the course.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_generator_block</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_generator_block</span>(<span class=\"params\">input_dim, output_dim</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Function for returning a block of the generator&#x27;s neural network</span></span><br><span class=\"line\"><span class=\"string\">    given input and output dimensions.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        a generator neural network layer, with a linear transformation </span></span><br><span class=\"line\"><span class=\"string\">          followed by a batch normalization and then a relu activation</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        <span class=\"comment\"># Hint: Replace all of the &quot;None&quot; with the appropriate dimensions.</span></span><br><span class=\"line\">        <span class=\"comment\"># The documentation may be useful if you&#x27;re less familiar with PyTorch:</span></span><br><span class=\"line\">        <span class=\"comment\"># https://pytorch.org/docs/stable/nn.html.</span></span><br><span class=\"line\">        <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">        nn.Linear(input_dim, output_dim),</span><br><span class=\"line\">        nn.BatchNorm1d(output_dim),</span><br><span class=\"line\">        nn.ReLU(inplace=<span class=\"literal\">True</span>),</span><br><span class=\"line\">        <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the generator block function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_gen_block</span>(<span class=\"params\">in_features, out_features, num_test=<span class=\"number\">1000</span></span>):</span></span><br><span class=\"line\">    block = get_generator_block(in_features, out_features)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check the three parts</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(block) == <span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">type</span>(block[<span class=\"number\">0</span>]) == nn.Linear</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">type</span>(block[<span class=\"number\">1</span>]) == nn.BatchNorm1d</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">type</span>(block[<span class=\"number\">2</span>]) == nn.ReLU</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check the output shape</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, in_features)</span><br><span class=\"line\">    test_output = block(test_input)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, out_features)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &gt; <span class=\"number\">0.55</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &lt; <span class=\"number\">0.65</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_gen_block(<span class=\"number\">25</span>, <span class=\"number\">12</span>)</span><br><span class=\"line\">test_gen_block(<span class=\"number\">15</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<p>Now you can build the generator class. It will take 3 values:</p>\n<ul>\n<li>  The noise vector dimension</li>\n<li>  The image dimension</li>\n<li>  The initial hidden dimension</li>\n</ul>\n<p>Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html\">sigmoid function</a>. </p>\n<p>Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.</p>\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hints for <code><font size=\"4\">Generator</font></code></b>\n</font>\n</summary>\n\n<ol>\n<li>The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.</li>\n<li><a href=\"https://pytorch.org/docs/master/generated/torch.nn.Linear.html\">nn.Linear</a> and <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html\">nn.Sigmoid</a> will be useful here. </details>\n\n\n\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: Generator</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Generator Class</span></span><br><span class=\"line\"><span class=\"string\">    Values:</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class=\"line\"><span class=\"string\">          (MNIST images are 28 x 28 = 784 so that is your default)</span></span><br><span class=\"line\"><span class=\"string\">        hidden_dim: the inner dimension, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, z_dim=<span class=\"number\">10</span>, im_dim=<span class=\"number\">784</span>, hidden_dim=<span class=\"number\">128</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Generator, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># Build the neural network</span></span><br><span class=\"line\">        self.gen = nn.Sequential(</span><br><span class=\"line\">            get_generator_block(z_dim, hidden_dim),</span><br><span class=\"line\">            get_generator_block(hidden_dim, hidden_dim * <span class=\"number\">2</span>),</span><br><span class=\"line\">            get_generator_block(hidden_dim * <span class=\"number\">2</span>, hidden_dim * <span class=\"number\">4</span>),</span><br><span class=\"line\">            get_generator_block(hidden_dim * <span class=\"number\">4</span>, hidden_dim * <span class=\"number\">8</span>),</span><br><span class=\"line\">            <span class=\"comment\"># There is a dropdown with hints if you need them! </span></span><br><span class=\"line\">            <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">            nn.Linear(hidden_dim * <span class=\"number\">8</span>, im_dim),</span><br><span class=\"line\">            nn.Sigmoid()</span><br><span class=\"line\">            <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">        )</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, noise</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Function for completing a forward pass of the generator: Given a noise tensor, </span></span><br><span class=\"line\"><span class=\"string\">        returns generated images.</span></span><br><span class=\"line\"><span class=\"string\">        Parameters:</span></span><br><span class=\"line\"><span class=\"string\">            noise: a noise tensor with dimensions (n_samples, z_dim)</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.gen(noise)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Needed for grading</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gen</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            the sequential model</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.gen</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the generator class</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_generator</span>(<span class=\"params\">z_dim, im_dim, hidden_dim, num_test=<span class=\"number\">10000</span></span>):</span></span><br><span class=\"line\">    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check there are six modules in the sequential part</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(gen) == <span class=\"number\">6</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, z_dim)</span><br><span class=\"line\">    test_output = gen(test_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check that the output shape is correct</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, im_dim)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.<span class=\"built_in\">max</span>() &lt; <span class=\"number\">1</span>, <span class=\"string\">&quot;Make sure to use a sigmoid&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.<span class=\"built_in\">min</span>() &gt; <span class=\"number\">0</span>, <span class=\"string\">&quot;Make sure to use a sigmoid&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.<span class=\"built_in\">min</span>() &lt; <span class=\"number\">0.5</span>, <span class=\"string\">&quot;Don&#x27;t use a block in your solution&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &gt; <span class=\"number\">0.05</span>, <span class=\"string\">&quot;Don&#x27;t use batchnorm here&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &lt; <span class=\"number\">0.15</span>, <span class=\"string\">&quot;Don&#x27;t use batchnorm here&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_generator(<span class=\"number\">5</span>, <span class=\"number\">10</span>, <span class=\"number\">20</span>)</span><br><span class=\"line\">test_generator(<span class=\"number\">20</span>, <span class=\"number\">8</span>, <span class=\"number\">24</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<h2 id=\"Noise\"><a href=\"#Noise\" class=\"headerlink\" title=\"Noise\"></a>Noise</h2><p>To be able to use your generator, you will need to be able to create noise vectors. The noise vector z has the important role of making sure the images generated from the same class don’t all look the same – think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.</p>\n<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. <code>torch.ones(3, 3, device=device)</code>, or move it onto the target device using <code>torch.ones(3, 3).to(device)</code>. You do not need to do this if you’re creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as <code>torch.ones_like</code>. In general, use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code> where possible.</p>\n<details>\n\n\n<summary>\n<font size=\"3\" color=\"green\">\n<b>Optional hint for <code><font size=\"4\">get_noise</font></code></b>\n</font>\n</summary>\n\n<ol>\n<li>You will probably find <a href=\"https://pytorch.org/docs/master/generated/torch.randn.html\">torch.randn</a> useful here.</details>\n\n\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_noise</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_noise</span>(<span class=\"params\">n_samples, z_dim, device=<span class=\"string\">&#x27;cpu&#x27;</span></span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),</span></span><br><span class=\"line\"><span class=\"string\">    creates a tensor of that shape filled with random numbers from the normal distribution.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        n_samples: the number of samples to generate, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        device: the device type</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">NOTE:</span> To use this on GPU with device=&#x27;cuda&#x27;, make sure to pass the device </span></span><br><span class=\"line\">    <span class=\"comment\"># argument to the function you use to generate the noise.</span></span><br><span class=\"line\">    <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.randn(n_samples, z_dim, device=device)</span><br><span class=\"line\">    <span class=\"comment\">#### END CODE HERE ####</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the noise vector function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_get_noise</span>(<span class=\"params\">n_samples, z_dim, device=<span class=\"string\">&#x27;cpu&#x27;</span></span>):</span></span><br><span class=\"line\">    noise = get_noise(n_samples, z_dim, device)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Make sure a normal distribution was used</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(noise.shape) == (n_samples, z_dim)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">abs</span>(noise.std() - torch.tensor(<span class=\"number\">1.0</span>)) &lt; <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">str</span>(noise.device).startswith(device)</span><br><span class=\"line\"></span><br><span class=\"line\">test_get_noise(<span class=\"number\">1000</span>, <span class=\"number\">100</span>, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    test_get_noise(<span class=\"number\">1000</span>, <span class=\"number\">32</span>, <span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<h2 id=\"Discriminator\"><a href=\"#Discriminator\" class=\"headerlink\" title=\"Discriminator\"></a>Discriminator</h2><p>The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.</p>\n<p><em>Note: You use leaky ReLUs to prevent the “dying ReLU” problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!</em> </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_discriminator_block</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_discriminator_block</span>(<span class=\"params\">input_dim, output_dim</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Discriminator Block</span></span><br><span class=\"line\"><span class=\"string\">    Function for returning a neural network of the discriminator given input and output dimensions.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        input_dim: the dimension of the input vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        output_dim: the dimension of the output vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        a discriminator neural network layer, with a linear transformation </span></span><br><span class=\"line\"><span class=\"string\">          followed by an nn.LeakyReLU activation with negative slope of 0.2 </span></span><br><span class=\"line\"><span class=\"string\">          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.Sequential(</span><br><span class=\"line\">        <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">        nn.Linear(input_dim, output_dim),</span><br><span class=\"line\">        nn.LeakyReLU(negative_slope=<span class=\"number\">0.2</span>)</span><br><span class=\"line\">        <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the discriminator block function</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_disc_block</span>(<span class=\"params\">in_features, out_features, num_test=<span class=\"number\">10000</span></span>):</span></span><br><span class=\"line\">    block = get_discriminator_block(in_features, out_features)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check there are two parts</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(block) == <span class=\"number\">2</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, in_features)</span><br><span class=\"line\">    test_output = block(test_input)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check that the shape is right</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, out_features)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check that the LeakyReLU slope is about 0.2</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> -test_output.<span class=\"built_in\">min</span>() / test_output.<span class=\"built_in\">max</span>() &gt; <span class=\"number\">0.1</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> -test_output.<span class=\"built_in\">min</span>() / test_output.<span class=\"built_in\">max</span>() &lt; <span class=\"number\">0.3</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &gt; <span class=\"number\">0.3</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> test_output.std() &lt; <span class=\"number\">0.5</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_disc_block(<span class=\"number\">25</span>, <span class=\"number\">12</span>)</span><br><span class=\"line\">test_disc_block(<span class=\"number\">15</span>, <span class=\"number\">28</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<p>Now you can use these blocks to make a discriminator! The discriminator class holds 2 values:</p>\n<ul>\n<li>  The image dimension</li>\n<li>  The hidden dimension</li>\n</ul>\n<p>The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator’s neural network you are given a forward pass function that takes in an image tensor to be classified.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: Discriminator</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Discriminator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Discriminator Class</span></span><br><span class=\"line\"><span class=\"string\">    Values:</span></span><br><span class=\"line\"><span class=\"string\">        im_dim: the dimension of the images, fitted for the dataset used, a scalar</span></span><br><span class=\"line\"><span class=\"string\">            (MNIST images are 28x28 = 784 so that is your default)</span></span><br><span class=\"line\"><span class=\"string\">        hidden_dim: the inner dimension, a scalar</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, im_dim=<span class=\"number\">784</span>, hidden_dim=<span class=\"number\">128</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Discriminator, self).__init__()</span><br><span class=\"line\">        self.disc = nn.Sequential(</span><br><span class=\"line\">            get_discriminator_block(im_dim, hidden_dim * <span class=\"number\">4</span>),</span><br><span class=\"line\">            get_discriminator_block(hidden_dim * <span class=\"number\">4</span>, hidden_dim * <span class=\"number\">2</span>),</span><br><span class=\"line\">            get_discriminator_block(hidden_dim * <span class=\"number\">2</span>, hidden_dim),</span><br><span class=\"line\">            <span class=\"comment\"># Hint: You want to transform the final output into a single value,</span></span><br><span class=\"line\">            <span class=\"comment\">#       so add one more linear map.</span></span><br><span class=\"line\">            <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">            nn.Linear(hidden_dim, <span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, image</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Function for completing a forward pass of the discriminator: Given an image tensor, </span></span><br><span class=\"line\"><span class=\"string\">        returns a 1-dimension tensor representing fake/real.</span></span><br><span class=\"line\"><span class=\"string\">        Parameters:</span></span><br><span class=\"line\"><span class=\"string\">            image: a flattened image tensor with dimension (im_dim)</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.disc(image)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Needed for grading</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_disc</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            the sequential model</span></span><br><span class=\"line\"><span class=\"string\">        &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.disc</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Verify the discriminator class</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_discriminator</span>(<span class=\"params\">z_dim, hidden_dim, num_test=<span class=\"number\">100</span></span>):</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    disc = Discriminator(z_dim, hidden_dim).get_disc()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check there are three parts</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">len</span>(disc) == <span class=\"number\">4</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Check the linear layer is correct</span></span><br><span class=\"line\">    test_input = torch.randn(num_test, z_dim)</span><br><span class=\"line\">    test_output = disc(test_input)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(test_output.shape) == (num_test, <span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Don&#x27;t use a block</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> <span class=\"built_in\">isinstance</span>(disc[-<span class=\"number\">1</span>], nn.Sequential)</span><br><span class=\"line\"></span><br><span class=\"line\">test_discriminator(<span class=\"number\">5</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">test_discriminator(<span class=\"number\">20</span>, <span class=\"number\">8</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<h2 id=\"Training\"><a href=\"#Training\" class=\"headerlink\" title=\"Training\"></a>Training</h2><p>Now you can put it all together!<br>First, you will set your parameters:</p>\n<ul>\n<li>  criterion: the loss function</li>\n<li>  n_epochs: the number of times you iterate through the entire dataset when training</li>\n<li>  z_dim: the dimension of the noise vector</li>\n<li>  display_step: how often to display/visualize the images</li>\n<li>  batch_size: the number of images per forward/backward pass</li>\n<li>  lr: the learning rate</li>\n<li>  device: the device type, here using a GPU (which runs CUDA), not CPU</li>\n</ul>\n<p>Next, you will load the MNIST dataset as tensors using a dataloader.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Set your parameters</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">Notes for nn.BCEWithLogitsLoss():</span></span><br><span class=\"line\"><span class=\"string\">This loss combines a Sigmoid layer and the BCELoss in one single class. </span></span><br><span class=\"line\"><span class=\"string\">This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, </span></span><br><span class=\"line\"><span class=\"string\">by combining the operations into one layer, </span></span><br><span class=\"line\"><span class=\"string\">we take advantage of the log-sum-exp trick for numerical stability.</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\">criterion = nn.BCEWithLogitsLoss()</span><br><span class=\"line\">n_epochs = <span class=\"number\">200</span></span><br><span class=\"line\">z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">display_step = <span class=\"number\">500</span></span><br><span class=\"line\">batch_size = <span class=\"number\">128</span></span><br><span class=\"line\">lr = <span class=\"number\">0.00001</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Load MNIST dataset as tensors</span></span><br><span class=\"line\">dataloader = DataLoader(</span><br><span class=\"line\">    MNIST(<span class=\"string\">&#x27;.&#x27;</span>, download=<span class=\"literal\">False</span>, transform=transforms.ToTensor()),</span><br><span class=\"line\">    batch_size=batch_size,</span><br><span class=\"line\">    shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">### DO NOT EDIT ###</span></span><br><span class=\"line\">device = <span class=\"string\">&#x27;cuda&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gen = Generator(z_dim).to(device)</span><br><span class=\"line\">gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class=\"line\">disc = Discriminator().to(device) </span><br><span class=\"line\">disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br></pre></td></tr></table></figure>\n<p>Before you train your GAN, you will need to create functions to calculate the discriminator’s loss and the generator’s loss. This is how the discriminator and generator will know how they are doing and improve themselves. <strong>Since the generator is needed when calculating the discriminator’s loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</strong></p>\n<p>Remember that you have already defined a loss function earlier (<code>criterion</code>) and you are encouraged to use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code>. If you use <code>torch.ones</code> or <code>torch.zeros</code>, you’ll need to pass <code>device=device</code> to them.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_disc_loss</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_disc_loss</span>(<span class=\"params\">gen, disc, criterion, real, num_images, z_dim, device</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Return the loss of the discriminator given inputs.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class=\"line\"><span class=\"string\">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class=\"line\"><span class=\"string\">        criterion: the loss function, which should be used to compare </span></span><br><span class=\"line\"><span class=\"string\">               the discriminator&#x27;s predictions to the ground truth reality of the images </span></span><br><span class=\"line\"><span class=\"string\">               (e.g. fake = 0, real = 1)</span></span><br><span class=\"line\"><span class=\"string\">        real: a batch of real images</span></span><br><span class=\"line\"><span class=\"string\">        num_images: the number of images the generator should produce, </span></span><br><span class=\"line\"><span class=\"string\">                which is also the length of the real images</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        device: the device type</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        disc_loss: a torch scalar loss value for the current batch</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\">#     These are the steps you will need to complete:</span></span><br><span class=\"line\">    <span class=\"comment\">#       1) Create noise vectors and generate a batch (num_images) of fake images. </span></span><br><span class=\"line\">    <span class=\"comment\">#            Make sure to pass the device argument to the noise.</span></span><br><span class=\"line\">    <span class=\"comment\">#       2) Get the discriminator&#x27;s prediction of the fake image </span></span><br><span class=\"line\">    <span class=\"comment\">#            and calculate the loss. Don&#x27;t forget to detach the generator!</span></span><br><span class=\"line\">    <span class=\"comment\">#            (Remember the loss function you set earlier -- criterion. You need a </span></span><br><span class=\"line\">    <span class=\"comment\">#            &#x27;ground truth&#x27; tensor in order to calculate the loss. </span></span><br><span class=\"line\">    <span class=\"comment\">#            For example, a ground truth tensor for a fake image is all zeros.)</span></span><br><span class=\"line\">    <span class=\"comment\">#       3) Get the discriminator&#x27;s prediction of the real image and calculate the loss.</span></span><br><span class=\"line\">    <span class=\"comment\">#       4) Calculate the discriminator&#x27;s loss by averaging the real and fake loss</span></span><br><span class=\"line\">    <span class=\"comment\">#            and set it to disc_loss.</span></span><br><span class=\"line\">    <span class=\"comment\">#     Note: Please do not use concatenation in your solution. The tests are being updated to </span></span><br><span class=\"line\">    <span class=\"comment\">#           support this, but for now, average the two losses as described in step (4).</span></span><br><span class=\"line\">    <span class=\"comment\">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class=\"line\">    <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1</span></span><br><span class=\"line\">    noise = get_noise(num_images, z_dim, device)</span><br><span class=\"line\">    fake_imgs = gen(noise)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 2</span></span><br><span class=\"line\">    disc_output_fake = disc(fake_imgs.detach())</span><br><span class=\"line\">    ground_truth_fake = torch.zeros_like(disc_output_fake)</span><br><span class=\"line\">    loss_fake = criterion(disc_output_fake, ground_truth_fake)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 3</span></span><br><span class=\"line\">    disc_output_real = disc(real)</span><br><span class=\"line\">    ground_truth_real = torch.ones_like(disc_output_real)</span><br><span class=\"line\">    loss_real = criterion(disc_output_real, ground_truth_real)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 3tep 4</span></span><br><span class=\"line\">    disc_loss = (loss_fake + loss_real) / <span class=\"number\">2.0</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> disc_loss</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_disc_reasonable</span>(<span class=\"params\">num_images=<span class=\"number\">10</span></span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># Don&#x27;t use explicit casts to cuda - use the device argument</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> inspect, re</span><br><span class=\"line\">    lines = inspect.getsource(get_disc_loss)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;to\\(.cuda.\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;\\.cuda\\(\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = torch.zeros_like</span><br><span class=\"line\">    disc = <span class=\"keyword\">lambda</span> x: x.mean(<span class=\"number\">1</span>)[:, <span class=\"literal\">None</span>]</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.ones(num_images, z_dim)</span><br><span class=\"line\">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(disc_loss.mean() - <span class=\"number\">0.5</span>) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    gen = torch.ones_like</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.zeros(num_images, z_dim)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    gen = <span class=\"keyword\">lambda</span> x: torch.ones(num_images, <span class=\"number\">10</span>)</span><br><span class=\"line\">    disc = <span class=\"keyword\">lambda</span> x: x.mean(<span class=\"number\">1</span>)[:, <span class=\"literal\">None</span>] + <span class=\"number\">10</span></span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.zeros(num_images, <span class=\"number\">10</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>).mean() - <span class=\"number\">5</span>) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    gen = torch.ones_like</span><br><span class=\"line\">    disc = nn.Linear(<span class=\"number\">64</span>, <span class=\"number\">1</span>, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    real = torch.ones(num_images, <span class=\"number\">64</span>) * <span class=\"number\">0.5</span></span><br><span class=\"line\">    disc.weight.data = torch.ones_like(disc.weight.data) * <span class=\"number\">0.5</span></span><br><span class=\"line\">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class=\"line\">    criterion = <span class=\"keyword\">lambda</span> x, y: torch.<span class=\"built_in\">sum</span>(x) + torch.<span class=\"built_in\">sum</span>(y)</span><br><span class=\"line\">    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>).mean()</span><br><span class=\"line\">    disc_loss.backward()</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.isclose(torch.<span class=\"built_in\">abs</span>(disc.weight.grad.mean() - <span class=\"number\">11.25</span>), torch.tensor(<span class=\"number\">3.75</span>))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_disc_loss</span>(<span class=\"params\">max_tests = <span class=\"number\">10</span></span>):</span></span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = Generator(z_dim).to(device)</span><br><span class=\"line\">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class=\"line\">    disc = Discriminator().to(device) </span><br><span class=\"line\">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class=\"line\">    num_steps = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> real, _ <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">        cur_batch_size = <span class=\"built_in\">len</span>(real)</span><br><span class=\"line\">        real = real.view(cur_batch_size, -<span class=\"number\">1</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Update discriminator ###</span></span><br><span class=\"line\">        <span class=\"comment\"># Zero out the gradient before backpropagation</span></span><br><span class=\"line\">        disc_opt.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate discriminator loss</span></span><br><span class=\"line\">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class=\"line\">        <span class=\"keyword\">assert</span> (disc_loss - <span class=\"number\">0.68</span>).<span class=\"built_in\">abs</span>() &lt; <span class=\"number\">0.05</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update gradients</span></span><br><span class=\"line\">        disc_loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Check that they detached correctly</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.grad <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update optimizer</span></span><br><span class=\"line\">        old_weight = disc.disc[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.data.clone()</span><br><span class=\"line\">        disc_opt.step()</span><br><span class=\"line\">        new_weight = disc.disc[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.data</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Check that some discriminator weights changed</span></span><br><span class=\"line\">        <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> torch.<span class=\"built_in\">all</span>(torch.eq(old_weight, new_weight))</span><br><span class=\"line\">        num_steps += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> num_steps &gt;= max_tests:</span><br><span class=\"line\">            <span class=\"keyword\">break</span></span><br><span class=\"line\"></span><br><span class=\"line\">test_disc_reasonable()</span><br><span class=\"line\">test_disc_loss()</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: get_gen_loss</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_gen_loss</span>(<span class=\"params\">gen, disc, criterion, num_images, z_dim, device</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    Return the loss of the generator given inputs.</span></span><br><span class=\"line\"><span class=\"string\">    Parameters:</span></span><br><span class=\"line\"><span class=\"string\">        gen: the generator model, which returns an image given z-dimensional noise</span></span><br><span class=\"line\"><span class=\"string\">        disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span></span><br><span class=\"line\"><span class=\"string\">        criterion: the loss function, which should be used to compare </span></span><br><span class=\"line\"><span class=\"string\">               the discriminator&#x27;s predictions to the ground truth reality of the images </span></span><br><span class=\"line\"><span class=\"string\">               (e.g. fake = 0, real = 1)</span></span><br><span class=\"line\"><span class=\"string\">        num_images: the number of images the generator should produce, </span></span><br><span class=\"line\"><span class=\"string\">                which is also the length of the real images</span></span><br><span class=\"line\"><span class=\"string\">        z_dim: the dimension of the noise vector, a scalar</span></span><br><span class=\"line\"><span class=\"string\">        device: the device type</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">        gen_loss: a torch scalar loss value for the current batch</span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    <span class=\"comment\">#     These are the steps you will need to complete:</span></span><br><span class=\"line\">    <span class=\"comment\">#       1) Create noise vectors and generate a batch of fake images. </span></span><br><span class=\"line\">    <span class=\"comment\">#           Remember to pass the device argument to the get_noise function.</span></span><br><span class=\"line\">    <span class=\"comment\">#       2) Get the discriminator&#x27;s prediction of the fake image.</span></span><br><span class=\"line\">    <span class=\"comment\">#       3) Calculate the generator&#x27;s loss. Remember the generator wants</span></span><br><span class=\"line\">    <span class=\"comment\">#          the discriminator to think that its fake images are real</span></span><br><span class=\"line\">    <span class=\"comment\">#     *Important*: You should NOT write your own loss function here - use criterion(pred, true)!</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"comment\"># step 1</span></span><br><span class=\"line\">    noise = get_noise(num_images, z_dim, device)</span><br><span class=\"line\">    gen_output_fake = gen(noise)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 2</span></span><br><span class=\"line\">    disc_output_fake = disc(gen_output_fake)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># step 3</span></span><br><span class=\"line\">    ground_truth_fake = torch.ones_like(disc_output_fake)</span><br><span class=\"line\">    gen_loss = criterion(disc_output_fake, ground_truth_fake)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> gen_loss</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_gen_reasonable</span>(<span class=\"params\">num_images=<span class=\"number\">10</span></span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># Don&#x27;t use explicit casts to cuda - use the device argument</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> inspect, re</span><br><span class=\"line\">    lines = inspect.getsource(get_gen_loss)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;to\\(.cuda.\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (re.search(<span class=\"string\">r&quot;\\.cuda\\(\\)&quot;</span>, lines)) <span class=\"keyword\">is</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = torch.zeros_like</span><br><span class=\"line\">    disc = nn.Identity()</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(gen_loss_tensor) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    <span class=\"comment\">#Verify shape. Related to gen_noise parametrization</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">    gen = torch.ones_like</span><br><span class=\"line\">    disc = nn.Identity()</span><br><span class=\"line\">    criterion = torch.mul <span class=\"comment\"># Multiply</span></span><br><span class=\"line\">    real = torch.zeros(num_images, <span class=\"number\">1</span>)</span><br><span class=\"line\">    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, <span class=\"string\">&#x27;cpu&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">all</span>(torch.<span class=\"built_in\">abs</span>(gen_loss_tensor - <span class=\"number\">1</span>) &lt; <span class=\"number\">1e-5</span>)</span><br><span class=\"line\">    <span class=\"comment\">#Verify shape. Related to gen_noise parametrization</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"built_in\">tuple</span>(gen_loss_tensor.shape) == (num_images, z_dim)</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_gen_loss</span>(<span class=\"params\">num_images</span>):</span></span><br><span class=\"line\">    z_dim = <span class=\"number\">64</span></span><br><span class=\"line\">    gen = Generator(z_dim).to(device)</span><br><span class=\"line\">    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)</span><br><span class=\"line\">    disc = Discriminator().to(device) </span><br><span class=\"line\">    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)</span><br><span class=\"line\">    </span><br><span class=\"line\">    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Check that the loss is reasonable</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (gen_loss - <span class=\"number\">0.7</span>).<span class=\"built_in\">abs</span>() &lt; <span class=\"number\">0.1</span></span><br><span class=\"line\">    gen_loss.backward()</span><br><span class=\"line\">    old_weight = gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.clone()</span><br><span class=\"line\">    gen_opt.step()</span><br><span class=\"line\">    new_weight = gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"keyword\">not</span> torch.<span class=\"built_in\">all</span>(torch.eq(old_weight, new_weight))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">test_gen_reasonable(<span class=\"number\">10</span>)</span><br><span class=\"line\">test_gen_loss(<span class=\"number\">18</span>)</span><br><span class=\"line\">print(<span class=\"string\">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>Success!</code></pre>\n<p>Finally, you can put everything together! For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. </p>\n<p>It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It’s important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.</p>\n<p>After you’ve submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.</p>\n<!-- In addition, be warned that this runs very slowly on a CPU. One way to run this more quickly is to use Google Colab: \n\n1.   Download the .ipynb\n2.   Upload it to Google Drive and open it with Google Colab\n3.   Make the runtime type GPU (under “Runtime” -> “Change runtime type” -> Select “GPU” from the dropdown)\n4.   Replace `device = \"cpu\"` with `device = \"cuda\"`\n5.   Make sure your `get_noise` function uses the right device -->\n\n<p>But remember, don’t expect anything spectacular: this is only the first lesson. The results will get better with later lessons as you learn methods to help keep your generator and discriminator at similar levels.</p>\n<p>You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span></span><br><span class=\"line\"><span class=\"comment\"># GRADED FUNCTION: </span></span><br><span class=\"line\"></span><br><span class=\"line\">cur_step = <span class=\"number\">0</span></span><br><span class=\"line\">mean_generator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">mean_discriminator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">test_generator = <span class=\"literal\">True</span> <span class=\"comment\"># Whether the generator should be tested</span></span><br><span class=\"line\">gen_loss = <span class=\"literal\">False</span></span><br><span class=\"line\">error = <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n_epochs):</span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\"># Dataloader returns the batches</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> real, _ <span class=\"keyword\">in</span> tqdm(dataloader):</span><br><span class=\"line\">        cur_batch_size = <span class=\"built_in\">len</span>(real)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Flatten the batch of real images from the dataset</span></span><br><span class=\"line\">        real = real.view(cur_batch_size, -<span class=\"number\">1</span>).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Update discriminator ###</span></span><br><span class=\"line\">        <span class=\"comment\"># Zero out the gradients before backpropagation</span></span><br><span class=\"line\">        disc_opt.zero_grad()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate discriminator loss</span></span><br><span class=\"line\">        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update gradients</span></span><br><span class=\"line\">        disc_loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update optimizer</span></span><br><span class=\"line\">        disc_opt.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># For testing purposes, to keep track of the generator weights</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_generator:</span><br><span class=\"line\">            old_generator_weights = gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.detach().clone()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Update generator ###</span></span><br><span class=\"line\">        <span class=\"comment\">#     Hint: This code will look a lot like the discriminator updates!</span></span><br><span class=\"line\">        <span class=\"comment\">#     These are the steps you will need to complete:</span></span><br><span class=\"line\">        <span class=\"comment\">#       1) Zero out the gradients.</span></span><br><span class=\"line\">        <span class=\"comment\">#       2) Calculate the generator loss, assigning it to gen_loss.</span></span><br><span class=\"line\">        <span class=\"comment\">#       3) Backprop through the generator: update the gradients and optimizer.</span></span><br><span class=\"line\">        <span class=\"comment\">#### START CODE HERE ####</span></span><br><span class=\"line\">        <span class=\"comment\"># step 1</span></span><br><span class=\"line\">        gen_opt.zero_grad()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># step 2</span></span><br><span class=\"line\">        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># step 3</span></span><br><span class=\"line\">        gen_loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        gen_opt.step()</span><br><span class=\"line\">        <span class=\"comment\">#### END CODE HERE ####</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># For testing purposes, to check that your code changes the generator weights</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> test_generator:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                <span class=\"keyword\">assert</span> lr &gt; <span class=\"number\">0.0000002</span> <span class=\"keyword\">or</span> (gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.grad.<span class=\"built_in\">abs</span>().<span class=\"built_in\">max</span>() &lt; <span class=\"number\">0.0005</span> <span class=\"keyword\">and</span> epoch == <span class=\"number\">0</span>)</span><br><span class=\"line\">                <span class=\"keyword\">assert</span> torch.<span class=\"built_in\">any</span>(gen.gen[<span class=\"number\">0</span>][<span class=\"number\">0</span>].weight.detach().clone() != old_generator_weights)</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                error = <span class=\"literal\">True</span></span><br><span class=\"line\">                print(<span class=\"string\">&quot;Runtime tests have failed&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Keep track of the average discriminator loss</span></span><br><span class=\"line\">        mean_discriminator_loss += disc_loss.item() / display_step</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Keep track of the average generator loss</span></span><br><span class=\"line\">        mean_generator_loss += gen_loss.item() / display_step</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">### Visualization code ###</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> cur_step % display_step == <span class=\"number\">0</span> <span class=\"keyword\">and</span> cur_step &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">f&quot;Epoch <span class=\"subst\">&#123;epoch&#125;</span>, step <span class=\"subst\">&#123;cur_step&#125;</span>: Generator loss: <span class=\"subst\">&#123;mean_generator_loss&#125;</span>, discriminator loss: <span class=\"subst\">&#123;mean_discriminator_loss&#125;</span>&quot;</span>)</span><br><span class=\"line\">            fake_noise = get_noise(cur_batch_size, z_dim, device=device)</span><br><span class=\"line\">            fake = gen(fake_noise)</span><br><span class=\"line\">            show_tensor_images(fake)</span><br><span class=\"line\">            show_tensor_images(real)</span><br><span class=\"line\">            mean_generator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">            mean_discriminator_loss = <span class=\"number\">0</span></span><br><span class=\"line\">        cur_step += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))\n\nHBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))\n\n\nEpoch 1, step 500: Generator loss: 1.537445880770684, discriminator loss: 0.4010176688432697</code></pre>\n<p><img src=\"output_31_4.png\" alt=\"png\"></p>\n<p><img src=\"output_31_5.png\" alt=\"png\"></p>\n<p> <strong>Notes that there are only two epoch’s results.</strong></p>\n<pre><code>HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value=&#39;&#39;)))\n\n\nEpoch 199, step 93500: Generator loss: 1.095844237446786, discriminator loss: 0.5419520480632782</code></pre>\n<p><img src=\"output_31_958.png\" alt=\"png\"></p>\n<p><img src=\"output_31_959.png\" alt=\"png\"></p>"},{"title":"A Survey for Vision Transformers (By Mar. 2021)","date":"2021-04-25T05:02:11.000Z","_content":"\n**ABSTRACT:** Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.\n\n<!-- more -->\n\n**KEYWORDS:** *Self-attention, transformer, convolution neural networks, deep neural networks.*\n\n# 1.  Introduction\n\nConvolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.\n\nDespite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. \n\nAt the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. \n\nInspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). \n\nEven though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? \n\nThe structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.\n\n# 2.  Related work\n\nVisio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.\n\n## 2.1 CNN\n\nThere are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. \n\nIn 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. \n\nIn 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. \n\nConsidering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. \n\nIn 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.\n\n![img](clip_image001.png)\n\nFigure 1. DenseNet (G. Huang et al., 2016)\n\nIt is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. \n\n## 2.2 Attention in CNN\n\nConsidering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. \n\n![img](clip_image002.png)\n\nFigure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)\n\nInspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. \n\n![img](clip_image003.png)\n\nFigure 3. Selective Kernel Convolution. (X. Li et al., 2019)\n\nSome researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. \n\nOther researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. \n\n![img](clip_image004.png)\n\nFigure 4. Non-local block. (X. Wang et al., 2018)\n\nAlthough NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. \n\nTo summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.\n\nTable 1. Model comparison for the application of attention in CNN.\n\n\n\n| **Model name** | **Characteristics**                                  |\n| -------------- | ---------------------------------------------------- |\n| SENet          | Spatial aggregation, channel attention               |\n| SKNet          | Adaptive receptive filed, channel attention          |\n| BAM            | Channel attention and spatial attention in parallel  |\n| CBAM           | Channel attention and spatial attention sequentially |\n| NLNet          | Spatial weighted sum per pixel                       |\n| GCNet          | Spatial weighted sum, channel attention              |\n\n \n\n## 2.3 Self-attention\n\nSelf-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. \n\n![img](clip_image005.png)\n\nFigure 5. Self-attention operation.\n\nThe calculation of self-attention can be formulated as a single function: \n\n\n\n| ![img](clip_image007.png) | (1)  |\n| ------------------------- | ---- |\n|                           |      |\n\nThe first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of ![img](clip_image009.png) and ![img](clip_image011.png) aims to calculate the similarity/distance between two different vectors. The score of ![img](clip_image013.png) can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. \n\nIt is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix ![img](clip_image011.png),value matrix ![img](clip_image016.png) and query matrix ![img](clip_image009.png) come from. For the encoder-decoder attention layer, ![img](clip_image019.png)and ![img](clip_image016.png) come from the encoder module and ![img](clip_image009.png) comes from the previous layer. Other operations are the same. \n\nThe limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:\n\n\n\n| ![img](clip_image023.png) | (2)  |\n| ------------------------- | ---- |\n|                           |      |\n\nwhere ![img](clip_image025.png), ![img](clip_image027.png), ![img](clip_image029.png), and ![img](clip_image031.png)are the concatenation of ![img](clip_image033.png). \n\n![img](clip_image034.png)\n\nFigure 6. Multi-head attention. (Vaswani et al., 2017)\n\n## 2.4 Fourier-based network\n\nConvolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. \n\nAnother network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.\n\n![img](clip_image035.png)\n\nFigure 7. Fourier Layer. (Z. Li et al., 2020)\n\nThere are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.\n\n# 3.  Topic and its applications\n\nIn this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. \n\n## 3.1 Transformer\n\nIn the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. \n\n![img](clip_image036.png)\n\nFigure 8. The architecture for transformer. (Vaswani et al., 2017)\n\nThe first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. \n\nThe second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:\n\n\n\n| ![img](clip_image038.png), | (3)  |\n| -------------------------- | ---- |\n|                            |      |\n\nwhere ![img](clip_image040.png)and ![img](clip_image042.png) are weights for the two linear transformation layers, ![img](clip_image044.png) is ReLU activation function. \n\nThe third thing is Add & Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. \n\nThe last thing is the output layer. After passing through ![img](clip_image046.png)decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.\n\nTo summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.\n\n## 3.2 Applications\n\nSince 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.\n\nInspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. \n\n## 3.2.1  Image classification\n\nViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image ![img](clip_image048.png) as a sequence of flattened 2D patches ![img](clip_image048.png), where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. \n\n![img](clip_image050.png)\n\nFigure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)\n\nViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. \n\n![img](clip_image051.png)\n\nFigure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)\n\nViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. \n\nAnother vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.\n\n![img](clip_image053.png)\n\nFigure 11. T2T architecture and feature visualization. (Yuan et al., 2021)\n\nViewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. \n\n![img](clip_image054.png)\n\nFigure 12. TNT block and TNT framework. (Han et al., 2021)\n\nBottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.\n\n![img](clip_image055.png)\n\nFigure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)\n\n## 3.2.2  Object detection\n\nCompared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. \n\n![img](clip_image056.png)\n\nFigure 14. The architecture for DETR. (Carion et al., 2020)\n\nTo solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.\n\nPyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. \n\nAdaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. \n\nSun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.\n\n\n\n## 3.2.3  Segmentation\n\nSegmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. \n\n![img](clip_image057.png)\n\nFigure 15. The architecture for VisTR. (Y. Wang et al., 2020)\n\nPoint Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. \n\n\n\n# 4.  Classification and comparison of various methods\n\nTransformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. \n\n \n\n| **Task**                      | **Category**                            | **Method**                                                  | **Characteristics**                                          |\n| ----------------------------- | --------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |\n| Image classification          | Transformer + global attention          | ViT   (Dosovitskiy  et al., 2020)                           | Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset |\n| Image classification          | Transformer + CNN                       | DeiT   (Touvron  et al., 2021)                              | Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher |\n| Image classification          | Transformer + global attention          | T2T-ViT   (Yuan et  al., 2021)                              | Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet |\n| Image classification          | Transformer + global attention          | TNT   (Han et  al., 2021)                                   | Operate on patch-level and pixel-level, more diverse  feature extraction |\n| Object detection              | Transformer + CNN                       | DETR   (Carion  et al., 2020)                               | End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge |\n| Object detection              | Transformer + CNN + efficient attention | Deformable DETR   (Zhu et  al., 2020)                       | Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time |\n| Segmentation                  | Transformer + CNN                       | MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)             | ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture |\n| Segmentation                  | Transformer + CNN                       | VisTR   (Y. Wang  et al., 2020)                             | instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction |\n| Image processing              | Transformer + CNN                       | Image processing transformer  (IPT) (H. Chen  et al., 2020) | The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning |\n| Image colorization            | Transformer + global attention          | Colorization Transformer (ColTran) (Kumar et  al., 2020)    | Conditional transformer layers, two parallel networks to upsample  low resolution images |\n| Segmentation                  | Transformer + efficient attention       | Criss-Cross Network (CCNet)                                 | Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance, |\n| NLP, architecture exploration | Transformer + efficient attention       | ConvBERT (Jiang et al., 2020)                               | Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies |\n\n \n\n# 5.  Future work\n\nTransformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. \n\nThe first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. \n\nThe second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. \n\nThe third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.\n\nThe fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. \n\n# 6.  Conclusion\n\nIn this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.\n\n# 7.  Course feedbacks\n\nThis course is a great graduate-level class. There are several reasons for that:\n\n\\1.   **Rich course content.** From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. \n\n\\2.   **Excellent lectures.** Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.\n\n\\3.   **Novel and creative illustration**. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.\n\n\\4.   **Excellent reading recommendation**. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.\n\n\\5.   For every lecture, professor always pointed out the “**take away home message**”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.\n\n\\6.   **The slides are clear and well-organized.** I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.\n\n# 8.  Reference\n\n[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. *ArXiv:2012.09958 [Cs]*. http://arxiv.org/abs/2012.09958\n\n[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. *IEEE Transactions on Neural Networks*, *5*(2), 157–166. https://doi.org/10.1109/72.279181\n\n[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). *Language Models are Few-Shot Learners*. https://arxiv.org/abs/2005.14165v4\n\n[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). *GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond*. 0–0. https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\n\n[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. *ArXiv:2005.12872 [Cs]*. http://arxiv.org/abs/2005.12872\n\n[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. *ArXiv:2012.00364 [Cs]*. http://arxiv.org/abs/2012.00364\n\n[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. *International Conference on Machine Learning*, 1691–1703. http://proceedings.mlr.press/v119/chen20s.html\n\n[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. *ArXiv:1606.03657 [Cs, Stat]*. http://arxiv.org/abs/1606.03657\n\n[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. *ArXiv:1601.06733 [Cs]*. http://arxiv.org/abs/1601.06733\n\n[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. *2009 IEEE Conference on Computer Vision and Pattern Recognition*, 248–255.\n\n[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *ArXiv:1810.04805 [Cs]*. http://arxiv.org/abs/1810.04805\n\n[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ArXiv:2010.11929 [Cs]*. http://arxiv.org/abs/2010.11929\n\n[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*, 249–256. http://proceedings.mlr.press/v9/glorot10a.html\n\n[14]         Goodfellow, I. (2016). *Generative Adversarial Networks (GANs)*. 86.\n\n[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep Learning*. MIT Press.\n\n[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. *ArXiv:2103.00112 [Cs]*. http://arxiv.org/abs/2103.00112\n\n[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. *ArXiv:1512.03385 [Cs]*. http://arxiv.org/abs/1512.03385\n\n[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. *ArXiv:1704.04861 [Cs]*. http://arxiv.org/abs/1704.04861\n\n[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. *ArXiv:1709.01507 [Cs]*. http://arxiv.org/abs/1709.01507\n\n[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. *ArXiv:1608.06993 [Cs]*. http://arxiv.org/abs/1608.06993\n\n[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 603–612.\n\n[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. *ArXiv Preprint ArXiv:2008.02496*.\n\n[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), *Advances in Neural Information Processing Systems 25* (pp. 1097–1105). Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). *Colorization Transformer*. International Conference on Learning Representations. https://openreview.net/forum?id=5NA1PinlGFu\n\n[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. *Neural Computation*, *1*(4), 541–551. https://doi.org/10.1162/neco.1989.1.4.541\n\n[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. *ArXiv:1903.06586 [Cs]*. http://arxiv.org/abs/1903.06586\n\n[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). *Fourier Neural Operator for Parametric Partial Differential Equations*. https://arxiv.org/abs/2010.08895v1\n\n[28]         Lu Chi. (2020). Fast Fourier Convolution. *Neural Information Processing Systems*, 767–774. https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\n\n[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. *ArXiv:1508.04025 [Cs]*. http://arxiv.org/abs/1508.04025\n\n[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. *Proceedings of the 27th International Conference on International Conference on Machine Learning*, 807–814.\n\n[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. *ArXiv:1807.06514 [Cs]*. http://arxiv.org/abs/1807.06514\n\n[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *Undefined*. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\n\n[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). *Improving language understanding by generative pre-training*.\n\n[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. *ArXiv:1804.02767 [Cs]*. http://arxiv.org/abs/1804.02767\n\n[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. *ArXiv:1506.01497 [Cs]*. http://arxiv.org/abs/1506.01497\n\n[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. *ArXiv:1409.0575 [Cs]*. http://arxiv.org/abs/1409.0575\n\n[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. *ArXiv:1409.1556 [Cs]*. http://arxiv.org/abs/1409.1556\n\n[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. *ArXiv:2101.11605 [Cs]*. http://arxiv.org/abs/2101.11605\n\n[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. *ArXiv E-Prints*, *1505*, arXiv:1505.00387.\n\n[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. *ArXiv:1707.02968 [Cs]*. http://arxiv.org/abs/1707.02968\n\n[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. *ArXiv:2011.10881 [Cs]*. http://arxiv.org/abs/2011.10881\n\n[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. *ArXiv:1409.4842 [Cs]*. http://arxiv.org/abs/1409.4842\n\n[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). *Rethinking the Inception Architecture for Computer Vision*. 2818–2826. http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\n\n[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers & distillation through attention. *ArXiv:2012.12877 [Cs]*. http://arxiv.org/abs/2012.12877\n\n[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), *Advances in Neural Information Processing Systems 30* (pp. 5998–6008). Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n\n[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. *ArXiv:2012.00759 [Cs]*. http://arxiv.org/abs/2012.00759\n\n[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. *European Conference on Computer Vision*, 108–126.\n\n[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. *2018 IEEE Winter Conference on Applications of Computer Vision (WACV)*, 1451–1460. https://doi.org/10.1109/WACV.2018.00163\n\n[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. *ArXiv:2102.12122 [Cs]*. http://arxiv.org/abs/2102.12122\n\n[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. *ArXiv:1711.07971 [Cs]*. http://arxiv.org/abs/1711.07971\n\n[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. *ArXiv:2011.14503 [Cs]*. http://arxiv.org/abs/2011.14503\n\n[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). *CBAM: Convolutional Block Attention Module*. 3–19. https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\n\n[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. *ArXiv:2006.04139 [Cs]*. http://arxiv.org/abs/2006.04139\n\n[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. *ArXiv:2101.11986 [Cs]*. http://arxiv.org/abs/2101.11986\n\n[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. *ArXiv:2007.09451 [Cs]*. http://arxiv.org/abs/2007.09451\n\n[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. *ArXiv:2012.09164 [Cs]*. http://arxiv.org/abs/2012.09164\n\n[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. *ArXiv:2011.09315 [Cs]*. http://arxiv.org/abs/2011.09315\n\n[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. *ArXiv:2012.15840 [Cs]*. http://arxiv.org/abs/2012.15840\n\n[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. *ArXiv:2010.04159 [Cs]*. http://arxiv.org/abs/2010.04159\n\n \n\n \n\n","source":"_posts/Survey-vision-transformer.md","raw":"---\ntitle: A Survey for Vision Transformers (By Mar. 2021)\ndate: 2021-04-25 13:02:11\ncategories: Course project \ntags: \n    - Survey\n    - Vision transformer\n    - Computer vision\n    - Course project\n---\n\n**ABSTRACT:** Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.\n\n<!-- more -->\n\n**KEYWORDS:** *Self-attention, transformer, convolution neural networks, deep neural networks.*\n\n# 1.  Introduction\n\nConvolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.\n\nDespite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. \n\nAt the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. \n\nInspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). \n\nEven though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? \n\nThe structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.\n\n# 2.  Related work\n\nVisio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.\n\n## 2.1 CNN\n\nThere are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. \n\nIn 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. \n\nIn 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. \n\nConsidering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. \n\nIn 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.\n\n![img](clip_image001.png)\n\nFigure 1. DenseNet (G. Huang et al., 2016)\n\nIt is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. \n\n## 2.2 Attention in CNN\n\nConsidering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. \n\n![img](clip_image002.png)\n\nFigure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)\n\nInspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. \n\n![img](clip_image003.png)\n\nFigure 3. Selective Kernel Convolution. (X. Li et al., 2019)\n\nSome researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. \n\nOther researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. \n\n![img](clip_image004.png)\n\nFigure 4. Non-local block. (X. Wang et al., 2018)\n\nAlthough NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. \n\nTo summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.\n\nTable 1. Model comparison for the application of attention in CNN.\n\n\n\n| **Model name** | **Characteristics**                                  |\n| -------------- | ---------------------------------------------------- |\n| SENet          | Spatial aggregation, channel attention               |\n| SKNet          | Adaptive receptive filed, channel attention          |\n| BAM            | Channel attention and spatial attention in parallel  |\n| CBAM           | Channel attention and spatial attention sequentially |\n| NLNet          | Spatial weighted sum per pixel                       |\n| GCNet          | Spatial weighted sum, channel attention              |\n\n \n\n## 2.3 Self-attention\n\nSelf-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. \n\n![img](clip_image005.png)\n\nFigure 5. Self-attention operation.\n\nThe calculation of self-attention can be formulated as a single function: \n\n\n\n| ![img](clip_image007.png) | (1)  |\n| ------------------------- | ---- |\n|                           |      |\n\nThe first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of ![img](clip_image009.png) and ![img](clip_image011.png) aims to calculate the similarity/distance between two different vectors. The score of ![img](clip_image013.png) can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. \n\nIt is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix ![img](clip_image011.png),value matrix ![img](clip_image016.png) and query matrix ![img](clip_image009.png) come from. For the encoder-decoder attention layer, ![img](clip_image019.png)and ![img](clip_image016.png) come from the encoder module and ![img](clip_image009.png) comes from the previous layer. Other operations are the same. \n\nThe limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:\n\n\n\n| ![img](clip_image023.png) | (2)  |\n| ------------------------- | ---- |\n|                           |      |\n\nwhere ![img](clip_image025.png), ![img](clip_image027.png), ![img](clip_image029.png), and ![img](clip_image031.png)are the concatenation of ![img](clip_image033.png). \n\n![img](clip_image034.png)\n\nFigure 6. Multi-head attention. (Vaswani et al., 2017)\n\n## 2.4 Fourier-based network\n\nConvolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. \n\nAnother network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.\n\n![img](clip_image035.png)\n\nFigure 7. Fourier Layer. (Z. Li et al., 2020)\n\nThere are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.\n\n# 3.  Topic and its applications\n\nIn this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. \n\n## 3.1 Transformer\n\nIn the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. \n\n![img](clip_image036.png)\n\nFigure 8. The architecture for transformer. (Vaswani et al., 2017)\n\nThe first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. \n\nThe second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:\n\n\n\n| ![img](clip_image038.png), | (3)  |\n| -------------------------- | ---- |\n|                            |      |\n\nwhere ![img](clip_image040.png)and ![img](clip_image042.png) are weights for the two linear transformation layers, ![img](clip_image044.png) is ReLU activation function. \n\nThe third thing is Add & Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. \n\nThe last thing is the output layer. After passing through ![img](clip_image046.png)decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.\n\nTo summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.\n\n## 3.2 Applications\n\nSince 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.\n\nInspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. \n\n## 3.2.1  Image classification\n\nViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image ![img](clip_image048.png) as a sequence of flattened 2D patches ![img](clip_image048.png), where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. \n\n![img](clip_image050.png)\n\nFigure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)\n\nViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. \n\n![img](clip_image051.png)\n\nFigure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)\n\nViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. \n\nAnother vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.\n\n![img](clip_image053.png)\n\nFigure 11. T2T architecture and feature visualization. (Yuan et al., 2021)\n\nViewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. \n\n![img](clip_image054.png)\n\nFigure 12. TNT block and TNT framework. (Han et al., 2021)\n\nBottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.\n\n![img](clip_image055.png)\n\nFigure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)\n\n## 3.2.2  Object detection\n\nCompared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. \n\n![img](clip_image056.png)\n\nFigure 14. The architecture for DETR. (Carion et al., 2020)\n\nTo solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.\n\nPyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. \n\nAdaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. \n\nSun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.\n\n\n\n## 3.2.3  Segmentation\n\nSegmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. \n\n![img](clip_image057.png)\n\nFigure 15. The architecture for VisTR. (Y. Wang et al., 2020)\n\nPoint Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. \n\n\n\n# 4.  Classification and comparison of various methods\n\nTransformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. \n\n \n\n| **Task**                      | **Category**                            | **Method**                                                  | **Characteristics**                                          |\n| ----------------------------- | --------------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------ |\n| Image classification          | Transformer + global attention          | ViT   (Dosovitskiy  et al., 2020)                           | Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset |\n| Image classification          | Transformer + CNN                       | DeiT   (Touvron  et al., 2021)                              | Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher |\n| Image classification          | Transformer + global attention          | T2T-ViT   (Yuan et  al., 2021)                              | Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet |\n| Image classification          | Transformer + global attention          | TNT   (Han et  al., 2021)                                   | Operate on patch-level and pixel-level, more diverse  feature extraction |\n| Object detection              | Transformer + CNN                       | DETR   (Carion  et al., 2020)                               | End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge |\n| Object detection              | Transformer + CNN + efficient attention | Deformable DETR   (Zhu et  al., 2020)                       | Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time |\n| Segmentation                  | Transformer + CNN                       | MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)             | ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture |\n| Segmentation                  | Transformer + CNN                       | VisTR   (Y. Wang  et al., 2020)                             | instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction |\n| Image processing              | Transformer + CNN                       | Image processing transformer  (IPT) (H. Chen  et al., 2020) | The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning |\n| Image colorization            | Transformer + global attention          | Colorization Transformer (ColTran) (Kumar et  al., 2020)    | Conditional transformer layers, two parallel networks to upsample  low resolution images |\n| Segmentation                  | Transformer + efficient attention       | Criss-Cross Network (CCNet)                                 | Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance, |\n| NLP, architecture exploration | Transformer + efficient attention       | ConvBERT (Jiang et al., 2020)                               | Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies |\n\n \n\n# 5.  Future work\n\nTransformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. \n\nThe first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. \n\nThe second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. \n\nThe third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.\n\nThe fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. \n\n# 6.  Conclusion\n\nIn this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.\n\n# 7.  Course feedbacks\n\nThis course is a great graduate-level class. There are several reasons for that:\n\n\\1.   **Rich course content.** From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. \n\n\\2.   **Excellent lectures.** Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.\n\n\\3.   **Novel and creative illustration**. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.\n\n\\4.   **Excellent reading recommendation**. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.\n\n\\5.   For every lecture, professor always pointed out the “**take away home message**”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.\n\n\\6.   **The slides are clear and well-organized.** I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.\n\n# 8.  Reference\n\n[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. *ArXiv:2012.09958 [Cs]*. http://arxiv.org/abs/2012.09958\n\n[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. *IEEE Transactions on Neural Networks*, *5*(2), 157–166. https://doi.org/10.1109/72.279181\n\n[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). *Language Models are Few-Shot Learners*. https://arxiv.org/abs/2005.14165v4\n\n[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). *GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond*. 0–0. https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\n\n[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. *ArXiv:2005.12872 [Cs]*. http://arxiv.org/abs/2005.12872\n\n[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. *ArXiv:2012.00364 [Cs]*. http://arxiv.org/abs/2012.00364\n\n[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. *International Conference on Machine Learning*, 1691–1703. http://proceedings.mlr.press/v119/chen20s.html\n\n[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. *ArXiv:1606.03657 [Cs, Stat]*. http://arxiv.org/abs/1606.03657\n\n[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. *ArXiv:1601.06733 [Cs]*. http://arxiv.org/abs/1601.06733\n\n[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. *2009 IEEE Conference on Computer Vision and Pattern Recognition*, 248–255.\n\n[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *ArXiv:1810.04805 [Cs]*. http://arxiv.org/abs/1810.04805\n\n[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *ArXiv:2010.11929 [Cs]*. http://arxiv.org/abs/2010.11929\n\n[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. *Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics*, 249–256. http://proceedings.mlr.press/v9/glorot10a.html\n\n[14]         Goodfellow, I. (2016). *Generative Adversarial Networks (GANs)*. 86.\n\n[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep Learning*. MIT Press.\n\n[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. *ArXiv:2103.00112 [Cs]*. http://arxiv.org/abs/2103.00112\n\n[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. *ArXiv:1512.03385 [Cs]*. http://arxiv.org/abs/1512.03385\n\n[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. *ArXiv:1704.04861 [Cs]*. http://arxiv.org/abs/1704.04861\n\n[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. *ArXiv:1709.01507 [Cs]*. http://arxiv.org/abs/1709.01507\n\n[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. *ArXiv:1608.06993 [Cs]*. http://arxiv.org/abs/1608.06993\n\n[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 603–612.\n\n[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. *ArXiv Preprint ArXiv:2008.02496*.\n\n[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), *Advances in Neural Information Processing Systems 25* (pp. 1097–1105). Curran Associates, Inc. http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n\n[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). *Colorization Transformer*. International Conference on Learning Representations. https://openreview.net/forum?id=5NA1PinlGFu\n\n[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. *Neural Computation*, *1*(4), 541–551. https://doi.org/10.1162/neco.1989.1.4.541\n\n[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. *ArXiv:1903.06586 [Cs]*. http://arxiv.org/abs/1903.06586\n\n[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). *Fourier Neural Operator for Parametric Partial Differential Equations*. https://arxiv.org/abs/2010.08895v1\n\n[28]         Lu Chi. (2020). Fast Fourier Convolution. *Neural Information Processing Systems*, 767–774. https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\n\n[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. *ArXiv:1508.04025 [Cs]*. http://arxiv.org/abs/1508.04025\n\n[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. *Proceedings of the 27th International Conference on International Conference on Machine Learning*, 807–814.\n\n[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. *ArXiv:1807.06514 [Cs]*. http://arxiv.org/abs/1807.06514\n\n[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. *Undefined*. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe\n\n[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). *Improving language understanding by generative pre-training*.\n\n[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. *ArXiv:1804.02767 [Cs]*. http://arxiv.org/abs/1804.02767\n\n[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. *ArXiv:1506.01497 [Cs]*. http://arxiv.org/abs/1506.01497\n\n[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. *ArXiv:1409.0575 [Cs]*. http://arxiv.org/abs/1409.0575\n\n[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. *ArXiv:1409.1556 [Cs]*. http://arxiv.org/abs/1409.1556\n\n[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. *ArXiv:2101.11605 [Cs]*. http://arxiv.org/abs/2101.11605\n\n[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. *ArXiv E-Prints*, *1505*, arXiv:1505.00387.\n\n[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. *ArXiv:1707.02968 [Cs]*. http://arxiv.org/abs/1707.02968\n\n[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. *ArXiv:2011.10881 [Cs]*. http://arxiv.org/abs/2011.10881\n\n[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. *ArXiv:1409.4842 [Cs]*. http://arxiv.org/abs/1409.4842\n\n[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). *Rethinking the Inception Architecture for Computer Vision*. 2818–2826. http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\n\n[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers & distillation through attention. *ArXiv:2012.12877 [Cs]*. http://arxiv.org/abs/2012.12877\n\n[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), *Advances in Neural Information Processing Systems 30* (pp. 5998–6008). Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n\n[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. *ArXiv:2012.00759 [Cs]*. http://arxiv.org/abs/2012.00759\n\n[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. *European Conference on Computer Vision*, 108–126.\n\n[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. *2018 IEEE Winter Conference on Applications of Computer Vision (WACV)*, 1451–1460. https://doi.org/10.1109/WACV.2018.00163\n\n[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. *ArXiv:2102.12122 [Cs]*. http://arxiv.org/abs/2102.12122\n\n[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. *ArXiv:1711.07971 [Cs]*. http://arxiv.org/abs/1711.07971\n\n[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. *ArXiv:2011.14503 [Cs]*. http://arxiv.org/abs/2011.14503\n\n[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). *CBAM: Convolutional Block Attention Module*. 3–19. https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\n\n[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. *ArXiv:2006.04139 [Cs]*. http://arxiv.org/abs/2006.04139\n\n[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. *ArXiv:2101.11986 [Cs]*. http://arxiv.org/abs/2101.11986\n\n[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. *ArXiv:2007.09451 [Cs]*. http://arxiv.org/abs/2007.09451\n\n[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. *ArXiv:2012.09164 [Cs]*. http://arxiv.org/abs/2012.09164\n\n[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. *ArXiv:2011.09315 [Cs]*. http://arxiv.org/abs/2011.09315\n\n[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. *ArXiv:2012.15840 [Cs]*. http://arxiv.org/abs/2012.15840\n\n[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. *ArXiv:2010.04159 [Cs]*. http://arxiv.org/abs/2010.04159\n\n \n\n \n\n","slug":"Survey-vision-transformer","published":1,"updated":"2021-04-25T05:54:50.539Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cknzeeym8001bemzwgkiq4kn6","content":"<p><strong>ABSTRACT:</strong> Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.</p>\n<a id=\"more\"></a>\n\n<p><strong>KEYWORDS:</strong> <em>Self-attention, transformer, convolution neural networks, deep neural networks.</em></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.  Introduction\"></a>1.  Introduction</h1><p>Convolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.</p>\n<p>Despite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. </p>\n<p>At the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. </p>\n<p>Inspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). </p>\n<p>Even though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? </p>\n<p>The structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.</p>\n<h1 id=\"2-Related-work\"><a href=\"#2-Related-work\" class=\"headerlink\" title=\"2.  Related work\"></a>2.  Related work</h1><p>Visio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.</p>\n<h2 id=\"2-1-CNN\"><a href=\"#2-1-CNN\" class=\"headerlink\" title=\"2.1 CNN\"></a>2.1 CNN</h2><p>There are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. </p>\n<p>In 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. </p>\n<p>In 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. </p>\n<p>Considering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. </p>\n<p>In 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.</p>\n<p><img src=\"clip_image001.png\" alt=\"img\"></p>\n<p>Figure 1. DenseNet (G. Huang et al., 2016)</p>\n<p>It is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. </p>\n<h2 id=\"2-2-Attention-in-CNN\"><a href=\"#2-2-Attention-in-CNN\" class=\"headerlink\" title=\"2.2 Attention in CNN\"></a>2.2 Attention in CNN</h2><p>Considering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. </p>\n<p><img src=\"clip_image002.png\" alt=\"img\"></p>\n<p>Figure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)</p>\n<p>Inspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. </p>\n<p><img src=\"clip_image003.png\" alt=\"img\"></p>\n<p>Figure 3. Selective Kernel Convolution. (X. Li et al., 2019)</p>\n<p>Some researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. </p>\n<p>Other researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. </p>\n<p><img src=\"clip_image004.png\" alt=\"img\"></p>\n<p>Figure 4. Non-local block. (X. Wang et al., 2018)</p>\n<p>Although NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. </p>\n<p>To summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.</p>\n<p>Table 1. Model comparison for the application of attention in CNN.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Model name</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SENet</td>\n<td>Spatial aggregation, channel attention</td>\n</tr>\n<tr>\n<td>SKNet</td>\n<td>Adaptive receptive filed, channel attention</td>\n</tr>\n<tr>\n<td>BAM</td>\n<td>Channel attention and spatial attention in parallel</td>\n</tr>\n<tr>\n<td>CBAM</td>\n<td>Channel attention and spatial attention sequentially</td>\n</tr>\n<tr>\n<td>NLNet</td>\n<td>Spatial weighted sum per pixel</td>\n</tr>\n<tr>\n<td>GCNet</td>\n<td>Spatial weighted sum, channel attention</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-3-Self-attention\"><a href=\"#2-3-Self-attention\" class=\"headerlink\" title=\"2.3 Self-attention\"></a>2.3 Self-attention</h2><p>Self-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. </p>\n<p><img src=\"clip_image005.png\" alt=\"img\"></p>\n<p>Figure 5. Self-attention operation.</p>\n<p>The calculation of self-attention can be formulated as a single function: </p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image007.png\" alt=\"img\"></th>\n<th>(1)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of <img src=\"clip_image009.png\" alt=\"img\"> and <img src=\"clip_image011.png\" alt=\"img\"> aims to calculate the similarity/distance between two different vectors. The score of <img src=\"clip_image013.png\" alt=\"img\"> can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. </p>\n<p>It is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix <img src=\"clip_image011.png\" alt=\"img\">,value matrix <img src=\"clip_image016.png\" alt=\"img\"> and query matrix <img src=\"clip_image009.png\" alt=\"img\"> come from. For the encoder-decoder attention layer, <img src=\"clip_image019.png\" alt=\"img\">and <img src=\"clip_image016.png\" alt=\"img\"> come from the encoder module and <img src=\"clip_image009.png\" alt=\"img\"> comes from the previous layer. Other operations are the same. </p>\n<p>The limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image023.png\" alt=\"img\"></th>\n<th>(2)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image025.png\" alt=\"img\">, <img src=\"clip_image027.png\" alt=\"img\">, <img src=\"clip_image029.png\" alt=\"img\">, and <img src=\"clip_image031.png\" alt=\"img\">are the concatenation of <img src=\"clip_image033.png\" alt=\"img\">. </p>\n<p><img src=\"clip_image034.png\" alt=\"img\"></p>\n<p>Figure 6. Multi-head attention. (Vaswani et al., 2017)</p>\n<h2 id=\"2-4-Fourier-based-network\"><a href=\"#2-4-Fourier-based-network\" class=\"headerlink\" title=\"2.4 Fourier-based network\"></a>2.4 Fourier-based network</h2><p>Convolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. </p>\n<p>Another network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.</p>\n<p><img src=\"clip_image035.png\" alt=\"img\"></p>\n<p>Figure 7. Fourier Layer. (Z. Li et al., 2020)</p>\n<p>There are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.</p>\n<h1 id=\"3-Topic-and-its-applications\"><a href=\"#3-Topic-and-its-applications\" class=\"headerlink\" title=\"3.  Topic and its applications\"></a>3.  Topic and its applications</h1><p>In this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. </p>\n<h2 id=\"3-1-Transformer\"><a href=\"#3-1-Transformer\" class=\"headerlink\" title=\"3.1 Transformer\"></a>3.1 Transformer</h2><p>In the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. </p>\n<p><img src=\"clip_image036.png\" alt=\"img\"></p>\n<p>Figure 8. The architecture for transformer. (Vaswani et al., 2017)</p>\n<p>The first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. </p>\n<p>The second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image038.png\" alt=\"img\">,</th>\n<th>(3)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image040.png\" alt=\"img\">and <img src=\"clip_image042.png\" alt=\"img\"> are weights for the two linear transformation layers, <img src=\"clip_image044.png\" alt=\"img\"> is ReLU activation function. </p>\n<p>The third thing is Add &amp; Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. </p>\n<p>The last thing is the output layer. After passing through <img src=\"clip_image046.png\" alt=\"img\">decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.</p>\n<p>To summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.</p>\n<h2 id=\"3-2-Applications\"><a href=\"#3-2-Applications\" class=\"headerlink\" title=\"3.2 Applications\"></a>3.2 Applications</h2><p>Since 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.</p>\n<p>Inspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. </p>\n<h2 id=\"3-2-1-Image-classification\"><a href=\"#3-2-1-Image-classification\" class=\"headerlink\" title=\"3.2.1  Image classification\"></a>3.2.1  Image classification</h2><p>ViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image <img src=\"clip_image048.png\" alt=\"img\"> as a sequence of flattened 2D patches <img src=\"clip_image048.png\" alt=\"img\">, where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. </p>\n<p><img src=\"clip_image050.png\" alt=\"img\"></p>\n<p>Figure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)</p>\n<p>ViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. </p>\n<p><img src=\"clip_image051.png\" alt=\"img\"></p>\n<p>Figure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)</p>\n<p>ViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. </p>\n<p>Another vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.</p>\n<p><img src=\"clip_image053.png\" alt=\"img\"></p>\n<p>Figure 11. T2T architecture and feature visualization. (Yuan et al., 2021)</p>\n<p>Viewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. </p>\n<p><img src=\"clip_image054.png\" alt=\"img\"></p>\n<p>Figure 12. TNT block and TNT framework. (Han et al., 2021)</p>\n<p>Bottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.</p>\n<p><img src=\"clip_image055.png\" alt=\"img\"></p>\n<p>Figure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)</p>\n<h2 id=\"3-2-2-Object-detection\"><a href=\"#3-2-2-Object-detection\" class=\"headerlink\" title=\"3.2.2  Object detection\"></a>3.2.2  Object detection</h2><p>Compared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. </p>\n<p><img src=\"clip_image056.png\" alt=\"img\"></p>\n<p>Figure 14. The architecture for DETR. (Carion et al., 2020)</p>\n<p>To solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.</p>\n<p>Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. </p>\n<p>Adaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. </p>\n<p>Sun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.</p>\n<h2 id=\"3-2-3-Segmentation\"><a href=\"#3-2-3-Segmentation\" class=\"headerlink\" title=\"3.2.3  Segmentation\"></a>3.2.3  Segmentation</h2><p>Segmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. </p>\n<p><img src=\"clip_image057.png\" alt=\"img\"></p>\n<p>Figure 15. The architecture for VisTR. (Y. Wang et al., 2020)</p>\n<p>Point Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. </p>\n<h1 id=\"4-Classification-and-comparison-of-various-methods\"><a href=\"#4-Classification-and-comparison-of-various-methods\" class=\"headerlink\" title=\"4.  Classification and comparison of various methods\"></a>4.  Classification and comparison of various methods</h1><p>Transformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. </p>\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong>Category</strong></th>\n<th><strong>Method</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>ViT   (Dosovitskiy  et al., 2020)</td>\n<td>Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + CNN</td>\n<td>DeiT   (Touvron  et al., 2021)</td>\n<td>Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>T2T-ViT   (Yuan et  al., 2021)</td>\n<td>Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>TNT   (Han et  al., 2021)</td>\n<td>Operate on patch-level and pixel-level, more diverse  feature extraction</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN</td>\n<td>DETR   (Carion  et al., 2020)</td>\n<td>End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN + efficient attention</td>\n<td>Deformable DETR   (Zhu et  al., 2020)</td>\n<td>Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)</td>\n<td>ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>VisTR   (Y. Wang  et al., 2020)</td>\n<td>instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction</td>\n</tr>\n<tr>\n<td>Image processing</td>\n<td>Transformer + CNN</td>\n<td>Image processing transformer  (IPT) (H. Chen  et al., 2020)</td>\n<td>The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning</td>\n</tr>\n<tr>\n<td>Image colorization</td>\n<td>Transformer + global attention</td>\n<td>Colorization Transformer (ColTran) (Kumar et  al., 2020)</td>\n<td>Conditional transformer layers, two parallel networks to upsample  low resolution images</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + efficient attention</td>\n<td>Criss-Cross Network (CCNet)</td>\n<td>Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance,</td>\n</tr>\n<tr>\n<td>NLP, architecture exploration</td>\n<td>Transformer + efficient attention</td>\n<td>ConvBERT (Jiang et al., 2020)</td>\n<td>Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies</td>\n</tr>\n</tbody></table>\n<h1 id=\"5-Future-work\"><a href=\"#5-Future-work\" class=\"headerlink\" title=\"5.  Future work\"></a>5.  Future work</h1><p>Transformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. </p>\n<p>The first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. </p>\n<p>The second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. </p>\n<p>The third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.</p>\n<p>The fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. </p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.  Conclusion\"></a>6.  Conclusion</h1><p>In this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.</p>\n<h1 id=\"7-Course-feedbacks\"><a href=\"#7-Course-feedbacks\" class=\"headerlink\" title=\"7.  Course feedbacks\"></a>7.  Course feedbacks</h1><p>This course is a great graduate-level class. There are several reasons for that:</p>\n<p>\\1.   <strong>Rich course content.</strong> From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. </p>\n<p>\\2.   <strong>Excellent lectures.</strong> Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.</p>\n<p>\\3.   <strong>Novel and creative illustration</strong>. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.</p>\n<p>\\4.   <strong>Excellent reading recommendation</strong>. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.</p>\n<p>\\5.   For every lecture, professor always pointed out the “<strong>take away home message</strong>”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.</p>\n<p>\\6.   <strong>The slides are clear and well-organized.</strong> I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.</p>\n<h1 id=\"8-Reference\"><a href=\"#8-Reference\" class=\"headerlink\" title=\"8.  Reference\"></a>8.  Reference</h1><p>[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. <em>ArXiv:2012.09958 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09958\">http://arxiv.org/abs/2012.09958</a></p>\n<p>[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks</em>, <em>5</em>(2), 157–166. <a href=\"https://doi.org/10.1109/72.279181\">https://doi.org/10.1109/72.279181</a></p>\n<p>[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). <em>Language Models are Few-Shot Learners</em>. <a href=\"https://arxiv.org/abs/2005.14165v4\">https://arxiv.org/abs/2005.14165v4</a></p>\n<p>[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). <em>GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond</em>. 0–0. <a href=\"https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\">https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html</a></p>\n<p>[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. <em>ArXiv:2005.12872 [Cs]</em>. <a href=\"http://arxiv.org/abs/2005.12872\">http://arxiv.org/abs/2005.12872</a></p>\n<p>[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. <em>ArXiv:2012.00364 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00364\">http://arxiv.org/abs/2012.00364</a></p>\n<p>[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. <em>International Conference on Machine Learning</em>, 1691–1703. <a href=\"http://proceedings.mlr.press/v119/chen20s.html\">http://proceedings.mlr.press/v119/chen20s.html</a></p>\n<p>[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. <em>ArXiv:1606.03657 [Cs, Stat]</em>. <a href=\"http://arxiv.org/abs/1606.03657\">http://arxiv.org/abs/1606.03657</a></p>\n<p>[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. <em>ArXiv:1601.06733 [Cs]</em>. <a href=\"http://arxiv.org/abs/1601.06733\">http://arxiv.org/abs/1601.06733</a></p>\n<p>[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255.</p>\n<p>[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>ArXiv:1810.04805 [Cs]</em>. <a href=\"http://arxiv.org/abs/1810.04805\">http://arxiv.org/abs/1810.04805</a></p>\n<p>[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>ArXiv:2010.11929 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.11929\">http://arxiv.org/abs/2010.11929</a></p>\n<p>[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–256. <a href=\"http://proceedings.mlr.press/v9/glorot10a.html\">http://proceedings.mlr.press/v9/glorot10a.html</a></p>\n<p>[14]         Goodfellow, I. (2016). <em>Generative Adversarial Networks (GANs)</em>. 86.</p>\n<p>[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>\n<p>[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. <em>ArXiv:2103.00112 [Cs]</em>. <a href=\"http://arxiv.org/abs/2103.00112\">http://arxiv.org/abs/2103.00112</a></p>\n<p>[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. <em>ArXiv:1512.03385 [Cs]</em>. <a href=\"http://arxiv.org/abs/1512.03385\">http://arxiv.org/abs/1512.03385</a></p>\n<p>[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. <em>ArXiv:1704.04861 [Cs]</em>. <a href=\"http://arxiv.org/abs/1704.04861\">http://arxiv.org/abs/1704.04861</a></p>\n<p>[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. <em>ArXiv:1709.01507 [Cs]</em>. <a href=\"http://arxiv.org/abs/1709.01507\">http://arxiv.org/abs/1709.01507</a></p>\n<p>[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. <em>ArXiv:1608.06993 [Cs]</em>. <a href=\"http://arxiv.org/abs/1608.06993\">http://arxiv.org/abs/1608.06993</a></p>\n<p>[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 603–612.</p>\n<p>[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. <em>ArXiv Preprint ArXiv:2008.02496</em>.</p>\n<p>[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), <em>Advances in Neural Information Processing Systems 25</em> (pp. 1097–1105). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<p>[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). <em>Colorization Transformer</em>. International Conference on Learning Representations. <a href=\"https://openreview.net/forum?id=5NA1PinlGFu\">https://openreview.net/forum?id=5NA1PinlGFu</a></p>\n<p>[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. <em>Neural Computation</em>, <em>1</em>(4), 541–551. <a href=\"https://doi.org/10.1162/neco.1989.1.4.541\">https://doi.org/10.1162/neco.1989.1.4.541</a></p>\n<p>[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. <em>ArXiv:1903.06586 [Cs]</em>. <a href=\"http://arxiv.org/abs/1903.06586\">http://arxiv.org/abs/1903.06586</a></p>\n<p>[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). <em>Fourier Neural Operator for Parametric Partial Differential Equations</em>. <a href=\"https://arxiv.org/abs/2010.08895v1\">https://arxiv.org/abs/2010.08895v1</a></p>\n<p>[28]         Lu Chi. (2020). Fast Fourier Convolution. <em>Neural Information Processing Systems</em>, 767–774. <a href=\"https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\">https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf</a></p>\n<p>[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. <em>ArXiv:1508.04025 [Cs]</em>. <a href=\"http://arxiv.org/abs/1508.04025\">http://arxiv.org/abs/1508.04025</a></p>\n<p>[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>, 807–814.</p>\n<p>[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. <em>ArXiv:1807.06514 [Cs]</em>. <a href=\"http://arxiv.org/abs/1807.06514\">http://arxiv.org/abs/1807.06514</a></p>\n<p>[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. <em>Undefined</em>. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</p>\n<p>[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em>Improving language understanding by generative pre-training</em>.</p>\n<p>[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. <em>ArXiv:1804.02767 [Cs]</em>. <a href=\"http://arxiv.org/abs/1804.02767\">http://arxiv.org/abs/1804.02767</a></p>\n<p>[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>ArXiv:1506.01497 [Cs]</em>. <a href=\"http://arxiv.org/abs/1506.01497\">http://arxiv.org/abs/1506.01497</a></p>\n<p>[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. <em>ArXiv:1409.0575 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.0575\">http://arxiv.org/abs/1409.0575</a></p>\n<p>[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>ArXiv:1409.1556 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.1556\">http://arxiv.org/abs/1409.1556</a></p>\n<p>[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. <em>ArXiv:2101.11605 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11605\">http://arxiv.org/abs/2101.11605</a></p>\n<p>[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. <em>ArXiv E-Prints</em>, <em>1505</em>, arXiv:1505.00387.</p>\n<p>[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. <em>ArXiv:1707.02968 [Cs]</em>. <a href=\"http://arxiv.org/abs/1707.02968\">http://arxiv.org/abs/1707.02968</a></p>\n<p>[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. <em>ArXiv:2011.10881 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.10881\">http://arxiv.org/abs/2011.10881</a></p>\n<p>[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. <em>ArXiv:1409.4842 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.4842\">http://arxiv.org/abs/1409.4842</a></p>\n<p>[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). <em>Rethinking the Inception Architecture for Computer Vision</em>. 2818–2826. <a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html</a></p>\n<p>[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. <em>ArXiv:2012.12877 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.12877\">http://arxiv.org/abs/2012.12877</a></p>\n<p>[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems 30</em> (pp. 5998–6008). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>\n<p>[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. <em>ArXiv:2012.00759 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00759\">http://arxiv.org/abs/2012.00759</a></p>\n<p>[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. <em>European Conference on Computer Vision</em>, 108–126.</p>\n<p>[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1451–1460. <a href=\"https://doi.org/10.1109/WACV.2018.00163\">https://doi.org/10.1109/WACV.2018.00163</a></p>\n<p>[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. <em>ArXiv:2102.12122 [Cs]</em>. <a href=\"http://arxiv.org/abs/2102.12122\">http://arxiv.org/abs/2102.12122</a></p>\n<p>[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. <em>ArXiv:1711.07971 [Cs]</em>. <a href=\"http://arxiv.org/abs/1711.07971\">http://arxiv.org/abs/1711.07971</a></p>\n<p>[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. <em>ArXiv:2011.14503 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.14503\">http://arxiv.org/abs/2011.14503</a></p>\n<p>[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). <em>CBAM: Convolutional Block Attention Module</em>. 3–19. <a href=\"https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\">https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html</a></p>\n<p>[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. <em>ArXiv:2006.04139 [Cs]</em>. <a href=\"http://arxiv.org/abs/2006.04139\">http://arxiv.org/abs/2006.04139</a></p>\n<p>[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. <em>ArXiv:2101.11986 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11986\">http://arxiv.org/abs/2101.11986</a></p>\n<p>[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. <em>ArXiv:2007.09451 [Cs]</em>. <a href=\"http://arxiv.org/abs/2007.09451\">http://arxiv.org/abs/2007.09451</a></p>\n<p>[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. <em>ArXiv:2012.09164 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09164\">http://arxiv.org/abs/2012.09164</a></p>\n<p>[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. <em>ArXiv:2011.09315 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.09315\">http://arxiv.org/abs/2011.09315</a></p>\n<p>[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. <em>ArXiv:2012.15840 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.15840\">http://arxiv.org/abs/2012.15840</a></p>\n<p>[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. <em>ArXiv:2010.04159 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.04159\">http://arxiv.org/abs/2010.04159</a></p>\n","site":{"data":{"about":{"avatar":"https://vico-image.oss-cn-hongkong.aliyuncs.com/avatar.jpg","name":"Xiaoyu Xie","tag":"Deep Learning","desc":"","skills":{"PyTorch":8,"TensorFlow":5,"Python":8,"Matlab":5,"Java":3,"C++":3},"projects":[{"name":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing","image":"url","tags":["Convolutional neural network","Machine learning","Wavelet transform","Process-structure-properties","Additive manufacturing"],"description":"Mechanistic data-driven prediction of as-built mechanical properties in metal additive manufacturing, Npj Computational Materials, 2021.","link_text":"Github Link","link":"https://github.com/xiaoyuxie-vico/DL-AM"}]},"hint":{"new":{"selector":[".menu-about"]}},"reading":{"define":{"readed":"Have Read","reading":"Reading","wanted":"To Read"},"contents":{"readed":[{"title":"21天学通C++","cover":"https://img9.doubanio.com/view/subject/l/public/s24422798.jpg","review":"非常使用，每个知识点都用了例子进行说明","score":"4/5","doubanLink":"https://book.douban.com/subject/20383712/"}],"reading":[{"title":"Machine Learning Refined","cover":"https://img9.doubanio.com/view/subject/l/public/s33772608.jpg","review":"Best first machine learning for almost everyone","score":"5/5","doubanLink":"https://book.douban.com/subject/35217898/"}],"wanted":[{"title":"Pattern Recognition and Machine Learning","cover":"https://img9.doubanio.com/view/subject/l/public/s4254558.jpg","review":"","score":null,"doubanLink":"https://book.douban.com/subject/2061116/"}]}},"link":{"social":{"github":"https://github.com/xiaoyuxie-vico"},"extern":{"Email":"xiaoyuxie.vico@gmail.com","Linkedin":"https://www.linkedin.com/in/xyxie/"}},"slider":[{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/andreas-gucklhorn-mawU2PoJWfU-unsplash.jpg","align":"center","title":"Welcome to Xiaoyu Xie's Blog","subtitle":"Share knowledge and have fun.","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/michael-baccin-sN4nCLXGiu8-unsplash.jpg","align":"center","title":"This blog is about my researches and life.","subtitle":"","link":"/"},{"image":"https://vico-image.oss-cn-hongkong.aliyuncs.com/background/screen.jpeg","align":"center","title":"Keep Moving!","subtitle":"","link":"/"}]}},"excerpt":"<p><strong>ABSTRACT:</strong> Inspired by the impressive performance of transformer in natural language tasks, a growing number of researchers is exploring vision transformer and trying to apply to their own fields. In this paper, I will give a comprehensive literature review about vision transformer. I start with the background, motivation and paper structure int the first section. Then, I will go through the development and fundamental knowledge of convolution neural networks, self-attention, and Fourier-based networks. After that, I will discuss the transformer structure and its applications and make some classification and comparison of various methods. Finally, I will clarify some difficulties, potential research directions, and give course feedback.</p>","more":"<p><strong>KEYWORDS:</strong> <em>Self-attention, transformer, convolution neural networks, deep neural networks.</em></p>\n<h1 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1.  Introduction\"></a>1.  Introduction</h1><p>Convolutional neural networks (CNN) has dominated computer vision field since the impressive success of a ImageNet Large Scale Visual Recognition Challenge in 2014 (Russakovsky et al., 2015). Various CNN architectures (He et al., 2015; Szegedy et al., 2016; G. Huang et al., 2016; Hu et al., 2017) springs up like mushrooms since that time. These carefully designed CNN architectures have created significant progress in various visual fields, like image classification (Howard et al., 2017; Szegedy et al., 2014; X. Wang et al., 2018), object detection (Redmon and Farhadi, 2018; Ren et al., 2016), semantic segmentation (Z. Huang et al., 2019; P. Wang et al., 2018; S. Zheng et al., 2020), image generation(X. Chen et al., 2016; Goodfellow, 2016), etc. Most of CNN architectures (He et al., 2015; Simonyan and Zisserman, 2014) heavily benefit from different convolution operations like 3×3 convolutions. These convolution operations can learn local information from visual inputs. Deep stacked convolutions then help to gather high level information (Goodfellow et al., 2016), which help to do different image understanding tasks. In about less than a decade, CNN has become the main design paradigm in many modern deep learning systems.</p>\n<p>Despite the great success of CNN, there are some limitations. First, while convolution operations can process a local neighbor in space and time, convolution is hard to capture long-distance dependencies. These dependencies are vital to semantic concepts. For example, a bird break in different angles have to be treated with different convolution operations due to the changes of break position. Generally, we can use different convolution operations, increase kernel sizes, increase model depth, or dilated convolutions to solve this problem. But the computation cost can be high and require careful design. Second, convolution treats all information as equal, but in fact not all pixels should be regarded as equal. For example, when we classify dogs and cats, we pay more attention on their ears and noses and ignore some useless background information. That is to say, image classification should give priority to important foreground objects instead of background. But convolution operations process all local neighborhood information with equal weights and do not have a concept of importance. This also causes inefficiency in computation. </p>\n<p>At the same time, transformer shows its power in natural language processing (NLP) tasks and almost become the standard architecture in NLP tasks. One of important reason is that transformer is good at modeling long distance information. In 2017, Vaswani et at. (Vaswani et al., 2017) first replaced convolutions and recurrence with transformer which only based on attention modules. The architecture allows to train model in a more parallelizable way and significantly decrease the training time. In 2019, Devlin et al. (Devlin et al., 2019) proposed a new language representation model called BERT, which aims to train a deep learning model from unlabeled text by jointly conditioning on all context in all layers. One of the most impressive things is BERT model can be easily transferred to other tasks by just fine tuning one additional output layer. In 2020, Brown et al. (Brown et al., 2020) proposed an autoregressive language model GPT-3 which achieves impressive performance on various NLP tasks, like machine translation and question-answering system. </p>\n<p>Inspired by the significant improvement of transformer in natural language processing (NLP) tasks, a growing number of researchers are trying to take advantage of transformer in computer vision (Carion et al., 2020; Dosovitskiy et al., 2020; W. Wang et al., 2021; Srinivas et al., 2021). Chen et al. (M. Chen et al., 2020) firstly tried to explore the application of transformer in images. They trained a sequence transformer model to predict pixels and found that the model can learn strong image representations. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed a fully-transformer model called Vision Transformer (ViT) that can be directly used in image classification. Vit treats each image as a set of fixed-size patches (14×14 or 16×16) that will be feed into a transformer encoder to make classification. Though ViT achieves excellent results compared to stat-of-the-art CNN models, it heavily relies on a large-size dataset like JFT-300M (C. Sun et al., 2017) (300 million images) and have relative worse performance on a medium size dataset like ImageNet (Deng et al., 2009).  To tackle this problem and make models train effectively on a medium size dataset, Touvron et al. (Touvron et al., 2021) also proposed an efficient convolution-free transformer model called DeiT. DeiT can be trained on a single 8-GPU node in about three days by leveraging a new distillation token and a specific teacher-student strategy. In the early of 2021, various explorations in vision transformer has been proposed, including Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), Transformer in Transformer (TiT) (Han et al., 2021), and Tokens-to-token ViT (Yuan et al., 2021). </p>\n<p>Even though the application of transformer has dominated a variety of NLP tasks since 2017, the application of transformer in CV is just rapidly developed especially since the late 2020. There is still no standard architecture in vision transformer and different models have their own advantages and disadvantages. Almost in every month, there are new proposed architectures about vision transformer. Thus, this paper tries to do a comprehensive survey about the recent vision transformer and compare different architectures in different applications. Specifically, this paper tries to answer four questions in vision transformer: 1) what is transformer? 2) why we need transformer in CV? 3) how transformer is applied in CV? 4) what are existing difficulties and potential development? </p>\n<p>The structure of the paper is as follows. In section 2, foundations and preliminary knowledge about transformer will be introduced. Some close related ideas like conventional CNN architectures, channel attention, self-attention, multi-head self-attention, and non-local network will be discussed. This section tries to discuss the evolution of the design of vision models from convolution to transformer. The first aforementioned question will be discussed in this section. In section 3, the core topic transformer will be discussed in detailed. The structure of transformer and different applications of transformer will be introduced, including image classification, object detection, segmentation, etc. This section will show the power of transformer in vision fields, which will also clarify the second and third question in depth. In section 4, vision transformer will be classified into three categories, including transformer with global attention, transformer plus CNN, and transformer with efficient attention. Section 3 and section 4 will help to answer the third aforementioned question. In section 5, existing problems and difficulties in vision transformer will be discussed. Some potential and valuable future work will also be discussed in this section. This section will answer the fourth aforementioned question. In the last section, I will conclude the development of vision transformer and summary the paper.</p>\n<h1 id=\"2-Related-work\"><a href=\"#2-Related-work\" class=\"headerlink\" title=\"2.  Related work\"></a>2.  Related work</h1><p>Visio transformer closely relates to convolution operation and self-attention. Thus, in this section I will firstly go through the milestone vision architectures since 2012 to explain how researchers designed models and improved performance. Then, I will give an introduction about self-attention, which is a core module in transformer. I will also introduce non-local network and two Fourier-based networks to explain there are other ways to capture long-range dependencies instead of self-attention.</p>\n<h2 id=\"2-1-CNN\"><a href=\"#2-1-CNN\" class=\"headerlink\" title=\"2.1 CNN\"></a>2.1 CNN</h2><p>There are a lot of CNN model taking advantage from a deeper neural network. Going deeper for a neural network is always the first choice for complex tasks. In 1989, LeNet (LeCun et al., 1989) , one of the earliest CNN, was proposed by LeCun et al. It only has 7 layers, including 3 convolutional layers, 2 pooling layers, and 1 fully-connected layer. It defines the fundamental modules for CNN. Trained with backpropagation algorithms, it can be successfully identified handwritten zip code numbers. </p>\n<p>In 2012, AlexNet (Krizhevsky et al., 2012) increased the model depth to 8 layers. It achieved a great improvement (a top-5 error of 15.3%, more than 10.8 percentage points than second one) in ImageNet (Deng et al., 2009). AlexNet shows the power of increase model depth and makes it feasible when use GPU during training. After AlexNet, there are countless number of researchers trying to use Deep CNN to improve performance in their fields. This also greatly improve the development of deep learning in all fields. </p>\n<p>In 2015, VGG (Simonyan and Zisserman, 2014) achieved the state-of-the-art performance on ILSVRC classiﬁcation and localisation tasks. It heavily uses 3×3 convolutions and increases the depth by adding more convolutional layers. </p>\n<p>Considering the significance of the depth, Residual Network (ResNet) (He et al., 2015) tried to explore how to increase the depth and avoid vanishing/exploding gradients (Bengio et al., 1994; Glorot and Bengio, 2010). This is because He et al. found that directly stacked more layers cannot have a better performance. They proposed a residual block and let these layers fit a residual mapping. They increased the depth from 18 layers to 152 layers and increase performance, which is the first time to surpass 100-layer barrier. ResNet has also been a popular backbone in various vision tasks since then. </p>\n<p>In 2016, Densely Connected Convolutional Networks (DenseNet) not only creates short paths from early layers to later layers, it also connects all layers directly with each other.  So, it uses direct connections between any two layers with the same feature-map size to implement this idea. This method also helps to solve the gradient vanishing problem. The maximum depth of DenseNet reached 264. The architecture of DenseNet is shown in Figure 1.</p>\n<p><img src=\"clip_image001.png\" alt=\"img\"></p>\n<p>Figure 1. DenseNet (G. Huang et al., 2016)</p>\n<p>It is needed to note that convolution operations have dominated various visual recognition tasks with a deep neural network since 2012. There are several reasons behind its popular. First, convolution layers can capture favorable visual context using filters. These filters shared weights over the entire image space, so convolution operations have translation equivariance. Second, convolution operation can take advantage of the rapid development of computational resource like GPU because it is inherent parallelable. And GPU allows to train a very deep CNN with fewer time, which helps to train on large-scale image dataset, like ImageNet. Third, multiple kernel paths help to achieve a competitive performance. GoogleNet (Szegedy et al., 2014) carefully designs a Inception module with 1×1 convolution, 3×3 convolution, and 5×5 convolution to increasing the depth and width of the network while keeping the computational budget constant. Fourth, CNN can use hierarchical structure to learn high-level information. This is because spatial and channel-wise information can be fused in a local receptive filed in convolutional layers. The fused information can then be passed to non-linear activation functions or/and down-sampling layers to learn a better representation that is vital to capture global theoretical receptive ﬁelds. </p>\n<h2 id=\"2-2-Attention-in-CNN\"><a href=\"#2-2-Attention-in-CNN\" class=\"headerlink\" title=\"2.2 Attention in CNN\"></a>2.2 Attention in CNN</h2><p>Considering the importance of channel relationship, SENet (Hu et al., 2017) uses a Squeeze-and-Excitation (SE) block to explicitly model the interdependencies between different channels. The architecture can learn different weights for different channels in SE block. It is an effective and lightweight gating mechanism to self-recalibrate the feature map. There are three important operations. First, in the squeeze operation, SE block will shrink features through spatial dimensions. Second, in the excitation operation, it will learn weights to explicitly model channel association. Third, it reweights the feature maps by the learned weights. The SE block is shown in Figure 2. </p>\n<p><img src=\"clip_image002.png\" alt=\"img\"></p>\n<p>Figure 2. A Squeeze-and-Excitation block. (Hu et al., 2017)</p>\n<p>Inspired the SENet’s channel recalibration mechanism and multi-branch convolutional networks (He et al., 2015; Srivastava et al., 2015), SKNet uses a non-linear approach to aggregate multiple scale feature and constructs a dynamic selection mechanism in CNN instead of fixed receptive fields in standard CNN. It has three basic operators – Split, Fuse, and Select, which are shown in Figure 3. Split operator means that they use two filters with different filter sizes. Fuse operator aims to enable neurons to adaptively adjust receptive fields based on the stimulus content. Select operator is a soft attention across channels, which is similar to SENet. This architecture improves the efﬁciency and effectiveness of object recognition. </p>\n<p><img src=\"clip_image003.png\" alt=\"img\"></p>\n<p>Figure 3. Selective Kernel Convolution. (X. Li et al., 2019)</p>\n<p>Some researchers tried to not only use channel attention, but also take into account spatial attention as well. For example, Bottleneck Attention Module (BAM) (Park et al., 2018) and Convolutional Block Attention Module (CBAM) (Woo et al., 2018) were proposed to infer an attention map along two separate dimensions, channel and spatial. But BAM’s attention computes in parallel, while CBAM computes attention sequentially. </p>\n<p>Other researchers only used spatial attention and did not consider channel attention. For example, Non-local Network (NLNet) (X. Wang et al., 2018) tries to capture long-range dependencies by avoiding messages delivered back and forth between distant positions. They proposed non-local operations to have spatial attention. This operation can use a weighted sum of the features to compute the response at a position. They applied NLNet in video classification and achieved competitive performance. The non-local block is shown in Figure 4. </p>\n<p><img src=\"clip_image004.png\" alt=\"img\"></p>\n<p>Figure 4. Non-local block. (X. Wang et al., 2018)</p>\n<p>Although NLNet presents a way to capture long-range information, global contexts are almost the same for different query positions (Cao et al., 2019). So, construct a global context network (GCNet) is proposed to solve this problem by unify the basic idea of SNNet and NLNet, which means it takes into account of spatial and channel information at the same time. </p>\n<p>To summary the application of attention in CNN, the comparison of NLNet, SENet, SKNet, and GCNet, etc. are shown in Table 1.</p>\n<p>Table 1. Model comparison for the application of attention in CNN.</p>\n<table>\n<thead>\n<tr>\n<th><strong>Model name</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>SENet</td>\n<td>Spatial aggregation, channel attention</td>\n</tr>\n<tr>\n<td>SKNet</td>\n<td>Adaptive receptive filed, channel attention</td>\n</tr>\n<tr>\n<td>BAM</td>\n<td>Channel attention and spatial attention in parallel</td>\n</tr>\n<tr>\n<td>CBAM</td>\n<td>Channel attention and spatial attention sequentially</td>\n</tr>\n<tr>\n<td>NLNet</td>\n<td>Spatial weighted sum per pixel</td>\n</tr>\n<tr>\n<td>GCNet</td>\n<td>Spatial weighted sum, channel attention</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-3-Self-attention\"><a href=\"#2-3-Self-attention\" class=\"headerlink\" title=\"2.3 Self-attention\"></a>2.3 Self-attention</h2><p>Self-attention (Vaswani et al., 2017) can transform an input to a better representation by allowing inputs to interact with each other. It will compute three hidden representations called query, key, value firstly by three learnable matrixes. After that, it will compute attention weights by the dot products of query and key. The weights will be divided by the sqrt root of the key’s dimension and obtain the weighted value as output. So, the output of self-attention operation is aggregates of the interactions between all inputs. Figure 5 shows how self-attention works. </p>\n<p><img src=\"clip_image005.png\" alt=\"img\"></p>\n<p>Figure 5. Self-attention operation.</p>\n<p>The calculation of self-attention can be formulated as a single function: </p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image007.png\" alt=\"img\"></th>\n<th>(1)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>The first important thing to understand the mechanism of self-attention is remembering the dimension of the input and output of self-attention is the same. That is to say, our goal is to have a new and better representation for our inputs, and the output needs to be the same dimension. So, what is a good representation for the input? When we are doing machine translation, we understand that even for a same word it can have different or even opposite meaning. What make the word has different meaning? The answer is context. So, we want to consider the whole context and capture the different relationships between different words. Self-attention is designed to do a similar thing. The dot product of <img src=\"clip_image009.png\" alt=\"img\"> and <img src=\"clip_image011.png\" alt=\"img\"> aims to calculate the similarity/distance between two different vectors. The score of <img src=\"clip_image013.png\" alt=\"img\"> can be used to determine how much weights need to be put in a certain input. But before we calculate a weighted value as the output, we need to do some normalization, which is shown in Equation 1. </p>\n<p>It is also need to note that the difference of the encoder-decoder attention layer (Luong et al., 2015) between self-attention layer is where the key matrix <img src=\"clip_image011.png\" alt=\"img\">,value matrix <img src=\"clip_image016.png\" alt=\"img\"> and query matrix <img src=\"clip_image009.png\" alt=\"img\"> come from. For the encoder-decoder attention layer, <img src=\"clip_image019.png\" alt=\"img\">and <img src=\"clip_image016.png\" alt=\"img\"> come from the encoder module and <img src=\"clip_image009.png\" alt=\"img\"> comes from the previous layer. Other operations are the same. </p>\n<p>The limitation for self-attention layer is that it cannot give different weights for different parts of the input. One way to solve that is to use multi-head attention instead of one single-head attention. The structure of multi-head attention is shown in Figure 6. It helps to boost the performance of the vanilla self-attention layer by giving attention layers different representation subspace. Different query, key, and value matrix are used for different heads So, they can learn different representation in training. For example, several heads are good at learning local information. Others are good at learning global information. Specifically, the operation of multi-head attention can be formulated as below:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image023.png\" alt=\"img\"></th>\n<th>(2)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image025.png\" alt=\"img\">, <img src=\"clip_image027.png\" alt=\"img\">, <img src=\"clip_image029.png\" alt=\"img\">, and <img src=\"clip_image031.png\" alt=\"img\">are the concatenation of <img src=\"clip_image033.png\" alt=\"img\">. </p>\n<p><img src=\"clip_image034.png\" alt=\"img\"></p>\n<p>Figure 6. Multi-head attention. (Vaswani et al., 2017)</p>\n<h2 id=\"2-4-Fourier-based-network\"><a href=\"#2-4-Fourier-based-network\" class=\"headerlink\" title=\"2.4 Fourier-based network\"></a>2.4 Fourier-based network</h2><p>Convolution and attention are not the only choice to improve model performance. Fourier analysis tells us that any signal can be represented as a linear combination of sin and cos functions with different coefficients regardless of signal’s dimension. Some researcher recently tried to integrate Short Time Fourier Transformation (STFT) in to vision architectures. For example, Fast Fourier Convolution (FFC) (Lu Chi, 2020) adds a global branch that manipulates image-level spectrum and a semi-global branch to process stacked image patches. This design help to capture non-local receptive fields and cross-scale fusion. </p>\n<p>Another network is call Fourier Neural Operator (FNO) (Z. Li et al., 2020). Although FNO is designed to solve partial differential equations in engineering and science, it is a pure neural network and does not have close relationship with some numerical methods, like Finite Element Analysis or Finite Difference Method. The basic idea is that they build a Fourier Layer/ Block and stacked many Fourier Layers to get a better performance. Specifically, it has two branches. The first branch uses STFT to change hidden representations in spatial domain to frequency domain. Then high frequency modes are removed, which means only low frequency modes remains. The coefficients for each low frequency are learned by the Neural Network. And the frequency information is inversed to the original spatial domain by inverse STFT. The second is 1×1 convolution operation which is designed to let the model learn the residual and help to alleviate the gradient vanishing problem. Figure 7 show the structure of Fourier Layer.</p>\n<p><img src=\"clip_image035.png\" alt=\"img\"></p>\n<p>Figure 7. Fourier Layer. (Z. Li et al., 2020)</p>\n<p>There are several reasons for researchers to use Fourier Analysis in Neural Network. First, the computation is very efficient in a quasi-linear level due to the use of STFT. Second, it helps to capture coarse and global in a more efficient way by using learnable coefficients for different frequencies. Thus, in some applications, we may not need to use stacked convolutions with different size. We can use Fourier Layer to learn a good representation in frequency domain.</p>\n<h1 id=\"3-Topic-and-its-applications\"><a href=\"#3-Topic-and-its-applications\" class=\"headerlink\" title=\"3.  Topic and its applications\"></a>3.  Topic and its applications</h1><p>In this section, I will firstly introduce the details of transformer and then provide a comprehensive review of its applications in different visual tasks, including image classification, object detection, segmentation, etc. </p>\n<h2 id=\"3-1-Transformer\"><a href=\"#3-1-Transformer\" class=\"headerlink\" title=\"3.1 Transformer\"></a>3.1 Transformer</h2><p>In the previous section, I have introduced the core module in transformer, single-head and multi-head attention layer. I will go through other modules and introduce the whole structure of transformer, which is shown in Figure 8. </p>\n<p><img src=\"clip_image036.png\" alt=\"img\"></p>\n<p>Figure 8. The architecture for transformer. (Vaswani et al., 2017)</p>\n<p>The first thing is the positional encoding. Self-attention operation is irrelevant to the position of each input. So, it cannot capture the positional information of inputs. To solve this potential problem, they proposed a positional encoding that is added to the input embedding. </p>\n<p>The second thing is the Feed-forward neural network (FFNN). A FFNN is used after a self-attention layer in each encoder and decoder. This FFNN has two linear transformation layers and a ReLU activation function (Nair and Hinton, 2010). It can be expressed as:</p>\n<table>\n<thead>\n<tr>\n<th><img src=\"clip_image038.png\" alt=\"img\">,</th>\n<th>(3)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<p>where <img src=\"clip_image040.png\" alt=\"img\">and <img src=\"clip_image042.png\" alt=\"img\"> are weights for the two linear transformation layers, <img src=\"clip_image044.png\" alt=\"img\"> is ReLU activation function. </p>\n<p>The third thing is Add &amp; Norm module. To strength the information flow in a deep neural network, a residual connection is added in each module, including FFNN and self-attention. This is a similar design in ResNet (He et al., 2015). To boost the training, a layer-normalization (Cheng et al., 2016) is followed by the residual connection. </p>\n<p>The last thing is the output layer. After passing through <img src=\"clip_image046.png\" alt=\"img\">decoders, the output will be passed to a linear layer and a softmax layer, which help to transform logits vector into word probabilities.</p>\n<p>To summarize, Transformer, an encoder-decoder architecture, is designed to handle sequential data.</p>\n<h2 id=\"3-2-Applications\"><a href=\"#3-2-Applications\" class=\"headerlink\" title=\"3.2 Applications\"></a>3.2 Applications</h2><p>Since 2017, Transformer has achieved great success in various NLP tasks. For example, after pretraining, BERT (Devlin et al., 2019) can be easily used in a wide range of downstream tasks by fine-tuning one newly added output layer. Generative Pre-Trained Transformer models (GPT (Alec Radford et al., 2018), GPT-2 (A. Radford et al., 2019), and GPT-3 (Brown et al., 2020)) are another popular pretrained models in NLP. GPT-3 is designed to directly do multiple tasks without fine-tuning, but it has 175 billion parameters and requires 45 TB compressed plaintext data to train.</p>\n<p>Inspired by the success of transformer in NLP, there are growing number of vision transformers being proposed (Beal et al., 2020; Dosovitskiy et al., 2020; Yang et al., 2020; Zhang et al., 2020; S. Zheng et al., 2020). Researchers also applied vision transformer in different fields. </p>\n<h2 id=\"3-2-1-Image-classification\"><a href=\"#3-2-1-Image-classification\" class=\"headerlink\" title=\"3.2.1  Image classification\"></a>3.2.1  Image classification</h2><p>ViT (Dosovitskiy et al., 2020) is a pure transformer applied directly to image patches. The architecture is shown in Figure 9. They reshape an image <img src=\"clip_image048.png\" alt=\"img\"> as a sequence of flattened 2D patches <img src=\"clip_image048.png\" alt=\"img\">, where H = 14 or 16. Then, a 1D positional embedding added with patch embedding are fed into transformer encoder.  The feature obtained from the first position is regarded as the global image representation and is used to do classification. ViT only uses the encoder of transformer and only has minimal changes compared to a standard transformer in NLP. </p>\n<p><img src=\"clip_image050.png\" alt=\"img\"></p>\n<p>Figure 9. Vision Transformer (ViT). (Dosovitskiy et al., 2020)</p>\n<p>ViT achieved excellent results when pre-trained on a large dataset JFT-300M (300 million images), but it does not generalize well in mid-sized datasets like ImageNet (1.2 million images). One of the reasons is that transformer lack inductive biases inherent to CNN, such as translation equivariance and locality. So, vision transformer needs sufficient data to have a good generalization, which is similar in NLP transformer. Moreover, they compared ViT with a hybrid model that feeds intermediate feature maps into ViT by leveraging CNN model. They found that hybrid model only has better performance than ViT at small computational budgets and the difference vanishes given sufficient data. This shows that CNN may not be a necessary part for a large model. Furthermore, they analyzed the relationship between network depth and mean attention distance, which is shown in Figure 10. They found that some heads can attend at global information even at the lowest layer. </p>\n<p><img src=\"clip_image051.png\" alt=\"img\"></p>\n<p>Figure 10. Attention distance in ViT. (Dosovitskiy et al., 2020)</p>\n<p>ViT needs to be trained on millions of images, which severely hinders its application in different fields. In the early of 2021, DeiT (Touvron et al., 2021) uses a teacher-student strategy and add a distillation token. It is a convolution-free architecture and similar to ViT, but it uses CNN as a teacher and transformer as a student. It is much more efficient and only needs to be trained about 3 days on a single 8-GPU node. To facilitate the training, it uses data augmentation and regularization strategies. These helps to greatly reduces the requirement of data and make it feasible to train on ImageNet instead of JFT-300M. They achieved better performance than ViT and EfficientNet. </p>\n<p>Another vision transformer called Tokens-to-Token ViT (T2T) (Yuan et al., 2021) was proposed in 2021. The architecture of T2T is shown in Figure 11 (a). T2T wants to train model in a more efficient way and avoid the reliance on a large-size dataset. They pointed out two problems in ViT. First, splitting each image into a sequence of tokens fails to capture key local structures like edges or lines. Second, the backbone of ViT is not suitable for vision tasks due to the redundancy in the attention module. From Figure 11 (b) , we can find that T2T can capture local structure features (green box), but ViT capture invalid feature maps (red box). Thus, they proposed a progressive tokenization module to replace the tokenization method in ViT. Their method helps to capture local structure information from neighbor tokens and achieve length reduction of tokens. They also compared different backbone from existing CNN architectures and found that “deep-narrow” architecture design with fewer channels but more layers in ViT is the best one.</p>\n<p><img src=\"clip_image053.png\" alt=\"img\"></p>\n<p>Figure 11. T2T architecture and feature visualization. (Yuan et al., 2021)</p>\n<p>Viewing images as a sequence of patches ignore the structure information between patches. To solve this problem, Transformer in Transformer (TNT), a pure transformer network, handles images in patch-level and pixel level. The architecture of TNT is shown in Figure 12. TNT uses two transformer blocks. The first one is an outer transformer block which is used to process patch embedding. The other one is an inner transformer block which is designed to caoture local features from pixel level. In a similar computational cost, TNT’s performance is 1.5% higher than that of DeiT on ImageNet. They also compared visualization of learned features between DeiT and TNT. Compared to DeiT, TNT can preserve better local information and have more diverse features. </p>\n<p><img src=\"clip_image054.png\" alt=\"img\"></p>\n<p>Figure 12. TNT block and TNT framework. (Han et al., 2021)</p>\n<p>Bottleneck Transformers Network (BoTNet) is also a backbone architecture. It only modifies the final three bottleneck blocks of ResNet with global self-attention, which is shown in Figure 13. It has two main stages. In the first stage, they use convolution operations to learn high-level feature maps. In the second stage, they use global self-attention to aggregate information. Based on such design, it is easy to handle large images (1024×1024) in an efficient way. They pointed out that the designed block can be regarded as transformer block. BoTNet is quite similar to NLNet. The first difference is that BoTNet uses multi-head self-attention but NLNet does not always use it in the implementation. The second difference is that non-local block is used as an additional block, while bottleneck block is used to replace convolution block.</p>\n<p><img src=\"clip_image055.png\" alt=\"img\"></p>\n<p>Figure 13. A comparison of ResNet bottleneck and bottleneck Transformer. (Srinivas et al., 2021)</p>\n<h2 id=\"3-2-2-Object-detection\"><a href=\"#3-2-2-Object-detection\" class=\"headerlink\" title=\"3.2.2  Object detection\"></a>3.2.2  Object detection</h2><p>Compared to image classification, object detection requires models to predict object labels and their corresponding bounding boxes. Detection Transformer (DETR) is the first end-to-end transformer architecture used in object detection. It removes the many traditional hand-designed components like region proposal network (RPN) and non-maximal suppression (NMS). It also can predict all objects at once, which is different to RNN that predicts sequence elements one by one. DETR uses a set-based global loss which performs bipartite matching between predicted and ground-truth objects to train the model. It does not require any customized layers, which makes it easily reproduced in any framework. It achieves comparable results to Faster R-CNN (Ren et al., 2016). But it has two main limitations. First, it takes a long time to converge. This is because the attention module has uniform attention at the start of training and needs more epochs to update to optimal attention weights. Second, it easily fails to detect small objects due to limited feature spatial resolution. </p>\n<p><img src=\"clip_image056.png\" alt=\"img\"></p>\n<p>Figure 14. The architecture for DETR. (Carion et al., 2020)</p>\n<p>To solve the drawbacks of DETR, Deformable DETR uses more efficient attention module that only attend on some key sampling points around the reference point. In this way, it greatly reduces the computational cost. Also, the deformable attention module cam fuse multi-scale features. It achieves better performance with 10 times less training epochs compared to the original DETR model.</p>\n<p>Pyramid Vision Transformer (PVT) (W. Wang et al., 2021), a recent pure transformer backbone, also aims to capture multi-scale features maps for dense predictions. Compared to ViT, PVT is trained on dense partition of images (the patch size is 4) to get high resolution outputs. To prevent the high computational costs, PVT uses a progressive shrinking pyramid and adopts a spatial -reduction attention layer. They applied PVT in image classification, object detection, and sematic segmentation and achieved a competitive performance. PVT also can be easily combined with DETR. The combination model is called PVT+DETR, which is an entirely convolution-free network. It has better performance than DETR plus ResNet50. </p>\n<p>Adaptive Clustering Transformer (ACT) (M. Zheng et al., 2020) is also designed to solve DETR’s high computational cost in training and testing. ACT cluster the query features using Locality Sensitive Hashing (LSH) and use the prototype-key interaction to approximate the query-key relationship. ACT greatly reduce the computation cost while just reduce little accuracy. </p>\n<p>Sun et al. (Z. Sun et al., 2020) pointed out that the slow convergence of the original DETR model comes from the Hungarian loss and the Transformer cross-attention. They proposed two possible solutions: TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). These two models achieve better performance than the original DETR model.</p>\n<h2 id=\"3-2-3-Segmentation\"><a href=\"#3-2-3-Segmentation\" class=\"headerlink\" title=\"3.2.3  Segmentation\"></a>3.2.3  Segmentation</h2><p>Segmentation requires model to have pixel-level understanding, which is different from image classification and object detection. Video Instance Segmentation with Transformers (VisTR) (Y. Wang et al., 2020)  is an end-to-end parallel sequence prediction network. The pipeline of VisTR is shown in Figure 15. VisTR takes a sequence of images from a video as input and make instance segmentation prediction for each image directly.  VisTR is designed to learn pixel-level and instance-level similarity. It can track instances seamlessly and naturally. A CNN model is used to extract feature maps from a sequence of images. After adding with positional encoding, extracted feature maps will be fed into encoder and decoder. Finally, VisTR will output the final mask sequences by an instance sequence segmentation. </p>\n<p><img src=\"clip_image057.png\" alt=\"img\"></p>\n<p>Figure 15. The architecture for VisTR. (Y. Wang et al., 2020)</p>\n<p>Point Transformer (Zhao et al., 2020) is a backbone architecture for 3D point cloud understanding tasks, including semantic scene segmentation, object part segmentation, and object classiﬁcation. They designed a Point Transformer layer which is invariant to permutation and cardinality. It outperforms the state-of-the-art model in S3DIS dataset for large -scale semantic scene segmentation. Axial-DeepLab (H. Wang, Zhu, Green, et al., 2020) is proposed to convert 2D self-attention as two 1D axial-attention layers which are applied to height and width sequentially. This design greatly reduces the computation and allow to capture non-local interactions easily. There is also a model called Attention-Based Transformers that is designed to instance segmentation of cells in microstructures. It uses long skip connections between the CNN encoder and CNN decoder. It achieves fast and accurate cell instance segmentation. </p>\n<h1 id=\"4-Classification-and-comparison-of-various-methods\"><a href=\"#4-Classification-and-comparison-of-various-methods\" class=\"headerlink\" title=\"4.  Classification and comparison of various methods\"></a>4.  Classification and comparison of various methods</h1><p>Transformer structure uses self-attention module to long-range information, while convolutions use hierarchical structure to learn high-level features. Generally, researchers try to explore architectures in these two different ways and take advantage from each other. Another important category is efficient transformer because transformer normally requires more data than standard CNN. </p>\n<table>\n<thead>\n<tr>\n<th><strong>Task</strong></th>\n<th><strong>Category</strong></th>\n<th><strong>Method</strong></th>\n<th><strong>Characteristics</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>ViT   (Dosovitskiy  et al., 2020)</td>\n<td>Only transformer encoder, patching embedding, and  positional encoding;  High computational cost, heavily rely on large-size dataset</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + CNN</td>\n<td>DeiT   (Touvron  et al., 2021)</td>\n<td>Teacher-student strategy, distillation token, data  augmentation and regularization strategies;  Rely on the CNN model to be a teacher</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>T2T-ViT   (Yuan et  al., 2021)</td>\n<td>Progressively structurize images to tokens, deep-narrow  structure, train from scratch on ImageNet</td>\n</tr>\n<tr>\n<td>Image classification</td>\n<td>Transformer + global attention</td>\n<td>TNT   (Han et  al., 2021)</td>\n<td>Operate on patch-level and pixel-level, more diverse  feature extraction</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN</td>\n<td>DETR   (Carion  et al., 2020)</td>\n<td>End-to-end learning, set-based prediction, remove  hand-design modules;  Struggle on small objects, slow to converge</td>\n</tr>\n<tr>\n<td>Object detection</td>\n<td>Transformer + CNN + efficient attention</td>\n<td>Deformable DETR   (Zhu et  al., 2020)</td>\n<td>Better performance on small objects, faster convergence  than DETR;  Two stage detector architecture, use data augmentation in  test time</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>MaX-DeepLab (H. Wang,  Zhu, Adam, et al., 2020)</td>\n<td>ﬁrst end-to-end model for panoptic segmentation, mask  transformer, bipartite matching, dual-path architecture</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + CNN</td>\n<td>VisTR   (Y. Wang  et al., 2020)</td>\n<td>instance sequence matching and segmentation, direct  end-to-end parallel sequence prediction</td>\n</tr>\n<tr>\n<td>Image processing</td>\n<td>Transformer + CNN</td>\n<td>Image processing transformer  (IPT) (H. Chen  et al., 2020)</td>\n<td>The multi-head and multi-trail structure, generate a large  amount of corrupted image pairs images by add noising, rain streak, or down-sampling;  can be used in different tasks in one model after ﬁne-tuning</td>\n</tr>\n<tr>\n<td>Image colorization</td>\n<td>Transformer + global attention</td>\n<td>Colorization Transformer (ColTran) (Kumar et  al., 2020)</td>\n<td>Conditional transformer layers, two parallel networks to upsample  low resolution images</td>\n</tr>\n<tr>\n<td>Segmentation</td>\n<td>Transformer + efficient attention</td>\n<td>Criss-Cross Network (CCNet)</td>\n<td>Criss-Cross Attention block, high efficiency and low  computation, The state-of-the-art performance,</td>\n</tr>\n<tr>\n<td>NLP, architecture exploration</td>\n<td>Transformer + efficient attention</td>\n<td>ConvBERT (Jiang et al., 2020)</td>\n<td>Span-based dynamic  convolution, mixed attention block to capture both global and local dependencies</td>\n</tr>\n</tbody></table>\n<h1 id=\"5-Future-work\"><a href=\"#5-Future-work\" class=\"headerlink\" title=\"5.  Future work\"></a>5.  Future work</h1><p>Transformer is still a young field and has different architectures proposed in almost every week. There are several important problems or difficulties that are worth to be noticed. </p>\n<p>The first one is to discover the mechanism of transformer. Since Google proposed transformer architecture in 2017, countless papers have been using this idea in different fields, from NLP to CV or to medical researches. But recently, Google also published a paper which points out that “Attention is not all you need: pure attention loses rank doubly exponentially with depth”. They decompose the outputs of self-attention layer into a sum of smaller terms and found that skip connections or multi-layer perceptron are vital to the final performance. We need to understand why and how transformer works. Only when we have a better understanding about that, we can know the direction of improvement and development. </p>\n<p>The second one is to build efficient vision transformer. There are several ways to accomplish that. For example, we can take advantages from convolution operations which has inductive biases, including translation invariance, weight sharing, scale invariance. Or we can build effective attention block to capture different scale information in a better way. Or we can discover the efficient to use training strategies (data augmentation or regularization) to avoid the reliance on large-size dataset. This problem is the core research direction for vision transformer for a long time since in practical we generally cannot find such large dataset. </p>\n<p>The third one is to reduce model complexity. Small and efficient model can be easily extended in different fields. Distillation maybe a choice to make transformer models more feasible in practice. Some useful techniques such as selective or sparse attention, low-rank matrix in attention, and efficient cluster attention may give some insights to solve this problem.</p>\n<p>The fourth one is to explore more backbone architecture for visual inputs instead of directly using transformer structure in NLP tasks. The last one is to discover more models that can do multiple tasks in one model. </p>\n<h1 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6.  Conclusion\"></a>6.  Conclusion</h1><p>In this survey, I give a comprehensive review about vision transformer. I introduce the development of the depth exploration in CNN and go through the researches in channel and spatial attention. I also introduce the all related modules in transformer structure, including self-attention, MLP, FFNN, and layer normalization. Then the application of vision transformer and their classification and comparison are discussed. I also list some potential and useful future research directions at the end.</p>\n<h1 id=\"7-Course-feedbacks\"><a href=\"#7-Course-feedbacks\" class=\"headerlink\" title=\"7.  Course feedbacks\"></a>7.  Course feedbacks</h1><p>This course is a great graduate-level class. There are several reasons for that:</p>\n<p>\\1.   <strong>Rich course content.</strong> From PCA to Bayesian, all important topics are covered in this 9-week course. It is amazing and really give me a good overview in this field. </p>\n<p>\\2.   <strong>Excellent lectures.</strong> Even though I toke class by watching video, I can feel that professor try to explain concept in detail rather quickly go through various concepts and equations. For some hard concepts or math-heavy part, professor is always willing to draw some lines or circles in patient, which really help me to understand them.</p>\n<p>\\3.   <strong>Novel and creative illustration</strong>. Even though I think I have learned PCA from other course or videos, I am surprised to learn new things from professor’s explanation. More importantly, most of other courses only cover PCA and do not introduce LDA. Only after this class, I finally understand that when do I need to choose PCA or LDA. Another example is backpropagation. This is my first time to learn sensitivity in backpropagation, which really help me understand more than other courses. I never heard it, although I have learned some course or read some books before.</p>\n<p>\\4.   <strong>Excellent reading recommendation</strong>. Professor have kindly given us some reading list in each lecture to help us learn more things. They are really helpful and help me to understand deeply.</p>\n<p>\\5.   For every lecture, professor always pointed out the “<strong>take away home message</strong>”. Personally, I really love this part. This short message really helps me to focus on the most important part and avoid loss from some relative trivial concepts.</p>\n<p>\\6.   <strong>The slides are clear and well-organized.</strong> I really love this class’s slides. The have a lot of sentence and organized very well, which help to in the lecture and review.</p>\n<h1 id=\"8-Reference\"><a href=\"#8-Reference\" class=\"headerlink\" title=\"8.  Reference\"></a>8.  Reference</h1><p>[1] Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., and Kislyuk, D. (2020). Toward Transformer-Based Object Detection. <em>ArXiv:2012.09958 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09958\">http://arxiv.org/abs/2012.09958</a></p>\n<p>[2] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks</em>, <em>5</em>(2), 157–166. <a href=\"https://doi.org/10.1109/72.279181\">https://doi.org/10.1109/72.279181</a></p>\n<p>[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). <em>Language Models are Few-Shot Learners</em>. <a href=\"https://arxiv.org/abs/2005.14165v4\">https://arxiv.org/abs/2005.14165v4</a></p>\n<p>[4] Cao, Y., Xu, J., Lin, S., Wei, F., and Hu, H. (2019). <em>GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond</em>. 0–0. <a href=\"https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html\">https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Cao_GCNet_Non-Local_Networks_Meet_Squeeze-Excitation_Networks_and_Beyond_ICCVW_2019_paper.html</a></p>\n<p>[5] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). End-to-End Object Detection with Transformers. <em>ArXiv:2005.12872 [Cs]</em>. <a href=\"http://arxiv.org/abs/2005.12872\">http://arxiv.org/abs/2005.12872</a></p>\n<p>[6] Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., and Gao, W. (2020). Pre-Trained Image Processing Transformer. <em>ArXiv:2012.00364 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00364\">http://arxiv.org/abs/2012.00364</a></p>\n<p>[7] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). Generative Pretraining From Pixels. <em>International Conference on Machine Learning</em>, 1691–1703. <a href=\"http://proceedings.mlr.press/v119/chen20s.html\">http://proceedings.mlr.press/v119/chen20s.html</a></p>\n<p>[8] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. <em>ArXiv:1606.03657 [Cs, Stat]</em>. <a href=\"http://arxiv.org/abs/1606.03657\">http://arxiv.org/abs/1606.03657</a></p>\n<p>[9] Cheng, J., Dong, L., and Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. <em>ArXiv:1601.06733 [Cs]</em>. <a href=\"http://arxiv.org/abs/1601.06733\">http://arxiv.org/abs/1601.06733</a></p>\n<p>[10]         Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–255.</p>\n<p>[11]         Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>ArXiv:1810.04805 [Cs]</em>. <a href=\"http://arxiv.org/abs/1810.04805\">http://arxiv.org/abs/1810.04805</a></p>\n<p>[12]         Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. <em>ArXiv:2010.11929 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.11929\">http://arxiv.org/abs/2010.11929</a></p>\n<p>[13]         Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 249–256. <a href=\"http://proceedings.mlr.press/v9/glorot10a.html\">http://proceedings.mlr.press/v9/glorot10a.html</a></p>\n<p>[14]         Goodfellow, I. (2016). <em>Generative Adversarial Networks (GANs)</em>. 86.</p>\n<p>[15]         Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>\n<p>[16]         Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y. (2021). Transformer in Transformer. <em>ArXiv:2103.00112 [Cs]</em>. <a href=\"http://arxiv.org/abs/2103.00112\">http://arxiv.org/abs/2103.00112</a></p>\n<p>[17]         He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep Residual Learning for Image Recognition. <em>ArXiv:1512.03385 [Cs]</em>. <a href=\"http://arxiv.org/abs/1512.03385\">http://arxiv.org/abs/1512.03385</a></p>\n<p>[18]         Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. (2017). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. <em>ArXiv:1704.04861 [Cs]</em>. <a href=\"http://arxiv.org/abs/1704.04861\">http://arxiv.org/abs/1704.04861</a></p>\n<p>[19]         Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2017). Squeeze-and-Excitation Networks. <em>ArXiv:1709.01507 [Cs]</em>. <a href=\"http://arxiv.org/abs/1709.01507\">http://arxiv.org/abs/1709.01507</a></p>\n<p>[20]         Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. <em>ArXiv:1608.06993 [Cs]</em>. <a href=\"http://arxiv.org/abs/1608.06993\">http://arxiv.org/abs/1608.06993</a></p>\n<p>[21]         Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W. (2019). Ccnet: Criss-cross attention for semantic segmentation. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 603–612.</p>\n<p>[22]         Jiang, Z., Yu, W., Zhou, D., Chen, Y., Feng, J., and Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. <em>ArXiv Preprint ArXiv:2008.02496</em>.</p>\n<p>[23]         Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (Eds.), <em>Advances in Neural Information Processing Systems 25</em> (pp. 1097–1105). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>\n<p>[24]         Kumar, M., Weissenborn, D., and Kalchbrenner, N. (2020, September 28). <em>Colorization Transformer</em>. International Conference on Learning Representations. <a href=\"https://openreview.net/forum?id=5NA1PinlGFu\">https://openreview.net/forum?id=5NA1PinlGFu</a></p>\n<p>[25]         LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation Applied to Handwritten Zip Code Recognition. <em>Neural Computation</em>, <em>1</em>(4), 541–551. <a href=\"https://doi.org/10.1162/neco.1989.1.4.541\">https://doi.org/10.1162/neco.1989.1.4.541</a></p>\n<p>[26]         Li, X., Wang, W., Hu, X., and Yang, J. (2019). Selective Kernel Networks. <em>ArXiv:1903.06586 [Cs]</em>. <a href=\"http://arxiv.org/abs/1903.06586\">http://arxiv.org/abs/1903.06586</a></p>\n<p>[27]         Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. (2020). <em>Fourier Neural Operator for Parametric Partial Differential Equations</em>. <a href=\"https://arxiv.org/abs/2010.08895v1\">https://arxiv.org/abs/2010.08895v1</a></p>\n<p>[28]         Lu Chi. (2020). Fast Fourier Convolution. <em>Neural Information Processing Systems</em>, 767–774. <a href=\"https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf\">https://proceedings.neurips.cc/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf</a></p>\n<p>[29]         Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. <em>ArXiv:1508.04025 [Cs]</em>. <a href=\"http://arxiv.org/abs/1508.04025\">http://arxiv.org/abs/1508.04025</a></p>\n<p>[30]         Nair, V., and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>, 807–814.</p>\n<p>[31]         Park, J., Woo, S., Lee, J.-Y., and Kweon, I. S. (2018). BAM: Bottleneck Attention Module. <em>ArXiv:1807.06514 [Cs]</em>. <a href=\"http://arxiv.org/abs/1807.06514\">http://arxiv.org/abs/1807.06514</a></p>\n<p>[32]         Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. <em>Undefined</em>. /paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</p>\n<p>[33]         Radford, Alec, Narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em>Improving language understanding by generative pre-training</em>.</p>\n<p>[34]         Redmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. <em>ArXiv:1804.02767 [Cs]</em>. <a href=\"http://arxiv.org/abs/1804.02767\">http://arxiv.org/abs/1804.02767</a></p>\n<p>[35]         Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. <em>ArXiv:1506.01497 [Cs]</em>. <a href=\"http://arxiv.org/abs/1506.01497\">http://arxiv.org/abs/1506.01497</a></p>\n<p>[36]         Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. <em>ArXiv:1409.0575 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.0575\">http://arxiv.org/abs/1409.0575</a></p>\n<p>[37]         Simonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>ArXiv:1409.1556 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.1556\">http://arxiv.org/abs/1409.1556</a></p>\n<p>[38]         Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. <em>ArXiv:2101.11605 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11605\">http://arxiv.org/abs/2101.11605</a></p>\n<p>[39]         Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway Networks. <em>ArXiv E-Prints</em>, <em>1505</em>, arXiv:1505.00387.</p>\n<p>[40]         Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. <em>ArXiv:1707.02968 [Cs]</em>. <a href=\"http://arxiv.org/abs/1707.02968\">http://arxiv.org/abs/1707.02968</a></p>\n<p>[41]         Sun, Z., Cao, S., Yang, Y., and Kitani, K. (2020). Rethinking Transformer-based Set Prediction for Object Detection. <em>ArXiv:2011.10881 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.10881\">http://arxiv.org/abs/2011.10881</a></p>\n<p>[42]         Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going Deeper with Convolutions. <em>ArXiv:1409.4842 [Cs]</em>. <a href=\"http://arxiv.org/abs/1409.4842\">http://arxiv.org/abs/1409.4842</a></p>\n<p>[43]         Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). <em>Rethinking the Inception Architecture for Computer Vision</em>. 2818–2826. <a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html\">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html</a></p>\n<p>[44]         Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. <em>ArXiv:2012.12877 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.12877\">http://arxiv.org/abs/2012.12877</a></p>\n<p>[45]         Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems 30</em> (pp. 5998–6008). Curran Associates, Inc. <a href=\"http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></p>\n<p>[46]         Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. (2020). MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. <em>ArXiv:2012.00759 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.00759\">http://arxiv.org/abs/2012.00759</a></p>\n<p>[47]         Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., and Chen, L.-C. (2020). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. <em>European Conference on Computer Vision</em>, 108–126.</p>\n<p>[48]         Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., and Cottrell, G. (2018). Understanding Convolution for Semantic Segmentation. <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1451–1460. <a href=\"https://doi.org/10.1109/WACV.2018.00163\">https://doi.org/10.1109/WACV.2018.00163</a></p>\n<p>[49]         Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. (2021). Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. <em>ArXiv:2102.12122 [Cs]</em>. <a href=\"http://arxiv.org/abs/2102.12122\">http://arxiv.org/abs/2102.12122</a></p>\n<p>[50]         Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local Neural Networks. <em>ArXiv:1711.07971 [Cs]</em>. <a href=\"http://arxiv.org/abs/1711.07971\">http://arxiv.org/abs/1711.07971</a></p>\n<p>[51]         Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H. (2020). End-to-End Video Instance Segmentation with Transformers. <em>ArXiv:2011.14503 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.14503\">http://arxiv.org/abs/2011.14503</a></p>\n<p>[52]         Woo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). <em>CBAM: Convolutional Block Attention Module</em>. 3–19. <a href=\"https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html\">https://openaccess.thecvf.com/content_ECCV_2018/html/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.html</a></p>\n<p>[53]         Yang, F., Yang, H., Fu, J., Lu, H., and Guo, B. (2020). Learning Texture Transformer Network for Image Super-Resolution. <em>ArXiv:2006.04139 [Cs]</em>. <a href=\"http://arxiv.org/abs/2006.04139\">http://arxiv.org/abs/2006.04139</a></p>\n<p>[54]         Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F. E., Feng, J., and Yan, S. (2021). Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. <em>ArXiv:2101.11986 [Cs]</em>. <a href=\"http://arxiv.org/abs/2101.11986\">http://arxiv.org/abs/2101.11986</a></p>\n<p>[55]         Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., and Sun, Q. (2020). Feature Pyramid Transformer. <em>ArXiv:2007.09451 [Cs]</em>. <a href=\"http://arxiv.org/abs/2007.09451\">http://arxiv.org/abs/2007.09451</a></p>\n<p>[56]         Zhao, H., Jiang, L., Jia, J., Torr, P., and Koltun, V. (2020). Point Transformer. <em>ArXiv:2012.09164 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.09164\">http://arxiv.org/abs/2012.09164</a></p>\n<p>[57]         Zheng, M., Gao, P., Wang, X., Li, H., and Dong, H. (2020). End-to-End Object Detection with Adaptive Clustering Transformer. <em>ArXiv:2011.09315 [Cs]</em>. <a href=\"http://arxiv.org/abs/2011.09315\">http://arxiv.org/abs/2011.09315</a></p>\n<p>[58]         Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., and Zhang, L. (2020). Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. <em>ArXiv:2012.15840 [Cs]</em>. <a href=\"http://arxiv.org/abs/2012.15840\">http://arxiv.org/abs/2012.15840</a></p>\n<p>[59]         Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable DETR: Deformable Transformers for End-to-End Object Detection. <em>ArXiv:2010.04159 [Cs]</em>. <a href=\"http://arxiv.org/abs/2010.04159\">http://arxiv.org/abs/2010.04159</a></p>"}],"PostAsset":[{"_id":"source/_posts/CS61B-Week2/image-20210425113258793.png","post":"cknzeeylb0001emzw8va12fh8","slug":"image-20210425113258793.png","modified":1,"renderable":1},{"_id":"source/_posts/CS61B-Week2/sllist_last_pointer.png","post":"cknzeeylb0001emzw8va12fh8","slug":"sllist_last_pointer.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/1.png","post":"cknzeeyln0007emzwavll6jqj","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/2.png","post":"cknzeeyln0007emzwavll6jqj","slug":"2.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/3.png","post":"cknzeeyln0007emzwavll6jqj","slug":"3.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/4.png","post":"cknzeeyln0007emzwavll6jqj","slug":"4.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/5.png","post":"cknzeeyln0007emzwavll6jqj","slug":"5.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/6.png","post":"cknzeeyln0007emzwavll6jqj","slug":"6.png","modified":1,"renderable":1},{"_id":"source/_posts/Robust-Image-Classification-with-Kernel-Density-Function/7.png","post":"cknzeeyln0007emzwavll6jqj","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214173356481.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214173356481.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214173840928.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214173840928.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174003005.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174003005.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174108194.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174108194.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174310749.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174310749.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174422276.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174422276.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174809740.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174809740.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174913278.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174913278.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214174957751.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214174957751.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214175022879.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214175022879.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214175304898.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214175304898.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214182334199.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214182334199.png","modified":1,"renderable":1},{"_id":"source/_posts/notes-ML-FM/image-20210214182520461.png","post":"cknzeeym0000zemzw9hzzdocn","slug":"image-20210214182520461.png","modified":1,"renderable":1},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/image-20210426120448703.png","post":"cknzeeym40014emzwa34u08b4","slug":"image-20210426120448703.png","modified":1,"renderable":1},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/image-20210426120534698.png","post":"cknzeeym40014emzwa34u08b4","slug":"image-20210426120534698.png","modified":1,"renderable":1},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_4.png","post":"cknzeeym40014emzwa34u08b4","slug":"output_31_4.png","modified":1,"renderable":1},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_5.png","post":"cknzeeym40014emzwa34u08b4","slug":"output_31_5.png","modified":1,"renderable":1},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_958.png","post":"cknzeeym40014emzwa34u08b4","slug":"output_31_958.png","modified":1,"renderable":1},{"_id":"source/_posts/GANs-specialization-Week1-Notes-and-codes/output_31_959.png","post":"cknzeeym40014emzwa34u08b4","slug":"output_31_959.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image001.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image001.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image002.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image002.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image003.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image003.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image004.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image004.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image005.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image005.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image007.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image007.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image009.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image009.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image011.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image011.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image013.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image013.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image016.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image016.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image019.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image019.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image023.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image023.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image025.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image025.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image027.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image027.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image029.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image029.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image031.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image031.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image033.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image033.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image034.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image034.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image035.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image035.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image036.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image036.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image038.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image038.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image040.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image040.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image042.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image042.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image044.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image044.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image046.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image046.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image048.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image048.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image050.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image050.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image051.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image051.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image053.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image053.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image054.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image054.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image055.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image055.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image056.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image056.png","modified":1,"renderable":1},{"_id":"source/_posts/Survey-vision-transformer/clip_image057.png","post":"cknzeeym8001bemzwgkiq4kn6","slug":"clip_image057.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cknzeeylb0001emzw8va12fh8","category_id":"cknzeeyli0004emzwcqogcipu","_id":"cknzeeylq000bemzwbyrkd2j3"},{"post_id":"cknzeeylg0003emzwfg7y19ax","category_id":"cknzeeylp0009emzwbfve7azk","_id":"cknzeeyls000eemzw5xqu7ttu"},{"post_id":"cknzeeyln0007emzwavll6jqj","category_id":"cknzeeylr000cemzwdobw86mg","_id":"cknzeeylt000gemzw7x1ncjvi"},{"post_id":"cknzeeym0000zemzw9hzzdocn","category_id":"cknzeeylp0009emzwbfve7azk","_id":"cknzeeym20012emzw25eidhxi"},{"post_id":"cknzeeym40014emzwa34u08b4","category_id":"cknzeeyli0004emzwcqogcipu","_id":"cknzeeym60016emzwee04ccfb"},{"post_id":"cknzeeym8001bemzwgkiq4kn6","category_id":"cknzeeylr000cemzwdobw86mg","_id":"cknzeeymd001demzw4kgwga89"}],"PostTag":[{"post_id":"cknzeeylb0001emzw8va12fh8","tag_id":"cknzeeyll0005emzw7wo95tpi","_id":"cknzeeylt000iemzwef9lfp87"},{"post_id":"cknzeeylb0001emzw8va12fh8","tag_id":"cknzeeylq000aemzwgzou0ca0","_id":"cknzeeylu000jemzwexjr7a1k"},{"post_id":"cknzeeylb0001emzw8va12fh8","tag_id":"cknzeeyls000demzw8wtec5xn","_id":"cknzeeylu000lemzw42xm03te"},{"post_id":"cknzeeylb0001emzw8va12fh8","tag_id":"cknzeeyls000femzwbwek61ew","_id":"cknzeeylu000memzweevw6jtv"},{"post_id":"cknzeeylg0003emzwfg7y19ax","tag_id":"cknzeeylt000hemzw8ej77uns","_id":"cknzeeylw000qemzwf9zx1tli"},{"post_id":"cknzeeylg0003emzwfg7y19ax","tag_id":"cknzeeylu000kemzwgwx20wtg","_id":"cknzeeylw000remzw17d2glp5"},{"post_id":"cknzeeylg0003emzwfg7y19ax","tag_id":"cknzeeylu000nemzw4a3b7e26","_id":"cknzeeyly000temzwc8v69xwq"},{"post_id":"cknzeeylg0003emzwfg7y19ax","tag_id":"cknzeeylv000oemzwc2t66p5w","_id":"cknzeeyly000uemzw07jwbg0t"},{"post_id":"cknzeeyln0007emzwavll6jqj","tag_id":"cknzeeylv000pemzw0e6igjam","_id":"cknzeeylz000wemzwcct35xm6"},{"post_id":"cknzeeyln0007emzwavll6jqj","tag_id":"cknzeeylu000kemzwgwx20wtg","_id":"cknzeeylz000xemzwb9ys17km"},{"post_id":"cknzeeyln0007emzwavll6jqj","tag_id":"cknzeeyly000vemzw6dbrf7n5","_id":"cknzeeylz000yemzw5ev536d8"},{"post_id":"cknzeeym0000zemzw9hzzdocn","tag_id":"cknzeeylt000hemzw8ej77uns","_id":"cknzeeym20010emzwckbi6ovk"},{"post_id":"cknzeeym0000zemzw9hzzdocn","tag_id":"cknzeeylu000kemzwgwx20wtg","_id":"cknzeeym20011emzw20gj58f1"},{"post_id":"cknzeeym0000zemzw9hzzdocn","tag_id":"cknzeeylu000nemzw4a3b7e26","_id":"cknzeeym20013emzwgyx08g93"},{"post_id":"cknzeeym40014emzwa34u08b4","tag_id":"cknzeeym50015emzweoin27rl","_id":"cknzeeym60018emzw2vt1dcj7"},{"post_id":"cknzeeym40014emzwa34u08b4","tag_id":"cknzeeym60017emzw6l0tebol","_id":"cknzeeym60019emzw0aiodp59"},{"post_id":"cknzeeym40014emzwa34u08b4","tag_id":"cknzeeyls000femzwbwek61ew","_id":"cknzeeym6001aemzw06a9h2ue"},{"post_id":"cknzeeym8001bemzwgkiq4kn6","tag_id":"cknzeeymb001cemzwdiiqh735","_id":"cknzeeymi001hemzw61hjft81"},{"post_id":"cknzeeym8001bemzwgkiq4kn6","tag_id":"cknzeeyme001eemzwhm438eim","_id":"cknzeeymi001iemzw3evlfjsp"},{"post_id":"cknzeeym8001bemzwgkiq4kn6","tag_id":"cknzeeymg001femzwhga83qty","_id":"cknzeeymj001jemzw40fbbq8s"},{"post_id":"cknzeeym8001bemzwgkiq4kn6","tag_id":"cknzeeymi001gemzwf0jf8d0d","_id":"cknzeeymj001kemzwfvu7ewqe"}],"Tag":[{"name":"Online Course","_id":"cknzeeyll0005emzw7wo95tpi"},{"name":"Algorithm","_id":"cknzeeylq000aemzwgzou0ca0"},{"name":"Data structure","_id":"cknzeeyls000demzw8wtec5xn"},{"name":"Notes","_id":"cknzeeyls000femzwbwek61ew"},{"name":"Machine Learning","_id":"cknzeeylt000hemzw8ej77uns"},{"name":"Deep Learning","_id":"cknzeeylu000kemzwgwx20wtg"},{"name":"Fluid Mechanics","_id":"cknzeeylu000nemzw4a3b7e26"},{"name":"Partial Differential Equations","_id":"cknzeeylv000oemzwc2t66p5w"},{"name":"Class Project","_id":"cknzeeylv000pemzw0e6igjam"},{"name":"Computer Vision","_id":"cknzeeyly000vemzw6dbrf7n5"},{"name":"Online course","_id":"cknzeeym50015emzweoin27rl"},{"name":"GANs","_id":"cknzeeym60017emzw6l0tebol"},{"name":"Survey","_id":"cknzeeymb001cemzwdiiqh735"},{"name":"Vision transformer","_id":"cknzeeyme001eemzwhm438eim"},{"name":"Computer vision","_id":"cknzeeymg001femzwhga83qty"},{"name":"Course project","_id":"cknzeeymi001gemzwf0jf8d0d"}]}}